[
    {
        "citation_text": "Asai et al., 2023",
        "paper_id": "19",
        "raw_claim": " Early efforts in this domain, such as Retrieval-Augmented Generation (RAG), augment LLMs with external knowledge retrieval to mitigate factual inaccuracies (Asai et al., 2023).",
        "paper_content": "Preprint.\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens to evaluate its own prediction after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n3.3\nSELF-RAG INFERENCE\nGenerating reflection tokens to self-evaluate its own output makes SELF-RAG controllable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\nAdaptive retrieval with threshold. SELF-RAG dynamically decides when to retrieve text passages by\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\nCritique token type. For each critique token group G (e.g.,\nISREL ), we denote its score at timestamp\nt as sG\nt , and we compute a segment score as follows:\nf(yt, d, Critique ) = p(yt|x, d, y<t)) + S( Critique ), where\n(3)\nS( Critique ) =\nX\nG\u2208G\nwGsG\nt for G = { ISREL , ISSUP , ISUSE },\n(4)\nwhere sG\nt =\npt(\u02c6r)\nPNG\ni=1 pt(ri) stands for the generation probability of the most desirable reflection token\n\u02c6r (e.g.,\nISREL =Relevant) for the critique token type G with N G distinct tokens (that represent\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\ny is mostly supported by evidence, we can set a weight term for the\nISSUP score higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\nfilter out a segment continuation when the model generates an undesirable\nCritique token (e.g.,\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\u2019\nbehaviors. SELF-RAG tailors an LM with no additional training.\n4\nEXPERIMENTS\n4.1\nTASKS AND DATASETS\nWe conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\nClosed-set tasks include two datasets, i.e., a fact verification dataset about public health (PubHealth;\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams (ARC-\n6\n\nPreprint.\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\nCRITIQUE THROUGH SELF-REFLECTION\nAkari Asai\u2020, Zeqiu Wu\u2020, Yizhong Wang\u2020\u00a7, Avirup Sil\u2021, Hannaneh Hajishirzi\u2020\u00a7\n\u2020University of Washington\n\u00a7Allen Institute for AI\n\u2021IBM Research AI\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\nABSTRACT\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\neration (SELF-RAG) that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called reflection tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models.1\n1\nINTRODUCTION\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n1Our code and trained models are available at https://selfrag.github.io/.\n1\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023\n\nPreprint.\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\nshot fine-tuning on task datasets (Izacard et al., 2022b).\nWhile prior work often retrieves only\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\nConcurrent RAG work.\nA few concurrent works2 on RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\non retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n3\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\nor confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n3.1\nPROBLEM FORMALIZATION AND OVERVIEW\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\n2All work is arXived within a week of this preprint.\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n3\n",
        "score": "Yes"
    },
    {
        "citation_text": "Slobodkin et al., 2024",
        "paper_id": "0",
        "raw_claim": " To address this, methods like \"Attribute First, then Generate\" prioritize concise, fine-grained attributions by breaking down the generation process into content selection, sentence planning, and sequential sentence generation (Slobodkin et al., 2024).",
        "paper_content": "Attribute First, then Generate:\nLocally-attributable Grounded Text Generation\nAviv Slobodkin1*\nEran Hirsch1\u2217\nArie Cattan1\nTal Schuster2\nIdo Dagan1\n1Bar-Ilan University\n2Google Research\n{lovodkin93, hirscheran, ariecattan}@gmail.com\ntalschuster@google.com\ndagan@cs.biu.ac.il\nAbstract\nRecent\nefforts\nto\naddress\nhallucinations\nin Large Language Models (LLMs) have\nfocused on attributed text generation, which\nsupplements generated texts with citations\nof supporting sources for post-generation\nfact-checking and corrections.\nYet, these\ncitations often point to entire documents or\nparagraphs, burdening users with extensive\nverification work. In this paper, we introduce\na locally-attributable text generation approach,\nprioritizing concise attributions. Our method,\nnamed \u201cAttribute First, then Generate\u201d, breaks\ndown the conventional end-to-end generation\nprocess into three intuitive steps:\ncontent\nselection, sentence planning, and sequential\nsentence generation. By initially identifying\nrelevant source segments (\u201cselect first\u201d) and\nthen conditioning the generation process on\nthem (\u201cthen generate\u201d), we ensure these\nsegments also act as the output\u2019s fine-grained\nattributions (\u201cselect\u201d becomes \u201cattribute\u201d).\nTested on Multi-document Summarization\nand\nLong-form\nQuestion-answering,\nour\nmethod not only yields more concise citations\nthan the baselines but also maintains\u2014and\nin some cases enhances\u2014both generation\nquality and attribution accuracy. Furthermore,\nit significantly reduces the time required for\nfact verification by human assessors.1\n1\nIntroduction\nGrounded text generation, which includes tasks\nlike summarization and question-answering, aims\nto produce content that is derived from specific\nsources, either user-provided or retrieved via re-\ntrieval mechanisms (e.g., RAG; Lewis et al., 2020).\nTo facilitate verification of models\u2019 adherence to\nthese sources, recent years have seen a growing\ninterest in attributed text generation (Bohnet et al.,\n* Equal contribution.\n1Our code is publicly available at https://github.com/\nlovodkin93/attribute-first-then-generate\n2023), which aims to create text alongside sup-\nporting evidence. This method enhances models\u2019\ncredibility by enabling factuality verification. It\nalso aids in detecting and addressing factual errors,\na critical need given the observed frequency of such\nerrors, termed \u201challucinations\u201d, in model outputs\ncompared to source texts (Mishra et al., 2024a).\nWhile these attributions are aimed to facilitate\nfactuality evaluation by focusing people\u2019s attention\non relevant supporting texts, current approaches\noften yield rather coarse attributions, pointing back\nto whole documents or paragraphs. Such attribu-\ntions, though better than having none, require hu-\nman assessors to exhaustively sift through many\nirrelevant details in the cited content, resulting in\na time-consuming and somewhat ineffective fact-\nchecking process. Can we do better?\nIn this work, we extend attributed text generation\nto also consider attribution conciseness, reformaliz-\ning the task as Locally-attributable Text Generation\n(see Section 2). We introduce a criterion for attribu-\ntions to be precise, targeting only the most relevant\ntext snippets, from specific sentences down to sub-\nsentence spans, while avoiding non-essential de-\ntails. This criterion complements the full coverage\nrequirement, which stipulates that every generated\nfact be backed by a cited snippet. As illustrated in\nFig. 1, given a query and several documents, the\nmodel output includes citations to relevant spans\nwithin the source documents for each generated\nsentence instead of broadly attributing the entire\ndocuments (Gao et al., 2023b). These spans rep-\nresent the minimal set of relevant source snippets\nneeded to cover the content of the output sentences.\nFollowing our localization criterion, we pro-\npose a novel attribution-driven generation ap-\nproach, aimed to ensure both conciseness and\nfull coverage, termed \u201cAttribute First, then Gen-\nerate\u201d. Rather than jointly producing text with\ncitations (Gao et al., 2023b) or getting the attri-\nbution post-generation (Bohnet et al., 2023), we\narXiv:2403.17104v3  [cs.CL]  4 Jul 2024\n\nis more suitable for in-context learning whereas\ntext-planning works well with finetuned special-\nized models. We explore both options in this work.\n7\nConclusion\nIn this work, we present \u201cAttribute First, then Gen-\nerate\u201d, a novel scheme for attributable grounded\ntext generation, comprised of separating the gener-\nation process into multiple steps, such that attribu-\ntion is achieved as a by-product of the generation.\nTo the best of our knowledge, we are the first to\nprovide such fine-grained attributions in both the\ngenerated text and the references. Through auto-\nmatic and manual evaluations, we showed that our\ndecomposed generation pipeline either matches or\noutperforms existing baselines in terms of task-\nspecific metrics. Importantly, our approach yields\norders of magnitude shorter attributions, shortening\nthe fact-checking time by almost 50%, with little\nimpact on the attribution accuracy. While this work\nfocused on a particular implementation of content\nselection and generation, we encourage future ex-\nplorations to extend our \u201cAttribute First, then Gen-\nerate\u201d paradigm to other configurations. We hope\nthat our research will motivate future work in this\ndirection, improving the quality of text generation\nwhile also providing helpful concise attributions to\nexternal sources, thereby increasing models\u2019 trust-\nworthiness and safe downstream use.\nLimitations\nOur Attribute First, Then Generate approach tends\nto use more computing resources and is also slower\ncompared to direct end-to-end generation, as de-\ntailed in Appendix J. This is because it requires\nseveral steps, each involving a separate call to the\nmodel.\nFurthermore, our pipeline is comprised of sev-\neral interlinked components, which can potentially\nlead to error propagation. However, this potential\nissue is partially alleviated by the inherent robust-\nness of the individual components. For instance,\nshould the content selection mechanism identify\na broad array of text fragments, including some\nthat are irrelevant, subsequent stages in the pro-\ncess are designed to filter out such non-essential\ninformation.\nEthics Statement\nCrowdsourcing.\nFor our human evaluation we\nused the Amazon Mechanical Turk13 (MTurk)\ncrowdsourcing platform. Recruitment of annota-\ntors was conducted via email invitations dispatched\nfrom the MTurk platform, aimed at individuals who\nhad previously demonstrated high performance in\nsimilar research endeavors. Compensation was pro-\nvided not only for the actual annotation tasks but\nalso for an initial onboarding phase, which included\nfamiliarization with the annotation guidelines and\ngoing over feedback from the authors. The pricing\nfor each annotation instance was calibrated based\non an estimated completion time, aiming to main-\ntain a compensation rate of approximately 12$ per\nhour. Additionally, we closely monitored the actual\ntime required for task completion and offered post-\nhoc reimbursements in compensation for instances\nwhere the required time exceeded our initial esti-\nmate by more than 5%.\nDatasets licenses.\nThe DUC and TAC datasets\nwere acquired according to the required NIST\nguidelines (duc.nist.gov), and we do not pub-\nlish any data from these datasets. The annotation\ndata from Liu et al. (2023) is published with the\nMIT license. The Multi-News dataset (Fabbri et al.,\n2019) is published with a non-commercial license\nintended for research and educational purposes. Ac-\ncordingly, our human evaluation annotations will\nalso be limited to non-commercial use.\nAI assistants\nAI assistants were exclusively em-\nployed to enhance the grammatical structuring of\nthe text.\nAcknowledgements\nThis work was supported by the Israel Science\nFoundation (grant no. 2827/21).\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an LLM knows when it\u2019s lying. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 967\u2013976, Singapore. Associa-\ntion for Computational Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\n13https://worker.mturk.com/\n9\n\nSentence-level Plan\nA fire has badly damaged the \nsupermarket in Wellington.\nAttributed Fused\nOutput\nThe fire has destroyed a large \nsection of the store.\nFire crews are still on the \nscene.\nPre-selected Content\n(in context)\nDocument\nDocument\nA fire has badly damaged the \nsupermarket in Wellington.\nA large section of the supermarket \nwas destroyed. [1] [2]\nFire crews are still in Wellington.\n[2] [3]\n(c)\nSentence \nFusion\n(b)\nGeneration\nPlanning\n(a) \nTask-speci\ufb01c \nContent Selection\nSource\nTexts\nDocument\nDocument\nFigure 2: Our attribute first process guides the model to output fluent texts that are consistent with input sources,\nand include fine-grained sentence-level attributions to localized text spans (i.e., highlights) from the inputs.\nreaders, it is vital that they support as much of the\noutput as possible, while also achieving the highest\ngranularity in both the input and output.\nConcretely, output granularity entails assigning\nthe smallest information units in the generated text\ny\u2014in this work, individual sentences si\u2014with a\ncorresponding attribution set Ci, consisting of one\nor more supporting citations cj\ni \u2208Ci. Simultane-\nously, input granularity requires each citation cj\ni\nto point to a specific span of text hk \u2208D, pos-\nsibly non-consecutive. Importantly, these spans\nshould be as concise as possible while ensuring\ncompleteness in attribution, i.e., that every piece\nof information in the output, which in our setting\nrefers to each output sentence si, is fully backed\nby the union of its citations Ci. For example, in\nFig. 1, Ci is depicted as squared brackets at the end\nof each sentence with pointers to spans hk, shown\nas colored highlights in D (we use \u201chighlights\u201d and\n\u201cspans\u201d interchangeably).\n3\nModeling\nThis work aims to integrate source attribution\nwith the content-grounded generation process, to\nachieve more targeted attribution. Our approach is\nbased on uncovering the inherent decision-making\nmechanisms involved in text generation. We first\ndescribe our suggested Attribute First, then Gen-\nerate scheme in Section 3.1. Then, we propose\ntwo strategies for this framework\u2019s application: a\nprompt-based in-context learning approach (Sec-\ntion 3.2) and fine-tuning designated components\n(Section 3.3). For each approach, we provide a\nbrief overview of our implementation strategy for\nthe framework\u2019s phases, while a more in-depth elab-\noration can be found in Appendix C.\n3.1\nAttribute First, then Generate\nTo reliably achieve the desirable fine-grained attri-\nbution, we introduce the Attribute First, then Gen-\nerate paradigm, outlined in Fig. 2. This method is\nstructured around three key steps, mirroring the in-\nherent decision-making processes involved in text\ngeneration: content selection, sentence-planning,\nand sentence-by-sentence generation. Though ap-\nplicable to any grounded text generation task, we\nspecifically test its application in Multi-document\nSummarization (MDS) and Long-form Question-\nanswering (LFQA), adapting only the content se-\nlection step to each task\u2019s unique requirements.\nContent Selection\nThe approach begins with\nidentifying relevant spans from the source that\nwould contribute information to the generated out-\nput ((a) in Fig. 2). These spans then function as a\nmore focused grounding content, guiding the lexi-\ncal choices in the generation process. Notably, as\nthese chosen segments constitute the primary infor-\nmation for the resulting text, they effectively serve\nas its (fine-grained) attribution.\nSentence Planning\nThis step is designed to\nachieve sentence-level attribution in the out-\nput through the introduction of an intermediate\nsentence-level planning step ((b) in Fig. 2). Here,\nthe selected spans from the previous step are or-\nganized into coherently ordered clusters, where\nthe spans grouped within each cluster naturally\nfit within the same sentence. This step, which\nbreaks down the constrained generation process\ninto smaller, more manageable steps, serves two\npurposes: it simplifies the model\u2019s task by condi-\ntioning each generation iteration on a single cluster\nrather than the entire set of selected spans, while\nalso leading to the desired, sentence-level attribu-\ntion in the output, in the form of the corresponding\ncluster\u2019s highlights.\n3\n",
        "score": "Yes"
    },
    {
        "citation_text": "Xia et al., 2024",
        "paper_id": "2",
        "raw_claim": " Similarly, ReClaim focuses on interleaved reference-claim generation to provide sentence-level citations, significantly improving verifiability (Xia et al., 2024).",
        "paper_content": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with\nInterleaved Reference-Claim Generation\nSirui Xia1, Xintao Wang1, Jiaqing Liang2\u2217, Yifei Zhang1, Weikang Zhou3\nJiaji Deng3, Fei Yu3, Yanghua Xiao1\u2217\n1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n2School of Data Science, Fudan University\n3AntGroup\n{srxia24, xtwang21, yifeizhang23}@m.fudan.edu.cn,\n{liangjiaqing, shawyh}@fudan.edu.cn,\nfeiyu.fyyu@gmail.com, {zhouweikang.zwk, dengjiaji.djj}@antgroup.com\nAbstract\nRetrieval-Augmented Generation (RAG) has\nbeen widely adopted to enhance Large Lan-\nguage Models (LLMs) in knowledge-intensive\ntasks. To enhance credibility and verifiability\nin RAG systems, Attributed Text Generation\n(ATG) is proposed, which provides citations\nto retrieval knowledge in LLM-generated re-\nsponses. Prior methods mainly adopt coarse-\ngrained attributions, with passage-level or\nparagraph-level references or citations, which\nfall short in verifiability. This paper proposes\nRECLAIM (Refer & Claim), a fine-grained\nATG method that alternates the generation of\nreferences and answers step by step. Differ-\nent from previous coarse-grained attribution,\nRECLAIM provides sentence-level citations in\nlong-form question-answering tasks. With ex-\ntensive experiments, we verify the effectiveness\nof RECLAIM in extensive settings, achieving a\ncitation accuracy rate of 90%.1\n1\nIntroduction\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020) is a technique that integrates informa-\ntion retrieval with natural language generation to\nenhance the performance of large language model\n(LLMs) responses. However, the RAG system still\nencounters challenges related to verifiability and\ncredibility. To address these issues, attributed text\ngeneration (ATG) (Bohnet et al., 2022) has been\nproposed. ATG aims to improve RAG systems in\nterms of: 1) Credibility, as explicit citations can\nreduce hallucinations; 2) Verifiability, making it\neasier for users to verify the answer.\nPrevious efforts on ATG mainly focus on\npassage-level (Thoppilan et al., 2022) or paragraph-\nlevel references (Menick et al., 2022; Nakano et al.,\n2021; Gao et al., 2023b). Although these attribution\nmethods have contributed to improving the verifi-\nability and credibility of model responses, current\n1Code and datasets are public at: https://github.com/\npdxthree/ReClaim\nIn economics, \nthe market \nprice is the \namount... In \nthe realm of \neconomics, the \nconcept of...\nA simple \nmarket price \ndefinition \nspecific to... \n(Definition of \nmarket price \nfrom the...\nAccording to the citation: In economics, the market \nprice is the... We can know that: The market price \nis... According to the citation: A simple market price \ndefinition... We can know that: It is the price of... \nAccording to the citation: ...\nWhat is the definition of market price?\nNow, I will answer this question, \ngrounding each sentence in the \nanswer to references from \ndocuments ...\nFigure 1: The task setup for RECLAIM. Given question\nand reference passages from a large corpus. The LLM\nthen generates a response with fine-grained citations.\nFor detailed examples, see Table 11.\nmethods often focus on relatively coarse-grained at-\ntributions, which may contain a significant amount\nof irrelevant information. This increases the time\nrequired for fact-checking.\nIn this paper, we propose RECLAIM, which\ngenerates attributed text with interleaving refer-\nences and answers for RAG systems, as is shown\nin Figure 1. This method enables sentence-level\nfine-grained attributions in long-form question-\nanswering using the RAG system.\nTo enhance the LLM\u2019s performance in attributed\ntext generation, we developed a training dataset and\nfine-tuned the LLM to facilitate sentence-level ci-\ntation selection from given reference passages and\nsubsequent answer generation. We implemented\nan alternating strategy between citation generation\nand answer text generation. We apply constrained\ndecoding during LLM inference by encoding refer-\nence passages into a token-level prefix tree. This\narXiv:2407.01796v2  [cs.CL]  23 May 2025\n\nrestricts the LLM to generate citations only along\nthe tree\u2019s paths, ensuring alignment with the refer-\nence passages and avoiding inconsistencies.\nThe results of experiments demonstrate that RE-\nCLAIM outperforms existing baselines. RECLAIM\nsignificantly improves the citation quality, enabling\nthe citations to better support the answer. Further-\nmore, RECLAIM greatly reduces the verbosity of\ncitations, thereby easing the fact-checking process.\nOur contributions are summarized as follows:\n1. We propose a method called RECLAIM, which\nalternately generates citations and answer sen-\ntences, enabling LLM to produce answers\nwith sentence-level citations, thus enhancing\nthe LLM\u2019s verifiability and credibility.\n2. To enhance LLMs in sentence-level citation\ngeneration, we construct a dataset based\non WebGLM-QA (Liu et al., 2023) and\nELI5 (Fan et al., 2019) dataset. Then, we\nfine-tune Llama3-8B-Instruct (Dubey et al.,\n2024) models for reference and claim gener-\nation respectively, achieving better citation\nquality compared to the baseline method with\nChatGPT (OpenAI, 2022).\n3. Through extensive experiments, we demon-\nstrate the effectiveness of our method in en-\nhancing the LLM\u2019s verifiability and credi-\nbility, achieving performance comparable to\nmuch bigger models like ChatGPT.\n2\nRelated Work\nRetrieval-Augmented Generation\nIn this paper,\nwe use the RAG (Retrieval-Augmented Genera-\ntion) system to generate answer with citations. The\nRAG system was proposed to combine informa-\ntion retrieval with generation models for tasks such\nas question answering and knowledge generation.\nSimilarly, this system has been widely applied to\nhandle complex tasks that require extracting in-\nformation from a large number of documents, in-\ncluding open-domain question answering, dialogue\nsystems, and information summarization (Izacard\nand Grave, 2021; Karpukhin et al., 2020).\nLong-form Text Question Answering\nOur work\nprimarily focuses on the long-form question an-\nswering (LFQA) task within the RAG system. Un-\nlike short-form QA (Rajpurkar et al., 2016; Joshi\net al., 2017), which concentrates on extracting con-\ncise facts, LFQA generates comprehensive answers\nthat require deep contextual understanding and in-\nformation integration from multiple sources (Fan\net al., 2019; Stelmakh et al., 2022; Malaviya et al.,\n2024).\nAttributed Text Generation\nMany current\nworks propose various methods for generating\nanswer text with citations, differing in their ap-\nproaches to attribution and citation granularity.\nLaMDA (Thoppilan et al., 2022) provides attri-\nbution for the entire response in the form of URLs\npointing to entire documents. WebGPT (Nakano\net al., 2021) and GopherCite (Menick et al., 2022)\nuse reinforcement learning from human prefer-\nences to enable LLMs to answer questions while\nproviding snippet citations. ALCE (Gao et al.,\n2023b) goes further by providing one or more input\ndocuments as attribution for each sentence in the\nanswer, in a manner similar to cross-referencing.\nAdditionally, some work has focused on fine-tuning\nmodels to improve the generation of attributed an-\nswer text (Huang et al., 2024a; Asai et al., 2023;\nHuang et al., 2024c).\nIn addition to the aforementioned methods that\nadd citations directly during answer generation,\nthere are some works that focus on finding cita-\ntions afterward (Gao et al., 2023a). Some research\nfurther achieves better attribution performance\nthrough multiple retrievals and validations (Li et al.,\n2024; Sun et al., 2023).\n3\nMethod\nWe introduce RECLAIM, which aims to generate\ntext with sentence-level citations. Specifically, RE-\nCLAIM generates citations with answers alterna-\ntively, in a step-by-step manner. We first introduce\nthe overview of RECLAIM in Section 3.1, and then\ndetail the specific implementation in Section 3.2\nto 3.5.\n3.1\nRECLAIM: Interleaving Reference and\nClaim\nOur task can be formally expressed as follows:\nGiven a query q and several reference passages\nretrieved by the RAG system D, the LLM is re-\nquired to generate an output O = { r1, c1, r2, c2...rn,\ncn }, where O consists of n sentence-level fine-\ngrained references r1, ..., rn, which represent the\nfine-grained citations coming from reference pas-\nsages (provided as text, not just source numbers)\nand n claims c1, ..., cn, which are portions of the\n\ngenerated answers are fully based on the informa-\ntion in the given reference paragraphs.\nWe prioritize the CAS metric and use faithful-\nness as a supplementary measure to evaluate cred-\nibility improvements. Since faithfulness to a ci-\ntation implies faithfulness to the original text, a\nhigh faithfulness score does not guarantee strong\nCAS performance, as an answer true to the full\ndocument may not align with the selected citation.\nWe evaluate the faithfulness metric on the test\ndatasets, and the results for the ELI5 dataset are\nshown in Figure 4. Results for other datasets are\nshown in Appendix Figure 7 and 8.\n8\n10\n12\n14\n16\n18\n20\nAccuracy\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFaithfulness\nALCE\nRS+RL\nSelf-RAG\nFront\nReClaim prompting\nReClaim unified\nReClaim w/IG\nFaithfulness-Accuracy Relationship\nFigure 4: The x-axis represents the accuracy of the\nLLM\u2019s responses, while the y-axis shows the faithful-\nness score. For the Self-RAG and RS+RL methods, we\nuse the fine-tuned 7B model, whereas for other methods,\nwe employ Llama3-8B-Instruction as the base model.\nOur method achieved the highest faithfulness\nscore. This demonstrates that while our approach\nmay slightly reduce answer quality, it significantly\nenhances answer faithfulness and minimizes unnec-\nessary hallucinations linked to the LLM\u2019s internal\nparameters.\nThe results show an inverse relationship between\nthe accuracy of LLM-generated answers and their\nfaithfulness to reference passages. Higher faithful-\nness and more granular citations have narrowed the\nscope of our answer sources, which may contribute\nto the lower accuracy in LFQA task.\n5\nExperiment Results\nIn the experiments, we wish to answer two research\nquestions: RQ1) How to improve the quality of\nanswers and citations? RQ2) Can RECLAIM en-\nhance the verifiability and credibility of RAG-\nbased question answering?\n5.1\nHow to Improve the Quality of Answers\nand Citations?\nThe overall performance is presented in Table 2.\nRECLAIM prompting Works\nExperimental re-\nsults show that directly prompting LLMs yields\nsatisfactory outcomes.\nOur approach improves\naverage answer fluency and citation accuracy\n(CAS) compared to baseline methods. Notably,\nLlama3-8B-Instruct surpasses other baselines in\nCAS scores, including ALCE+ChatGPT.\nAlthough our method performs worse in CRS,\nthe finer granularity of our citations reduces the\nimpact of redundant content. Redundant citations\nonly add a single irrelevant sentence, which does\nnot significantly increase the cost of fact-checking.\nRECLAIM Unified Cannot Improve ATG\nEx-\nperimental results show that the RECLAIM Unified\nmethod significantly reduces citation quality\n(CAS). This indicates that fine-tuning the LLM\nin this way fails to teach it how to generate claims\nbased solely on the information from the previous\nreference.\nRECLAIM w/IG Improves Attribution\nExper-\nimental results indicate that the RECLAIM w/IG\nmethod outperforms other methods in two citation\nquality metrics while maintaining high fluency and\ncorrectness scores.\nCompared to ALCE using ChatGPT, our method\n(using Llama3-8B) shows an average improvement\nof 31.3% on the CAS, 16.7% on the CRS, and\n25.7% on the MAUVE across three test datasets,\nwith only a 6.0% decrease in answer accuracy.\nSpecifically, we achieved an average CAS score\nof 90.7 across three test datasets, which is a crucial\nmetric for assessing the degree of text attribution.\nCompared to the RECLAIM Unified method, the\nRECLAIM w/IG approach\u2019s biggest difference lies\nin the training and inference strategies during the\nclaim generation phase. It filters out extraneous\ncontextual information and trains the LLM to gen-\nerate claims based solely on the preceding refer-\nence part. The significant improvements in citation\nquality demonstrate the effectiveness of the strategy\nadopted by the RECLAIM w/IG method.\nAs shown in Table 4, the results of the ablation\nexperiments indicate that fine-tuning two LLMs\nfor alternating generation of references and claims\nachieves the most balanced performance.\nWhile ReferModel-Only w/Sum method yields\na high citation quality score, it compromises the\n",
        "score": "Yes"
    },
    {
        "citation_text": "Slobodkin et al., 2024",
        "paper_id": "0",
        "raw_claim": "\n\nThe broader landscape of attributed text generation also encompasses foundational work in defining and evaluating attribution. Bohnet et al. (2022) formulated Attributed Question Answering (QA) and proposed a reproducible evaluation framework, including metrics like AIS and AutoAIS, to measure attribution quality.",
        "paper_content": "Attribute First, then Generate:\nLocally-attributable Grounded Text Generation\nAviv Slobodkin1*\nEran Hirsch1\u2217\nArie Cattan1\nTal Schuster2\nIdo Dagan1\n1Bar-Ilan University\n2Google Research\n{lovodkin93, hirscheran, ariecattan}@gmail.com\ntalschuster@google.com\ndagan@cs.biu.ac.il\nAbstract\nRecent\nefforts\nto\naddress\nhallucinations\nin Large Language Models (LLMs) have\nfocused on attributed text generation, which\nsupplements generated texts with citations\nof supporting sources for post-generation\nfact-checking and corrections.\nYet, these\ncitations often point to entire documents or\nparagraphs, burdening users with extensive\nverification work. In this paper, we introduce\na locally-attributable text generation approach,\nprioritizing concise attributions. Our method,\nnamed \u201cAttribute First, then Generate\u201d, breaks\ndown the conventional end-to-end generation\nprocess into three intuitive steps:\ncontent\nselection, sentence planning, and sequential\nsentence generation. By initially identifying\nrelevant source segments (\u201cselect first\u201d) and\nthen conditioning the generation process on\nthem (\u201cthen generate\u201d), we ensure these\nsegments also act as the output\u2019s fine-grained\nattributions (\u201cselect\u201d becomes \u201cattribute\u201d).\nTested on Multi-document Summarization\nand\nLong-form\nQuestion-answering,\nour\nmethod not only yields more concise citations\nthan the baselines but also maintains\u2014and\nin some cases enhances\u2014both generation\nquality and attribution accuracy. Furthermore,\nit significantly reduces the time required for\nfact verification by human assessors.1\n1\nIntroduction\nGrounded text generation, which includes tasks\nlike summarization and question-answering, aims\nto produce content that is derived from specific\nsources, either user-provided or retrieved via re-\ntrieval mechanisms (e.g., RAG; Lewis et al., 2020).\nTo facilitate verification of models\u2019 adherence to\nthese sources, recent years have seen a growing\ninterest in attributed text generation (Bohnet et al.,\n* Equal contribution.\n1Our code is publicly available at https://github.com/\nlovodkin93/attribute-first-then-generate\n2023), which aims to create text alongside sup-\nporting evidence. This method enhances models\u2019\ncredibility by enabling factuality verification. It\nalso aids in detecting and addressing factual errors,\na critical need given the observed frequency of such\nerrors, termed \u201challucinations\u201d, in model outputs\ncompared to source texts (Mishra et al., 2024a).\nWhile these attributions are aimed to facilitate\nfactuality evaluation by focusing people\u2019s attention\non relevant supporting texts, current approaches\noften yield rather coarse attributions, pointing back\nto whole documents or paragraphs. Such attribu-\ntions, though better than having none, require hu-\nman assessors to exhaustively sift through many\nirrelevant details in the cited content, resulting in\na time-consuming and somewhat ineffective fact-\nchecking process. Can we do better?\nIn this work, we extend attributed text generation\nto also consider attribution conciseness, reformaliz-\ning the task as Locally-attributable Text Generation\n(see Section 2). We introduce a criterion for attribu-\ntions to be precise, targeting only the most relevant\ntext snippets, from specific sentences down to sub-\nsentence spans, while avoiding non-essential de-\ntails. This criterion complements the full coverage\nrequirement, which stipulates that every generated\nfact be backed by a cited snippet. As illustrated in\nFig. 1, given a query and several documents, the\nmodel output includes citations to relevant spans\nwithin the source documents for each generated\nsentence instead of broadly attributing the entire\ndocuments (Gao et al., 2023b). These spans rep-\nresent the minimal set of relevant source snippets\nneeded to cover the content of the output sentences.\nFollowing our localization criterion, we pro-\npose a novel attribution-driven generation ap-\nproach, aimed to ensure both conciseness and\nfull coverage, termed \u201cAttribute First, then Gen-\nerate\u201d. Rather than jointly producing text with\ncitations (Gao et al., 2023b) or getting the attri-\nbution post-generation (Bohnet et al., 2023), we\narXiv:2403.17104v3  [cs.CL]  4 Jul 2024\n\nis more suitable for in-context learning whereas\ntext-planning works well with finetuned special-\nized models. We explore both options in this work.\n7\nConclusion\nIn this work, we present \u201cAttribute First, then Gen-\nerate\u201d, a novel scheme for attributable grounded\ntext generation, comprised of separating the gener-\nation process into multiple steps, such that attribu-\ntion is achieved as a by-product of the generation.\nTo the best of our knowledge, we are the first to\nprovide such fine-grained attributions in both the\ngenerated text and the references. Through auto-\nmatic and manual evaluations, we showed that our\ndecomposed generation pipeline either matches or\noutperforms existing baselines in terms of task-\nspecific metrics. Importantly, our approach yields\norders of magnitude shorter attributions, shortening\nthe fact-checking time by almost 50%, with little\nimpact on the attribution accuracy. While this work\nfocused on a particular implementation of content\nselection and generation, we encourage future ex-\nplorations to extend our \u201cAttribute First, then Gen-\nerate\u201d paradigm to other configurations. We hope\nthat our research will motivate future work in this\ndirection, improving the quality of text generation\nwhile also providing helpful concise attributions to\nexternal sources, thereby increasing models\u2019 trust-\nworthiness and safe downstream use.\nLimitations\nOur Attribute First, Then Generate approach tends\nto use more computing resources and is also slower\ncompared to direct end-to-end generation, as de-\ntailed in Appendix J. This is because it requires\nseveral steps, each involving a separate call to the\nmodel.\nFurthermore, our pipeline is comprised of sev-\neral interlinked components, which can potentially\nlead to error propagation. However, this potential\nissue is partially alleviated by the inherent robust-\nness of the individual components. For instance,\nshould the content selection mechanism identify\na broad array of text fragments, including some\nthat are irrelevant, subsequent stages in the pro-\ncess are designed to filter out such non-essential\ninformation.\nEthics Statement\nCrowdsourcing.\nFor our human evaluation we\nused the Amazon Mechanical Turk13 (MTurk)\ncrowdsourcing platform. Recruitment of annota-\ntors was conducted via email invitations dispatched\nfrom the MTurk platform, aimed at individuals who\nhad previously demonstrated high performance in\nsimilar research endeavors. Compensation was pro-\nvided not only for the actual annotation tasks but\nalso for an initial onboarding phase, which included\nfamiliarization with the annotation guidelines and\ngoing over feedback from the authors. The pricing\nfor each annotation instance was calibrated based\non an estimated completion time, aiming to main-\ntain a compensation rate of approximately 12$ per\nhour. Additionally, we closely monitored the actual\ntime required for task completion and offered post-\nhoc reimbursements in compensation for instances\nwhere the required time exceeded our initial esti-\nmate by more than 5%.\nDatasets licenses.\nThe DUC and TAC datasets\nwere acquired according to the required NIST\nguidelines (duc.nist.gov), and we do not pub-\nlish any data from these datasets. The annotation\ndata from Liu et al. (2023) is published with the\nMIT license. The Multi-News dataset (Fabbri et al.,\n2019) is published with a non-commercial license\nintended for research and educational purposes. Ac-\ncordingly, our human evaluation annotations will\nalso be limited to non-commercial use.\nAI assistants\nAI assistants were exclusively em-\nployed to enhance the grammatical structuring of\nthe text.\nAcknowledgements\nThis work was supported by the Israel Science\nFoundation (grant no. 2827/21).\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an LLM knows when it\u2019s lying. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 967\u2013976, Singapore. Associa-\ntion for Computational Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\n13https://worker.mturk.com/\n9\n\nSentence-level Plan\nA fire has badly damaged the \nsupermarket in Wellington.\nAttributed Fused\nOutput\nThe fire has destroyed a large \nsection of the store.\nFire crews are still on the \nscene.\nPre-selected Content\n(in context)\nDocument\nDocument\nA fire has badly damaged the \nsupermarket in Wellington.\nA large section of the supermarket \nwas destroyed. [1] [2]\nFire crews are still in Wellington.\n[2] [3]\n(c)\nSentence \nFusion\n(b)\nGeneration\nPlanning\n(a) \nTask-speci\ufb01c \nContent Selection\nSource\nTexts\nDocument\nDocument\nFigure 2: Our attribute first process guides the model to output fluent texts that are consistent with input sources,\nand include fine-grained sentence-level attributions to localized text spans (i.e., highlights) from the inputs.\nreaders, it is vital that they support as much of the\noutput as possible, while also achieving the highest\ngranularity in both the input and output.\nConcretely, output granularity entails assigning\nthe smallest information units in the generated text\ny\u2014in this work, individual sentences si\u2014with a\ncorresponding attribution set Ci, consisting of one\nor more supporting citations cj\ni \u2208Ci. Simultane-\nously, input granularity requires each citation cj\ni\nto point to a specific span of text hk \u2208D, pos-\nsibly non-consecutive. Importantly, these spans\nshould be as concise as possible while ensuring\ncompleteness in attribution, i.e., that every piece\nof information in the output, which in our setting\nrefers to each output sentence si, is fully backed\nby the union of its citations Ci. For example, in\nFig. 1, Ci is depicted as squared brackets at the end\nof each sentence with pointers to spans hk, shown\nas colored highlights in D (we use \u201chighlights\u201d and\n\u201cspans\u201d interchangeably).\n3\nModeling\nThis work aims to integrate source attribution\nwith the content-grounded generation process, to\nachieve more targeted attribution. Our approach is\nbased on uncovering the inherent decision-making\nmechanisms involved in text generation. We first\ndescribe our suggested Attribute First, then Gen-\nerate scheme in Section 3.1. Then, we propose\ntwo strategies for this framework\u2019s application: a\nprompt-based in-context learning approach (Sec-\ntion 3.2) and fine-tuning designated components\n(Section 3.3). For each approach, we provide a\nbrief overview of our implementation strategy for\nthe framework\u2019s phases, while a more in-depth elab-\noration can be found in Appendix C.\n3.1\nAttribute First, then Generate\nTo reliably achieve the desirable fine-grained attri-\nbution, we introduce the Attribute First, then Gen-\nerate paradigm, outlined in Fig. 2. This method is\nstructured around three key steps, mirroring the in-\nherent decision-making processes involved in text\ngeneration: content selection, sentence-planning,\nand sentence-by-sentence generation. Though ap-\nplicable to any grounded text generation task, we\nspecifically test its application in Multi-document\nSummarization (MDS) and Long-form Question-\nanswering (LFQA), adapting only the content se-\nlection step to each task\u2019s unique requirements.\nContent Selection\nThe approach begins with\nidentifying relevant spans from the source that\nwould contribute information to the generated out-\nput ((a) in Fig. 2). These spans then function as a\nmore focused grounding content, guiding the lexi-\ncal choices in the generation process. Notably, as\nthese chosen segments constitute the primary infor-\nmation for the resulting text, they effectively serve\nas its (fine-grained) attribution.\nSentence Planning\nThis step is designed to\nachieve sentence-level attribution in the out-\nput through the introduction of an intermediate\nsentence-level planning step ((b) in Fig. 2). Here,\nthe selected spans from the previous step are or-\nganized into coherently ordered clusters, where\nthe spans grouped within each cluster naturally\nfit within the same sentence. This step, which\nbreaks down the constrained generation process\ninto smaller, more manageable steps, serves two\npurposes: it simplifies the model\u2019s task by condi-\ntioning each generation iteration on a single cluster\nrather than the entire set of selected spans, while\nalso leading to the desired, sentence-level attribu-\ntion in the output, in the form of the corresponding\ncluster\u2019s highlights.\n3\n",
        "score": "Yes"
    },
    {
        "citation_text": "Xia et al., 2024",
        "paper_id": "2",
        "raw_claim": " This work provides crucial insights into how well current state-of-the-art methods perform on attribution and hints at how to build LLMs with better attribution capabilities (Bohnet et al., 2022).",
        "paper_content": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with\nInterleaved Reference-Claim Generation\nSirui Xia1, Xintao Wang1, Jiaqing Liang2\u2217, Yifei Zhang1, Weikang Zhou3\nJiaji Deng3, Fei Yu3, Yanghua Xiao1\u2217\n1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n2School of Data Science, Fudan University\n3AntGroup\n{srxia24, xtwang21, yifeizhang23}@m.fudan.edu.cn,\n{liangjiaqing, shawyh}@fudan.edu.cn,\nfeiyu.fyyu@gmail.com, {zhouweikang.zwk, dengjiaji.djj}@antgroup.com\nAbstract\nRetrieval-Augmented Generation (RAG) has\nbeen widely adopted to enhance Large Lan-\nguage Models (LLMs) in knowledge-intensive\ntasks. To enhance credibility and verifiability\nin RAG systems, Attributed Text Generation\n(ATG) is proposed, which provides citations\nto retrieval knowledge in LLM-generated re-\nsponses. Prior methods mainly adopt coarse-\ngrained attributions, with passage-level or\nparagraph-level references or citations, which\nfall short in verifiability. This paper proposes\nRECLAIM (Refer & Claim), a fine-grained\nATG method that alternates the generation of\nreferences and answers step by step. Differ-\nent from previous coarse-grained attribution,\nRECLAIM provides sentence-level citations in\nlong-form question-answering tasks. With ex-\ntensive experiments, we verify the effectiveness\nof RECLAIM in extensive settings, achieving a\ncitation accuracy rate of 90%.1\n1\nIntroduction\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020) is a technique that integrates informa-\ntion retrieval with natural language generation to\nenhance the performance of large language model\n(LLMs) responses. However, the RAG system still\nencounters challenges related to verifiability and\ncredibility. To address these issues, attributed text\ngeneration (ATG) (Bohnet et al., 2022) has been\nproposed. ATG aims to improve RAG systems in\nterms of: 1) Credibility, as explicit citations can\nreduce hallucinations; 2) Verifiability, making it\neasier for users to verify the answer.\nPrevious efforts on ATG mainly focus on\npassage-level (Thoppilan et al., 2022) or paragraph-\nlevel references (Menick et al., 2022; Nakano et al.,\n2021; Gao et al., 2023b). Although these attribution\nmethods have contributed to improving the verifi-\nability and credibility of model responses, current\n1Code and datasets are public at: https://github.com/\npdxthree/ReClaim\nIn economics, \nthe market \nprice is the \namount... In \nthe realm of \neconomics, the \nconcept of...\nA simple \nmarket price \ndefinition \nspecific to... \n(Definition of \nmarket price \nfrom the...\nAccording to the citation: In economics, the market \nprice is the... We can know that: The market price \nis... According to the citation: A simple market price \ndefinition... We can know that: It is the price of... \nAccording to the citation: ...\nWhat is the definition of market price?\nNow, I will answer this question, \ngrounding each sentence in the \nanswer to references from \ndocuments ...\nFigure 1: The task setup for RECLAIM. Given question\nand reference passages from a large corpus. The LLM\nthen generates a response with fine-grained citations.\nFor detailed examples, see Table 11.\nmethods often focus on relatively coarse-grained at-\ntributions, which may contain a significant amount\nof irrelevant information. This increases the time\nrequired for fact-checking.\nIn this paper, we propose RECLAIM, which\ngenerates attributed text with interleaving refer-\nences and answers for RAG systems, as is shown\nin Figure 1. This method enables sentence-level\nfine-grained attributions in long-form question-\nanswering using the RAG system.\nTo enhance the LLM\u2019s performance in attributed\ntext generation, we developed a training dataset and\nfine-tuned the LLM to facilitate sentence-level ci-\ntation selection from given reference passages and\nsubsequent answer generation. We implemented\nan alternating strategy between citation generation\nand answer text generation. We apply constrained\ndecoding during LLM inference by encoding refer-\nence passages into a token-level prefix tree. This\narXiv:2407.01796v2  [cs.CL]  23 May 2025\n\nrestricts the LLM to generate citations only along\nthe tree\u2019s paths, ensuring alignment with the refer-\nence passages and avoiding inconsistencies.\nThe results of experiments demonstrate that RE-\nCLAIM outperforms existing baselines. RECLAIM\nsignificantly improves the citation quality, enabling\nthe citations to better support the answer. Further-\nmore, RECLAIM greatly reduces the verbosity of\ncitations, thereby easing the fact-checking process.\nOur contributions are summarized as follows:\n1. We propose a method called RECLAIM, which\nalternately generates citations and answer sen-\ntences, enabling LLM to produce answers\nwith sentence-level citations, thus enhancing\nthe LLM\u2019s verifiability and credibility.\n2. To enhance LLMs in sentence-level citation\ngeneration, we construct a dataset based\non WebGLM-QA (Liu et al., 2023) and\nELI5 (Fan et al., 2019) dataset. Then, we\nfine-tune Llama3-8B-Instruct (Dubey et al.,\n2024) models for reference and claim gener-\nation respectively, achieving better citation\nquality compared to the baseline method with\nChatGPT (OpenAI, 2022).\n3. Through extensive experiments, we demon-\nstrate the effectiveness of our method in en-\nhancing the LLM\u2019s verifiability and credi-\nbility, achieving performance comparable to\nmuch bigger models like ChatGPT.\n2\nRelated Work\nRetrieval-Augmented Generation\nIn this paper,\nwe use the RAG (Retrieval-Augmented Genera-\ntion) system to generate answer with citations. The\nRAG system was proposed to combine informa-\ntion retrieval with generation models for tasks such\nas question answering and knowledge generation.\nSimilarly, this system has been widely applied to\nhandle complex tasks that require extracting in-\nformation from a large number of documents, in-\ncluding open-domain question answering, dialogue\nsystems, and information summarization (Izacard\nand Grave, 2021; Karpukhin et al., 2020).\nLong-form Text Question Answering\nOur work\nprimarily focuses on the long-form question an-\nswering (LFQA) task within the RAG system. Un-\nlike short-form QA (Rajpurkar et al., 2016; Joshi\net al., 2017), which concentrates on extracting con-\ncise facts, LFQA generates comprehensive answers\nthat require deep contextual understanding and in-\nformation integration from multiple sources (Fan\net al., 2019; Stelmakh et al., 2022; Malaviya et al.,\n2024).\nAttributed Text Generation\nMany current\nworks propose various methods for generating\nanswer text with citations, differing in their ap-\nproaches to attribution and citation granularity.\nLaMDA (Thoppilan et al., 2022) provides attri-\nbution for the entire response in the form of URLs\npointing to entire documents. WebGPT (Nakano\net al., 2021) and GopherCite (Menick et al., 2022)\nuse reinforcement learning from human prefer-\nences to enable LLMs to answer questions while\nproviding snippet citations. ALCE (Gao et al.,\n2023b) goes further by providing one or more input\ndocuments as attribution for each sentence in the\nanswer, in a manner similar to cross-referencing.\nAdditionally, some work has focused on fine-tuning\nmodels to improve the generation of attributed an-\nswer text (Huang et al., 2024a; Asai et al., 2023;\nHuang et al., 2024c).\nIn addition to the aforementioned methods that\nadd citations directly during answer generation,\nthere are some works that focus on finding cita-\ntions afterward (Gao et al., 2023a). Some research\nfurther achieves better attribution performance\nthrough multiple retrievals and validations (Li et al.,\n2024; Sun et al., 2023).\n3\nMethod\nWe introduce RECLAIM, which aims to generate\ntext with sentence-level citations. Specifically, RE-\nCLAIM generates citations with answers alterna-\ntively, in a step-by-step manner. We first introduce\nthe overview of RECLAIM in Section 3.1, and then\ndetail the specific implementation in Section 3.2\nto 3.5.\n3.1\nRECLAIM: Interleaving Reference and\nClaim\nOur task can be formally expressed as follows:\nGiven a query q and several reference passages\nretrieved by the RAG system D, the LLM is re-\nquired to generate an output O = { r1, c1, r2, c2...rn,\ncn }, where O consists of n sentence-level fine-\ngrained references r1, ..., rn, which represent the\nfine-grained citations coming from reference pas-\nsages (provided as text, not just source numbers)\nand n claims c1, ..., cn, which are portions of the\n\ngenerated answers are fully based on the informa-\ntion in the given reference paragraphs.\nWe prioritize the CAS metric and use faithful-\nness as a supplementary measure to evaluate cred-\nibility improvements. Since faithfulness to a ci-\ntation implies faithfulness to the original text, a\nhigh faithfulness score does not guarantee strong\nCAS performance, as an answer true to the full\ndocument may not align with the selected citation.\nWe evaluate the faithfulness metric on the test\ndatasets, and the results for the ELI5 dataset are\nshown in Figure 4. Results for other datasets are\nshown in Appendix Figure 7 and 8.\n8\n10\n12\n14\n16\n18\n20\nAccuracy\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFaithfulness\nALCE\nRS+RL\nSelf-RAG\nFront\nReClaim prompting\nReClaim unified\nReClaim w/IG\nFaithfulness-Accuracy Relationship\nFigure 4: The x-axis represents the accuracy of the\nLLM\u2019s responses, while the y-axis shows the faithful-\nness score. For the Self-RAG and RS+RL methods, we\nuse the fine-tuned 7B model, whereas for other methods,\nwe employ Llama3-8B-Instruction as the base model.\nOur method achieved the highest faithfulness\nscore. This demonstrates that while our approach\nmay slightly reduce answer quality, it significantly\nenhances answer faithfulness and minimizes unnec-\nessary hallucinations linked to the LLM\u2019s internal\nparameters.\nThe results show an inverse relationship between\nthe accuracy of LLM-generated answers and their\nfaithfulness to reference passages. Higher faithful-\nness and more granular citations have narrowed the\nscope of our answer sources, which may contribute\nto the lower accuracy in LFQA task.\n5\nExperiment Results\nIn the experiments, we wish to answer two research\nquestions: RQ1) How to improve the quality of\nanswers and citations? RQ2) Can RECLAIM en-\nhance the verifiability and credibility of RAG-\nbased question answering?\n5.1\nHow to Improve the Quality of Answers\nand Citations?\nThe overall performance is presented in Table 2.\nRECLAIM prompting Works\nExperimental re-\nsults show that directly prompting LLMs yields\nsatisfactory outcomes.\nOur approach improves\naverage answer fluency and citation accuracy\n(CAS) compared to baseline methods. Notably,\nLlama3-8B-Instruct surpasses other baselines in\nCAS scores, including ALCE+ChatGPT.\nAlthough our method performs worse in CRS,\nthe finer granularity of our citations reduces the\nimpact of redundant content. Redundant citations\nonly add a single irrelevant sentence, which does\nnot significantly increase the cost of fact-checking.\nRECLAIM Unified Cannot Improve ATG\nEx-\nperimental results show that the RECLAIM Unified\nmethod significantly reduces citation quality\n(CAS). This indicates that fine-tuning the LLM\nin this way fails to teach it how to generate claims\nbased solely on the information from the previous\nreference.\nRECLAIM w/IG Improves Attribution\nExper-\nimental results indicate that the RECLAIM w/IG\nmethod outperforms other methods in two citation\nquality metrics while maintaining high fluency and\ncorrectness scores.\nCompared to ALCE using ChatGPT, our method\n(using Llama3-8B) shows an average improvement\nof 31.3% on the CAS, 16.7% on the CRS, and\n25.7% on the MAUVE across three test datasets,\nwith only a 6.0% decrease in answer accuracy.\nSpecifically, we achieved an average CAS score\nof 90.7 across three test datasets, which is a crucial\nmetric for assessing the degree of text attribution.\nCompared to the RECLAIM Unified method, the\nRECLAIM w/IG approach\u2019s biggest difference lies\nin the training and inference strategies during the\nclaim generation phase. It filters out extraneous\ncontextual information and trains the LLM to gen-\nerate claims based solely on the preceding refer-\nence part. The significant improvements in citation\nquality demonstrate the effectiveness of the strategy\nadopted by the RECLAIM w/IG method.\nAs shown in Table 4, the results of the ablation\nexperiments indicate that fine-tuning two LLMs\nfor alternating generation of references and claims\nachieves the most balanced performance.\nWhile ReferModel-Only w/Sum method yields\na high citation quality score, it compromises the\n",
        "score": "Yes"
    },
    {
        "citation_text": "Bohnet et al., 2022",
        "paper_id": "1",
        "raw_claim": " Li et al. (2023) offer a comprehensive survey of attribution mechanisms in LLMs, highlighting challenges such as ambiguous knowledge reservoirs and the need for comprehensive attribution, which underscores the motivation for more robust generation techniques.",
        "paper_content": "Attributed Question Answering:\nEvaluation and Modeling for Attributed Large Language Models\nBernd Bohnet\u2217\nVinh Q. Tran\u2217\nPat Verga\u2217\nRoee Aharoni\nDaniel Andor\nLivio Baldini Soares\nMassimiliano Ciaramita\nJacob Eisenstein\nKuzman Ganchev\nJonathan Herzig\nKai Hui\nTom Kwiatkowski\nJi Ma\nJianmo Ni\nLierni Sestorain Saralegui\nTal Schuster\nWilliam W. Cohen\nMichael Collins\nDipanjan Das\nDonald Metzler\nSlav Petrov\nKellie Webster\u2020\nGoogle Research\nAbstract\nLarge language models (LLMs) have shown\nimpressive results while requiring little or no\ndirect supervision.\nFurther, there is mount-\ning evidence that LLMs may have potential in\ninformation-seeking scenarios. We believe the\nability of an LLM to attribute the text that it\ngenerates is likely to be crucial in this setting.\nWe formulate and study Attributed QA as a\nkey \ufb01rst step in the development of attributed\nLLMs. We propose a reproducible evaluation\nframework for the task and benchmark a broad\nset of architectures. We take human annota-\ntions as a gold standard and show that a corre-\nlated automatic metric is suitable for develop-\nment.1 Our experimental work gives concrete\nanswers to two key questions (How to measure\nattribution?, and How well do current state-of-\nthe-art methods perform on attribution?), and\ngive some hints as to how to address a third\n(How to build LLMs with attribution?).\n1\nIntroduction\nLarge language models (LLMs) have shown im-\npressive results across a variety of natural language\nunderstanding and generation tasks (Devlin et al.,\n2019; Raffel et al., 2020; Brown et al., 2020; Rae\net al., 2021; Zhang et al., 2022; Chowdhery et al.,\n2022; Chung et al., 2022) while requiring little\nor no direct supervision,2 instead using few-shot\n(Brown et al., 2020) or in-context learning (Xie\n1We publicly release all system responses and their\nhuman and automatic ratings, at https://github.com/\ngoogle-research-datasets/Attributed-QA\n\u2217Equal contribution.\n\u2020 Final author.\n2By \u201cdirect supervision\u201d we refer to labeled examples\nfor the speci\ufb01c task in mind, for example datasets such as\nthe Natural Questions corpus (Kwiatkowski et al., 2019) for\nquestion answering. We use the term \u201cdirect supervision\u201d\nto distinguish this form of supervision from the term \u201cself\nsupervision\u201d sometimes used in the context of LLMs.\nSystem Input\nQuestion:\nwhat is the order of the net\ufb02ix marvel\nshows?\nSystem Output\nAnswer:\nDaredevil, Jessica Jones, Luke Cage,\nIron Fist, The Defenders, The Punisher\nAttribution:\n(URL =\nA deal between Marvel\nand Net\ufb02ix to produce several inter-\nconnected series was announced in\nNovember 2013, with the individual\nseries Daredevil (2015\u20132018),\nJes-\nsica Jones (2015\u20132019), Luke Cage\n(2016\u20132018), and Iron Fist (2017\u20132018)\nculminating in the crossover miniseries\nThe Defenders (2017). A spin-off from\nDaredevil, The Punisher (2017\u20132019),\nwas ordered in April 2016. The series\nwere all \ufb01lmed in New York State,\nforming the state\u2019s largest television\nproduction\ncommitment\nwith\n161\nepisodes between them.\n[https:\n//en.wikipedia.org/wiki/Marvel\u2019\ns_Netflix_television_series]\nFigure 1: In attributed question answering the input to\nthe model is a question, and the output from the model\nis an answer string together with a pointer to a short\nsegment of text that supports that answer.\net al., 2021). There is increasing evidence that\nLLMs may have potential in information-seeking\nscenarios, producing compelling output in scenar-\nios ranging from \u201csimple\u201d question answering (e.g.,\nKwiatkowski et al. (2019); Rajpurkar et al. (2016);\nJoshi et al. (2017)), to long-form question answer-\ning (Amplayo et al., 2022; Stelmakh et al., 2022),\nand information-seeking dialog (Thoppilan et al.,\n2022; Glaese et al., 2022; Shuster et al., 2022;\nNakano et al., 2021). This lack of direct supervi-\nsion is particularly appealing given the dif\ufb01culties\nof constructing labeled datasets for even simple\nquestion answering,3 let alone more complex (but\n3Here we are referring to the traditional approach to data\ncollection for supervised learning, where human raters provide\narXiv:2212.08037v2  [cs.CL]  10 Feb 2023\n\nimportant) tasks such as multi-faceted question an-\nswering or interactive information-seeking dialog.\nIn many information-seeking scenarios, the abil-\nity of an LLM to attribute the text that it gener-\nates is likely to be crucial for both system develop-\ners and users (see Metzler et al. (2021); Rashkin\net al. (2021); Menick et al. (2022); Thoppilan et al.\n(2022), and section 3.1, for a discussion). Ideally,\nan \u201cattributed LLM\u201d would seamlessly provide evi-\ndence snippets that support the text that it generates\nwhere appropriate (speci\ufb01cally, whenever it makes\nstatements about the world, e.g., see Rashkin et al.\n(2021)). While there has been important work in\nthe direction of adding attribution to LLMs (see\nSection 2), we argue that we as a \ufb01eld currently\nhave very limited understanding of the challenge\nand how to make progress. Critical questions are:\n1. How to measure attribution?\n2. How well do current state-of-the-art methods\nperform on attribution? Even for the simplest\npossible information-seeking scenario, simple\nQA, this is not well understood.\n3. How to build LLMs with attribution?\nTo explore these questions, we propose At-\ntributed Question Answering (QA). In our formu-\nlation, the input to the model/system is a question,\nand the output is an (answer, attribution) pair\nwhere answer is an answer string, and attribution\nis a pointer into a \ufb01xed corpus, e.g. of paragraphs.\nThe returned attribution should give supporting evi-\ndence for the answer; for example, it should satisfy\nthe conditions in Rashkin et al. (2021) (see Sec-\ntion 3.1). Figure 1 gives an example.\nOur motivation for studying attribution in QA\nis two-fold.\nFirst, it is perhaps the simplest\ninformation-seeking application, and as such it is\nmore straightforward to evaluate. However, in spite\nof its simplicity, models and experiments for at-\ntributed QA are likely to be highly informative\nto the general goal of building attributed LLMs\n(see Section 3.1 for more discussion). Second, At-\ntributed QA is an interesting task in its own right. It\nhas advantages over existing approaches to evalua-\ntion of question answering systems (see Section 3.1\nand Section 5). Attribution provided by a QA sys-\ntem is likely to be of bene\ufb01t to both system devel-\nlabeled examples. An alternative approach is to use an LLM\nto generate labeled examples that are then rated by humans.\nFor many tasks, this latter approach is considerably simpler.\nopers and users. With this motivation, we make the\nfollowing contributions.\nFirst, we de\ufb01ne a reproducible evaluation\nframework for Attributed QA, using human an-\nnotations as a gold standard. To facilitate progress,\nwe additionally study AutoAIS (Gao et al., 2022),\nan automatic metric that formulates evaluation as\na Natural Language Inference task (Dagan et al.,\n2005; Bowman et al., 2015). We \ufb01nd strong corre-\nlation between the two, making AutoAIS a suitable\nevaluation strategy in development settings.\nFurther, we perform a systematic analysis\nof a broad set of systems based on state-of-\nthe-art components, exploring different architec-\ntures and levels of supervision. While retrieve-\nthen-read architectures are attractive for their\nstrong performance, they typically require a large\namount of data to train and can be resource in-\ntensive.\nWe are excited by the possibility of\npost-hoc attribution of LLM-generated answers\n(though this remains challenging), and end-to-end\nmodeling that makes limited use of QA exam-\nples. We release scored system outputs to fos-\nter further exploration, at https://github.com/\ngoogle-research-datasets/Attributed-QA.\nAs such, our contributions give some concrete\nanswers to questions 1 and 2 above (How to mea-\nsure attribution?, and How well do current state-of-\nthe-art methods perform on attribution?), and give\nsome hints as to how to address question 3 (How\nto build LLMs with attribution?).\n2\nRelated Work\nThis section focuses on a few areas of related work.\n2.1\nQuestion Answering Tasks\nQuestion answering has emerged as a key way\nto discover and demonstrate advances in LLMs.\nReading comprehension asks a model to take as\ninput a question and a passage which possibly con-\ntains an answer to the question, and to extract that\nanswer. Since the seminal work of SQuAD (Ra-\njpurkar et al., 2016), there has been a proliferation\nof reading comprehension datasets developed to\nbenchmark different machine capabilities that are\nimportant for QA (Joshi et al., 2017; Choi et al.,\n2018; Reddy et al., 2019; Rodriguez et al., 2019).\nThe Natural Questions (Kwiatkowski et al.,\n2019) effort provided a large reading comprehen-\nsion dataset based on real information-seeking\n\nquality to similarly distinguish between when they\nare being used as system components and when\nthey are being used for evaluation.\n5\nExperiments\nWe now describe experiments on Attributed QA.\nWe \ufb01rst give technical details, then present system\nresults, before concluding with an analysis of the\nevaluation metrics.\n5.1\nDatasets\nQuestion Set.\nWe evaluate the short-answer\nseeking questions from the validation set of the\nNatural Questions (Kwiatkowski et al., 2019), i.e.\nthose that appear in OpenNQ (Lee et al., 2019).\nAttribution Corpus.\nWe use a snapshot of\nWikipedia from 2021-10-13 to derive C, using Py-\nserini7 to extract paragraphs from each page.\n5.2\nEvaluation Metrics\nWe report three metrics for all experiments.\n(Human) AIS\nThe gold-standard metric is the\nAIS measure assessed by human raters, as de-\nscribed in Section 3.1. Raters are trained using\nrepeated annotations with feedback, until reaching\nhigh performance on the task and we take the ma-\njority vote from 5 raters. Given the cost of human\nrating, we evaluate on 1000 randomly-chosen ques-\ntions and estimate standard errors using two-sided\nbootstrap re-sampling8.\nAutoAIS\nAutoAIS formulates evaluation as a\nNatural Language Inference task that asks a model\nwhether the question and answer are entailed by\nthe provided attribution. We use a T5 (Raffel et al.,\n2020) checkpoint with 11B parameters \ufb01ne-tuned\non a collection of NLI-related tasks (Williams et al.,\n2018; Bowman et al., 2015; Thorne et al., 2018;\nZhang et al., 2019; Khot et al., 2018; Schuster et al.,\n2021). We score a given (premise, hypothesis)\ninput by measuring the output probability when\nforce-decoding the positive label, resulting in a\nscore between 0 (no entailment) and 1 (entailment).\nWe treat values \u22650.5 as indicating valid (answer,\nattribution) predictions.\n7https://pypi.org/project/pyserini/\n8https://docs.scipy.org/doc/scipy/reference/\ngenerated/scipy.stats.bootstrap.html\nArchitecture\nEM\nAutoAIS\nAIS\nRetrieve-then-read\n41.1\n66.3\n65.5 \u00b1 1.5\n+ AutoAIS reranking\n53.3\n-\n71.4 \u00b1 1.4\nPost-hoc-retrieval\n49.5\n53.9\n55.6 \u00b1 1.5\n+ AutoAIS reranking\n49.5\n-\n59.0 \u00b1 1.5\nLow resource\n39.5\n41.9\n48.6 \u00b1 1.6\nLLM-as-retriever\n50.1\n41.5\n46.0 \u00b1 1.6\nTable 1: Results for the highest-AIS systems in each architec-\nture and reranked variants, as outlined in Section 4. With Auto-\nAIS reranking, AutoAIS is used to select attribution passages,\nto assess how good the system is at producing answers which\ncould be attributed. AutoAIS is not reported for reranked\nvariants given its use as a system component.\nExact Match (EM)\nFinally, for comparison to\nprior work, we also report EM9 for the answer\nstring alone, ignoring attribution.\n5.3\nSystem Results\nTable 1 shows results for the systems in each archi-\ntecture class with the best AIS score, with Auto-\nAIS Reranked variants. The most striking result\nis that the systems which perform best on AIS do\nnot necessarily achieve the strongest EM accu-\nracy (cf. Tables 2 and 3). This is discussed below\nin Section 5.5, where we \ufb01nd EM correlates only\nmodestly with human judgment of AIS and has\nimportant limitations for Attributed QA evaluation.\nAt the same time, we note that we did no special\nmodeling to maximize EM score, such as instruc-\ntion tuning (Wei et al., 2021) or chain of thought\nprompting (Wei et al., 2022), and that models tuned\nfor greater EM may also achieve higher AIS scores.\nBest RTR achieves the highest performance\n(p \u226a10\u22125, t = 4.55, in comparison with the\nbest non-RTR system), despite using LLMs with\nrelatively small numbers of parameters (using T5\nXL with 3B parameters, compared to PaLM with\n540B). However, RTR approaches have the short-\ncoming that they require relatively large amounts\nof explicit supervision, for example in the form of\nNQ examples (an open question is whether RTR\nsystems with much less supervision can be devel-\noped). They are also likely to be highly dependent\non the accuracy of the retrieval step.\nIt is encouraging that Best Post-hoc achieves\nrelatively high EM because it requires minimal\namounts of supervision for answer generation (us-\ning prompting). However, these models generally\n9https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\n2ce1574a0c2f5ed65a08e87cc38ad8ceb222b239/t5/\nevaluation/metrics.py#L154\n",
        "score": "Yes"
    },
    {
        "citation_text": "Bohnet et al., 2022",
        "paper_id": "1",
        "raw_claim": " Furthermore, the issue of hallucination is tackled post-hoc by systems like RARR, which automatically finds attribution for existing LM outputs and post-edits them to fix unsupported content (Gao et al., 2022).",
        "paper_content": "Attributed Question Answering:\nEvaluation and Modeling for Attributed Large Language Models\nBernd Bohnet\u2217\nVinh Q. Tran\u2217\nPat Verga\u2217\nRoee Aharoni\nDaniel Andor\nLivio Baldini Soares\nMassimiliano Ciaramita\nJacob Eisenstein\nKuzman Ganchev\nJonathan Herzig\nKai Hui\nTom Kwiatkowski\nJi Ma\nJianmo Ni\nLierni Sestorain Saralegui\nTal Schuster\nWilliam W. Cohen\nMichael Collins\nDipanjan Das\nDonald Metzler\nSlav Petrov\nKellie Webster\u2020\nGoogle Research\nAbstract\nLarge language models (LLMs) have shown\nimpressive results while requiring little or no\ndirect supervision.\nFurther, there is mount-\ning evidence that LLMs may have potential in\ninformation-seeking scenarios. We believe the\nability of an LLM to attribute the text that it\ngenerates is likely to be crucial in this setting.\nWe formulate and study Attributed QA as a\nkey \ufb01rst step in the development of attributed\nLLMs. We propose a reproducible evaluation\nframework for the task and benchmark a broad\nset of architectures. We take human annota-\ntions as a gold standard and show that a corre-\nlated automatic metric is suitable for develop-\nment.1 Our experimental work gives concrete\nanswers to two key questions (How to measure\nattribution?, and How well do current state-of-\nthe-art methods perform on attribution?), and\ngive some hints as to how to address a third\n(How to build LLMs with attribution?).\n1\nIntroduction\nLarge language models (LLMs) have shown im-\npressive results across a variety of natural language\nunderstanding and generation tasks (Devlin et al.,\n2019; Raffel et al., 2020; Brown et al., 2020; Rae\net al., 2021; Zhang et al., 2022; Chowdhery et al.,\n2022; Chung et al., 2022) while requiring little\nor no direct supervision,2 instead using few-shot\n(Brown et al., 2020) or in-context learning (Xie\n1We publicly release all system responses and their\nhuman and automatic ratings, at https://github.com/\ngoogle-research-datasets/Attributed-QA\n\u2217Equal contribution.\n\u2020 Final author.\n2By \u201cdirect supervision\u201d we refer to labeled examples\nfor the speci\ufb01c task in mind, for example datasets such as\nthe Natural Questions corpus (Kwiatkowski et al., 2019) for\nquestion answering. We use the term \u201cdirect supervision\u201d\nto distinguish this form of supervision from the term \u201cself\nsupervision\u201d sometimes used in the context of LLMs.\nSystem Input\nQuestion:\nwhat is the order of the net\ufb02ix marvel\nshows?\nSystem Output\nAnswer:\nDaredevil, Jessica Jones, Luke Cage,\nIron Fist, The Defenders, The Punisher\nAttribution:\n(URL =\nA deal between Marvel\nand Net\ufb02ix to produce several inter-\nconnected series was announced in\nNovember 2013, with the individual\nseries Daredevil (2015\u20132018),\nJes-\nsica Jones (2015\u20132019), Luke Cage\n(2016\u20132018), and Iron Fist (2017\u20132018)\nculminating in the crossover miniseries\nThe Defenders (2017). A spin-off from\nDaredevil, The Punisher (2017\u20132019),\nwas ordered in April 2016. The series\nwere all \ufb01lmed in New York State,\nforming the state\u2019s largest television\nproduction\ncommitment\nwith\n161\nepisodes between them.\n[https:\n//en.wikipedia.org/wiki/Marvel\u2019\ns_Netflix_television_series]\nFigure 1: In attributed question answering the input to\nthe model is a question, and the output from the model\nis an answer string together with a pointer to a short\nsegment of text that supports that answer.\net al., 2021). There is increasing evidence that\nLLMs may have potential in information-seeking\nscenarios, producing compelling output in scenar-\nios ranging from \u201csimple\u201d question answering (e.g.,\nKwiatkowski et al. (2019); Rajpurkar et al. (2016);\nJoshi et al. (2017)), to long-form question answer-\ning (Amplayo et al., 2022; Stelmakh et al., 2022),\nand information-seeking dialog (Thoppilan et al.,\n2022; Glaese et al., 2022; Shuster et al., 2022;\nNakano et al., 2021). This lack of direct supervi-\nsion is particularly appealing given the dif\ufb01culties\nof constructing labeled datasets for even simple\nquestion answering,3 let alone more complex (but\n3Here we are referring to the traditional approach to data\ncollection for supervised learning, where human raters provide\narXiv:2212.08037v2  [cs.CL]  10 Feb 2023\n\nimportant) tasks such as multi-faceted question an-\nswering or interactive information-seeking dialog.\nIn many information-seeking scenarios, the abil-\nity of an LLM to attribute the text that it gener-\nates is likely to be crucial for both system develop-\ners and users (see Metzler et al. (2021); Rashkin\net al. (2021); Menick et al. (2022); Thoppilan et al.\n(2022), and section 3.1, for a discussion). Ideally,\nan \u201cattributed LLM\u201d would seamlessly provide evi-\ndence snippets that support the text that it generates\nwhere appropriate (speci\ufb01cally, whenever it makes\nstatements about the world, e.g., see Rashkin et al.\n(2021)). While there has been important work in\nthe direction of adding attribution to LLMs (see\nSection 2), we argue that we as a \ufb01eld currently\nhave very limited understanding of the challenge\nand how to make progress. Critical questions are:\n1. How to measure attribution?\n2. How well do current state-of-the-art methods\nperform on attribution? Even for the simplest\npossible information-seeking scenario, simple\nQA, this is not well understood.\n3. How to build LLMs with attribution?\nTo explore these questions, we propose At-\ntributed Question Answering (QA). In our formu-\nlation, the input to the model/system is a question,\nand the output is an (answer, attribution) pair\nwhere answer is an answer string, and attribution\nis a pointer into a \ufb01xed corpus, e.g. of paragraphs.\nThe returned attribution should give supporting evi-\ndence for the answer; for example, it should satisfy\nthe conditions in Rashkin et al. (2021) (see Sec-\ntion 3.1). Figure 1 gives an example.\nOur motivation for studying attribution in QA\nis two-fold.\nFirst, it is perhaps the simplest\ninformation-seeking application, and as such it is\nmore straightforward to evaluate. However, in spite\nof its simplicity, models and experiments for at-\ntributed QA are likely to be highly informative\nto the general goal of building attributed LLMs\n(see Section 3.1 for more discussion). Second, At-\ntributed QA is an interesting task in its own right. It\nhas advantages over existing approaches to evalua-\ntion of question answering systems (see Section 3.1\nand Section 5). Attribution provided by a QA sys-\ntem is likely to be of bene\ufb01t to both system devel-\nlabeled examples. An alternative approach is to use an LLM\nto generate labeled examples that are then rated by humans.\nFor many tasks, this latter approach is considerably simpler.\nopers and users. With this motivation, we make the\nfollowing contributions.\nFirst, we de\ufb01ne a reproducible evaluation\nframework for Attributed QA, using human an-\nnotations as a gold standard. To facilitate progress,\nwe additionally study AutoAIS (Gao et al., 2022),\nan automatic metric that formulates evaluation as\na Natural Language Inference task (Dagan et al.,\n2005; Bowman et al., 2015). We \ufb01nd strong corre-\nlation between the two, making AutoAIS a suitable\nevaluation strategy in development settings.\nFurther, we perform a systematic analysis\nof a broad set of systems based on state-of-\nthe-art components, exploring different architec-\ntures and levels of supervision. While retrieve-\nthen-read architectures are attractive for their\nstrong performance, they typically require a large\namount of data to train and can be resource in-\ntensive.\nWe are excited by the possibility of\npost-hoc attribution of LLM-generated answers\n(though this remains challenging), and end-to-end\nmodeling that makes limited use of QA exam-\nples. We release scored system outputs to fos-\nter further exploration, at https://github.com/\ngoogle-research-datasets/Attributed-QA.\nAs such, our contributions give some concrete\nanswers to questions 1 and 2 above (How to mea-\nsure attribution?, and How well do current state-of-\nthe-art methods perform on attribution?), and give\nsome hints as to how to address question 3 (How\nto build LLMs with attribution?).\n2\nRelated Work\nThis section focuses on a few areas of related work.\n2.1\nQuestion Answering Tasks\nQuestion answering has emerged as a key way\nto discover and demonstrate advances in LLMs.\nReading comprehension asks a model to take as\ninput a question and a passage which possibly con-\ntains an answer to the question, and to extract that\nanswer. Since the seminal work of SQuAD (Ra-\njpurkar et al., 2016), there has been a proliferation\nof reading comprehension datasets developed to\nbenchmark different machine capabilities that are\nimportant for QA (Joshi et al., 2017; Choi et al.,\n2018; Reddy et al., 2019; Rodriguez et al., 2019).\nThe Natural Questions (Kwiatkowski et al.,\n2019) effort provided a large reading comprehen-\nsion dataset based on real information-seeking\n\nquality to similarly distinguish between when they\nare being used as system components and when\nthey are being used for evaluation.\n5\nExperiments\nWe now describe experiments on Attributed QA.\nWe \ufb01rst give technical details, then present system\nresults, before concluding with an analysis of the\nevaluation metrics.\n5.1\nDatasets\nQuestion Set.\nWe evaluate the short-answer\nseeking questions from the validation set of the\nNatural Questions (Kwiatkowski et al., 2019), i.e.\nthose that appear in OpenNQ (Lee et al., 2019).\nAttribution Corpus.\nWe use a snapshot of\nWikipedia from 2021-10-13 to derive C, using Py-\nserini7 to extract paragraphs from each page.\n5.2\nEvaluation Metrics\nWe report three metrics for all experiments.\n(Human) AIS\nThe gold-standard metric is the\nAIS measure assessed by human raters, as de-\nscribed in Section 3.1. Raters are trained using\nrepeated annotations with feedback, until reaching\nhigh performance on the task and we take the ma-\njority vote from 5 raters. Given the cost of human\nrating, we evaluate on 1000 randomly-chosen ques-\ntions and estimate standard errors using two-sided\nbootstrap re-sampling8.\nAutoAIS\nAutoAIS formulates evaluation as a\nNatural Language Inference task that asks a model\nwhether the question and answer are entailed by\nthe provided attribution. We use a T5 (Raffel et al.,\n2020) checkpoint with 11B parameters \ufb01ne-tuned\non a collection of NLI-related tasks (Williams et al.,\n2018; Bowman et al., 2015; Thorne et al., 2018;\nZhang et al., 2019; Khot et al., 2018; Schuster et al.,\n2021). We score a given (premise, hypothesis)\ninput by measuring the output probability when\nforce-decoding the positive label, resulting in a\nscore between 0 (no entailment) and 1 (entailment).\nWe treat values \u22650.5 as indicating valid (answer,\nattribution) predictions.\n7https://pypi.org/project/pyserini/\n8https://docs.scipy.org/doc/scipy/reference/\ngenerated/scipy.stats.bootstrap.html\nArchitecture\nEM\nAutoAIS\nAIS\nRetrieve-then-read\n41.1\n66.3\n65.5 \u00b1 1.5\n+ AutoAIS reranking\n53.3\n-\n71.4 \u00b1 1.4\nPost-hoc-retrieval\n49.5\n53.9\n55.6 \u00b1 1.5\n+ AutoAIS reranking\n49.5\n-\n59.0 \u00b1 1.5\nLow resource\n39.5\n41.9\n48.6 \u00b1 1.6\nLLM-as-retriever\n50.1\n41.5\n46.0 \u00b1 1.6\nTable 1: Results for the highest-AIS systems in each architec-\nture and reranked variants, as outlined in Section 4. With Auto-\nAIS reranking, AutoAIS is used to select attribution passages,\nto assess how good the system is at producing answers which\ncould be attributed. AutoAIS is not reported for reranked\nvariants given its use as a system component.\nExact Match (EM)\nFinally, for comparison to\nprior work, we also report EM9 for the answer\nstring alone, ignoring attribution.\n5.3\nSystem Results\nTable 1 shows results for the systems in each archi-\ntecture class with the best AIS score, with Auto-\nAIS Reranked variants. The most striking result\nis that the systems which perform best on AIS do\nnot necessarily achieve the strongest EM accu-\nracy (cf. Tables 2 and 3). This is discussed below\nin Section 5.5, where we \ufb01nd EM correlates only\nmodestly with human judgment of AIS and has\nimportant limitations for Attributed QA evaluation.\nAt the same time, we note that we did no special\nmodeling to maximize EM score, such as instruc-\ntion tuning (Wei et al., 2021) or chain of thought\nprompting (Wei et al., 2022), and that models tuned\nfor greater EM may also achieve higher AIS scores.\nBest RTR achieves the highest performance\n(p \u226a10\u22125, t = 4.55, in comparison with the\nbest non-RTR system), despite using LLMs with\nrelatively small numbers of parameters (using T5\nXL with 3B parameters, compared to PaLM with\n540B). However, RTR approaches have the short-\ncoming that they require relatively large amounts\nof explicit supervision, for example in the form of\nNQ examples (an open question is whether RTR\nsystems with much less supervision can be devel-\noped). They are also likely to be highly dependent\non the accuracy of the retrieval step.\nIt is encouraging that Best Post-hoc achieves\nrelatively high EM because it requires minimal\namounts of supervision for answer generation (us-\ning prompting). However, these models generally\n9https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\n2ce1574a0c2f5ed65a08e87cc38ad8ceb222b239/t5/\nevaluation/metrics.py#L154\n",
        "score": "Yes"
    },
    {
        "citation_text": "Li et al., 2023",
        "paper_id": "3",
        "raw_claim": " Similarly, Citation-Enhanced Generation (CEG) for chatbots uses a post-hoc approach with retrieval and natural language inference to ensure all statements are supported by citations, even regenerating responses if necessary (Li et al., 2024).",
        "paper_content": "A Survey of Large Language Models Attribution\nDongfang Li1\nZetian Sun1\nXinshuo Hu1\nZhenyu Liu1\nZiyang Chen2\nBaotian Hu1\nAiguo Wu1\nMin Zhang1\n1 Harbin Institute of Technology (Shenzhen), Shenzhen, China\n2 Laboratory for Big Data and Decision, National University of Defense Technology, China\n{lidongfang,hubaotian,zhangmin2021}@hit.edu.cn\nAbstract\nOpen-domain generative systems have gained\nsignificant attention in the field of conver-\nsational AI (e.g., generative search engines).\nThis paper presents a comprehensive review of\nthe attribution mechanisms employed by these\nsystems, particularly large language models.\nThough attribution or citation improve the fac-\ntuality and verifiability, issues like ambiguous\nknowledge reservoirs, inherent biases, and the\ndrawbacks of excessive attribution can hinder\nthe effectiveness of these systems. The aim of\nthis survey is to provide valuable insights for\nresearchers, aiding in the refinement of attribu-\ntion methodologies to enhance the reliability\nand veracity of responses generated by open-\ndomain generative systems. We believe that\nthis field is still in its early stages; hence, we\nmaintain a repository to keep track of ongoing\nstudies at https://github.com/HITsz-TMG/\nawesome-llm-attributions.\n1\nIntroduction\nSince the advent of open-domain generative\nsystems\ndriven\nby\nlarge\nlanguage\nmodels\n(LLMs) (Anil et al., 2023; OpenAI, 2022, 2023),\naddressing the coherent generation of potentially\ninaccurate or fabricated content has been a\npersistent challenge (Rawte et al., 2023; Ye et al.,\n2023a; Zhang et al., 2023b). Such issues are often\nreferred to by the community as \u201challucination\u201d\nproblems, wherein the generated content presents\ndistorted or invented facts which lacks credible\nsourcing (Peskoff and Stewart, 2023). This be-\ncomes particularly obvious in information-seeking\nand knowledge question-answering scenarios,\nwhere users rely on large language models for\nexpert knowledge (Malaviya et al., 2023).\nThe essence of the hallucination problem\nmay stem from the fact that pre-trained mod-\nels are sourced from vast, unfiltered real-world\ntexts (Penedo et al., 2023). These human-generated\nVerifiability\nBy User\nLLMs\nAttribution\nFactuality\nBy Model\n\u2022 Fact Verification\n\u2022 Uncertainy Estimation\n\u2022 Auto Evaluation\n\u2022 ...\nSufficiency  -   Precision\nComprehensive - Recall \nGroundedness\nCorrectness\n\u2260\nFigure 1: By providing attribution, both developers and\nusers can view the possible source of an answer and\nevaluate factuality and reliability to form their own as-\nsessment. Attribution as a more realistic way to reduce\nhallucinations bypasses the task of directly determin-\ning the \u201ctruthfulness\u201d of statements, a feat difficult to\nachieve except for the most basic queries.\ntexts inherently contain inconsistencies and false-\nhoods. The objective of pre-training is merely to\npredict the next word, without explicitly model-\ning the veracity of the generated content. Even\nafter utilizing reinforcement learning from human\nfeedback (Ouyang et al., 2022), models can still\nexhibit external hallucinations (Bai et al., 2022).\nTo address the issue of external hallucinations, re-\nsearchers have begun to employ measures like ex-\nternal references to enhance the authenticity and re-\nliability of chatbots (Thoppilan et al., 2022; Menick\net al., 2022; Nakano et al., 2021). The distinc-\ntion between explicit attribution and reinforcement\nlearning lies not only in the need for human ver-\nification and compliance but also in recognizing\nthat generated content might become outdated or\ninvalid over time. As shown in Figure 1, attribu-\ntion can leverage real-time information to ensure\nrelevance and accuracy. However, the fundamen-\ntal challenge of attribution revolves around two\nessential requirements (Liu et al., 2023):\n1. Comprehensive attribution or citation\n(high recall): Every claim and statement (ex-\ncept debatable or subjective text, e.g., ab-\nstained text) made by the model-generated\ncontent should be fully backed by appropriate\narXiv:2311.03731v2  [cs.CL]  14 Dec 2023\n",
        "score": "Yes"
    },
    {
        "citation_text": "Gao et al., 2022",
        "paper_id": "5",
        "raw_claim": "\n\nRecent research also delves into the nuances of attribution and its evaluation. Rashkin et al. (2021) introduced the Attributable to Identified Sources (AIS) framework for assessing whether NLG output is supported by underlying sources, providing a common framework for measuring attribution.",
        "paper_content": "RARR: Researching and Revising What Language Models Say,\nUsing Language Models\nLuyu Gao1\u22c4\u2217\nZhuyun Dai2\u2217\nPanupong Pasupat2\u2217\nAnthony Chen3\u22c4\u2217\nArun Tejasvi Chaganty2\u2217\nYicheng Fan2\u2217\nVincent Y. Zhao2\nNi Lao2\nHongrae Lee2\nDa-Cheng Juan2\nKelvin Guu2\u2217\n1Carnegie Mellon University, 2Google Research, 3UC Irvine\nluyug@cs.cmu.edu\nanthony.chen@uci.edu\n{zhuyundai,ppasupat,arunchaganty,yichengfan,vzhao,nlao,hrlee,dacheng,kguu}@google.com\nAbstract\nLanguage models (LMs) now excel at many\ntasks such as question answering, reasoning,\nand dialog. However, they sometimes gener-\nate unsupported or misleading content. A user\ncannot easily determine whether their outputs\nare trustworthy or not, because most LMs do\nnot have any built-in mechanism for attribu-\ntion to external evidence. To enable attribution\nwhile still preserving all the powerful advan-\ntages of recent generation models, we propose\nRARR (Retrofit Attribution using Research and\nRevision), a system that 1) automatically finds\nattribution for the output of any text genera-\ntion model, and 2) post-edits the output to fix\nunsupported content while preserving the origi-\nnal output as much as possible. When applied\nto the output of several state-of-the-art LMs\non a diverse set of generation tasks, we find\nthat RARR significantly improves attribution\nwhile otherwise preserving the original input to\na much greater degree than previously explored\nedit models. Furthermore, the implementation\nof RARR requires only a handful of training ex-\namples, a large language model, and standard\nweb search.1\n1\nIntroduction\nGenerative language models (LMs) and other text\ngeneration models are now the backbone of many\nAI systems. For example, large language models\ncan perform multi-step reasoning (Nye et al., 2021;\nWei et al., 2022), generate plans (Ahn et al., 2022),\nuse tools and APIs (Shin et al., 2021; Thoppilan\net al., 2022), and answer open-domain questions\n(Petroni et al., 2019; Roberts et al., 2020).\nDespite these incredible advances, state-of-the-\nart LMs still frequently produce biased, misleading,\n\u2217Lead contributors. Please see Contributions section for\ndetails. \u22c4Work done during an internship at Google Research.\n1We release open-source implementations of RARR, the\nevaluation pipeline, and the evaluation sets at https://\ngithub.com/anthonywchen/RARR.\nThe marathon world \nrecord is 2:01:39 2:01:09, \nset by Eliud Kipchoge of \nKenya in 2018 2022.\nResearch\n& Revision\nRevised output,  y\nAttribution report,  A\n[wikipedia.org] \u2026 Kipchoge is a \nKenyan long-distance runner \u2026\n[npr.org] 2022 ... Kipchoge shaved \n30 seconds \u2026 to finish in 2:01:09\nAttribution score\nA \u2192 y\nPreservation score\nx \u2192 y\nHuman / automatic evaluation\nThe marathon world record is \n2:01:39, set by Eliud Kipchoge \nof Kenya in 2018.\nText generation model\nOriginal model \noutput,  x\nDocument \nCorpus\nFigure 1: The Editing for Attribution task. The input\nx is a text passage produced by a generation model.\nOur Research & Revision model outputs an attribution\nreport A containing retrieved evidence snippets, along\nwith a revision y whose content can be attributed to the\nevidence in A while preserving other properties of x\nsuch as style or structure.\nor unsupported content, colloquially called \u201chal-\nlucinations\u201d (Maynez et al., 2020; Menick et al.,\n2022). To make LMs more trustworthy, we want\nto justify each generation by an attribution report\n(Rashkin et al., 2021; Bohnet et al., 2022) that\ncontains supporting evidence from trusted sources\n(e.g., encyclopedia or articles) where appropriate.\nMost existing LMs, such as those based on\nsequence-to-sequence architectures, lack a built-\nin mechanism for attribution.\nEven retrieval-\naugmented models (Guu et al., 2020; Lewis et al.,\n2020), which retrieve relevant documents and then\ncondition on them to generate text, still do not\nguarantee attribution. Prior work has shown that\narXiv:2210.08726v3  [cs.CL]  31 May 2023\n\nFactoid statements\nWe prompt PaLM 540B and\nGPT-3 text-davinci-002 to generate long-form an-\nswers to questions from the Natural Questions dev\nset (NQ; Kwiatkowski et al., 2019). The result-\ning passages are mostly coherent but often contain\nfactual errors. This setup examines the ability to\nattribute a diverse range of factoid knowledge.\nReasoning chains\nLanguage models can gener-\nate reasoning chains to answer complex questions\n(Wei et al., 2022). We use PaLM and GPT-3 to gen-\nerate reasoning chains for the StrategyQA train set\n(SQA; Geva et al., 2021). This setup tests whether\nthe revision model can provide better attribution for\nintermediate steps of reasoning, while preserving\nthe overall reasoning process.\nKnowledge-intensive dialogs\nWe consider the\nconversational QA task from the QReCC dev set\n(Anantha et al., 2021). Given the previous dia-\nlog turns, which are rounds of questions and an-\nswers (Q1, A1, Q2, A2, . . . , Qk), we use LaMDA\nand GPT-3 to answer to the final question Qk condi-\ntioned on the dialog history. The answer tends to be\ncontext-dependent, featuring pronouns and implicit\nreferences. All dialog turns are given alongside the\nanswer as inputs to the revision model.\n5.2\nModels\nWe compare RARR to several systems that have a\nresearch-and-revise workflow.\nEFEC\nWe consider EFEC (Thorne and Vlachos,\n2021) as a representative fine-tuned editor. EFEC\nfine-tunes a T5-based model to revise text condi-\ntioned on multiple evidence snippets using both\nsemi-supervised and fully-supervised approaches.\nWe compare against their fully-supervised ap-\nproach, which performed best in their experiments.\nEFEC uses a neural retrieval model (Karpukhin\net al., 2020) to retrieve from Wikipedia; however,\nnot all passages in our experiments are supported\nby Wikipedia articles. To more fairly compare the\nediting capabilities of EFEC, we instead use the\nevidence retrieved by our research stages (CQGen\nand web search). Note that the EFEC editor condi-\ntions on multiple pieces of evidence at once, while\nour editor iteratively conditions on one at a time.\nLaMDA\nLaMDA (Thoppilan et al., 2022) gener-\nates responses in three steps: 1) generate a \u201cbase\nresponse\u201d; 2) generate search queries from the base\nresponse; 3) generate a \u201crevised response\u201d condi-\ntioned on the base response and retrieved evidence.\nAttribution\nPreservation\nModel\nauto-AIS\nAIS\nintent\nLev\ncomb F1AP\nPaLM outputs on NQ\nEFEC\n45.6 \u219264.3 35.4 \u219248.3\n16.0 39.1\n10.4\n17.1\nLaMDA 39.5 \u219249.9 18.3 \u219230.4\n26.0 39.6\n21.1\n24.9\nRARR\n45.6 \u219254.9 35.4 \u219243.4\n90.0 89.6\n83.1\n57.0\nPaLM outputs on SQA\nEFEC\n37.8 \u219258.6 24.5 \u219251.7\n6.0 31.0\n3.8\n7.1\nLaMDA 32.7 \u219243.2 15.8 \u219227.0\n40.0 46.4\n33.7\n30.0\nRARR\n37.6 \u219245.1 24.5 \u219231.5\n92.6 89.9\n84.6\n45.9\nLaMDA outputs on QReCC\nEFEC\n19.1 \u219247.4 13.2 \u219248.7\n39.7 39.4\n23.7\n31.9\nLaMDA 16.4 \u219236.2 16.0 \u219227.1\n21.3 24.8\n12.0\n16.6\nRARR\n18.8 \u219229.4 13.2 \u219228.3\n95.6 80.2\n78.1\n41.5\nTable 1: Evaluation results. For attribution, we report\nthe AIS scores of the texts both before and after editing\n(before \u2192after). For preservation, we report intent\npreservation Presintent, Levenshtein similarity PresLev,\nand the combined Prescomb. We summarize AttrAIS and\nPrescomb using their harmonic mean (F1AP).\nTo apply LaMDA on a given text x, we simply set\nthe base response in step 1 to x, and then run steps\n2 and 3 (we call these latter two stages \u201cLaMDA\nResearch\u201d). LaMDA was trained as a dialog sys-\ntem, and always expects a dialog context where\nthe user speaks first. So, for non-dialog tasks, we\ninsert an artificial user utterance as dialog history:\n\u201cTell me something interesting.\u201d For the attribution\nreport, we take all evidence documents retrieved\nby LaMDA during its research process.\nRARR\nOur model uses few-shot prompting on\nPaLM 540B for query generation, the agreement\nmodel, and the edit model.\nWe use the same\nprompts for all tasks except when the context\ncomes from a dialog, where we slightly modify the\nprompts to use the dialog context (e.g., CQGen now\nmaps dialog context + x to queries). The query-\nevidence relevance model Srelevance is a pretrained\nT5-large model (Raffel et al., 2020) fine-tuned fol-\nlowing Ni et al. (2021) on MS MARCO (Nguyen\net al., 2016). See Appendix D for the few-shot\nprompting strategies and more modeling details.\n5.3\nResults\nFor the main experiments, we report results on pas-\nsages generated by PaLM and LaMDA. Results on\nGPT-3 passages show similar trends (Appendix A).\nTable 1 and Figure 5 show attribution and preser-\nvation results for each model and dataset. We also\nreport F1AP, the harmonic mean of the two metrics,\nwhich is shown as level curves in Figure 5.\n\nretrieval-augmented models generate text that ei-\nther includes additional information outside the\nretrieved documents (Dziri et al., 2022), ignores\nthe documents altogether (Krishna et al., 2021),\nor even contradicts the documents (Longpre et al.,\n2021). In fact, occasionally ignoring the retrievals\ncan make the models more robust to bad retrievals\n(Khandelwal et al., 2020), illustrating that end-task\nperformance and attribution are not always aligned.\nInstead of constraining LMs to generate at-\ntributed text, we propose a model-agnostic ap-\nproach to improve the attribution of any existing\nLM: Retrofit Attribution using Research and Revi-\nsion (RARR). The approach is inspired by works on\nfact-checking2 where simple research-and-revise\nworkflows are effective at attributing or correct-\ning unattributed claims made by humans (Thorne\net al., 2018; Schuster et al., 2021; Thorne and Vla-\nchos, 2021). As shown in Figure 1, after gener-\nating text with the LM, RARR does research to\nretrieve relevant evidence, and then revises the text\nto make it consistent with the evidence while pre-\nserving qualities like style or structure, enabling\nthe revised text to be seamlessly used in place of\nthe original. RARR can be viewed as a retrieval-\naugmented model where retrieval happens after\ngeneration rather than before. This allows RARR\nto stand on the shoulders of giant LMs without\nhaving to modify them to support attribution.\nIn our effort to expand the scope of Research &\nRevision models to handle the output of arbitrary\nLMs, we make the following contributions. First,\nwe formalize the Editing for Attribution task and\npropose new metrics that evaluate revision models\nnot just on their ability to produce well-attributed\nrevisions, but also on their ability to otherwise pre-\nserve original properties of the text. Second, we\nuse these metrics to benchmark how existing re-\nvision models perform on various types of LM\noutputs such as knowledge-intensive statements,\nreasoning chains, and dialog responses. Finally,\nwe find that existing revision models do not al-\nways generalize across many tasks (and were not\noriginally intended to), and therefore propose a new\nresearch-and-revise model that leverages the power\nof few-shot prompting in large language models to\nrobustly generalize across domains.\n2In this paper, we generally avoid the term \u201cfact-checking\u201d\nother than to reference relevant literature, because we only\naddress attribution, and attribution does not entail correctness.\nEven if a claim is attributed to a particular source, it does not\nguarantee that the source is \u201ccorrect\u201d (Menick et al., 2022).\nQuery Generation\nWhen did Millie \nInbetween premiere?\nWhat channel was Millie \nInbetween on?\nRetrieval\nRetrieval\n[fandom.com]\n\u2026 the first series \npremiered on\n1 October 2014.\n[comedy.co.uk]\nMillie Inbetween. \nCBBC sitcom \nabout a young \u2026\nAgreement\nAgreement\nOutput Attribution \nReport A={e1, .., eM}\nEdit\nEdit skipped\n[fandom.com]\n\u2026 the first series \npremiered on\n1 October 2014.\n[comedy.co.uk]\nMillie Inbetween. \nCBBC sitcom \nabout a young \u2026\nq1\nqN\n[comedy.co.uk]\nMillie Inbetween. \nCBBC sitcom \nabout a young \u2026\n[fandom.com]\n\u2026 the first series \npremiered on\n1 October 2014.\n{e1,j}\n{eN,j}\nMillie Inbetween \npremiered on 24 \nFebruary 2014 \non CBBC.\nInput Passage x\nMillie Inbetween \npremiered on 1 \nOctober 2014 \non CBBC.\nMillie Inbetween \npremiered on 1 \nOctober 2014 \non CBBC.\nOutput Passage y\nFigure 2: An overview of RARR, which improves attri-\nbution for a text passage via Research & Revision. Given\nthe input text passage, the research stage uses a query\ngenerator to raise questions about different aspects of\nthe text. The retriever then searches for evidence to\ninvestigate each query. The revision stage first runs an\nagreement model to detect disagreement between the\ntext and the evidence, then runs an edit model to revise\nthe text if needed. Finally, M evidence snippets are\nselected to form an attribution report.\n2\nTask formulation\nWe propose the task of Editing for Attribution as\nfollows. As Figure 1 shows, the input to the system\nis a text passage x produced by a generation model.\nThe output is a revised text passage y along with\nan attribution report A, which contains evidence\nsnippets e1, . . . , eM that support the content in y.\nOptionally, the attribution report can contain addi-\ntional information such as the alignment between\nevidence snippets and relevant parts in y.\nWe propose to measure the quality of the revised\ntext y and attribution report A along two dimen-\nsions: (1) attribution: how much of the revised\ntext y can be attributed to the evidence in A, and\n(2) preservation: how much the revised text y\npreserves aspects of the original text x.\n2.1\nMeasuring attribution\nPreviously, Rashkin et al. (2021) proposed At-\ntributable to Identified Sources (AIS), a human\nevaluation framework which considers a binary no-\ntion of attribution. Roughly speaking, a text pas-\nsage y is attributable to a set A of evidence if a\n",
        "score": "Yes"
    },
    {
        "citation_text": "Li et al., 2024",
        "paper_id": "15",
        "raw_claim": " Huang et al. (2024) address the challenge of acquiring high-quality attribution data by proposing START, a self-improvement framework that iteratively enhances LLM attribution capabilities through self-constructed synthetic data and fine-grained preference signals.",
        "paper_content": "Accepted at ACL 2024 Main Conference\nCitation-Enhanced Generation for LLM-based Chatbots\nWeitao Li1,2, Junkai Li1,2, Weizhi Ma2,\u2020, Yang Liu1,2,3,\u2020\n1 Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2 Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n3 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China\nAbstract\nLarge language models (LLMs) exhibit pow-\nerful general intelligence across diverse sce-\nnarios, including their integration into chat-\nbots.\nHowever, a vital challenge of LLM-\nbased chatbots is that they may produce hal-\nlucinated content in responses, which signif-\nicantly limits their applicability. Various ef-\nforts have been made to alleviate hallucina-\ntion, such as retrieval augmented generation\nand reinforcement learning with human feed-\nback, but most of them require additional train-\ning and data annotation.\nIn this paper, we\npropose a novel post-hoc Citation-Enhanced\nGeneration (CEG) approach combined with\nretrieval argumentation. Unlike previous stud-\nies that focus on preventing hallucinations dur-\ning generation, our method addresses this is-\nsue in a post-hoc way. It incorporates a re-\ntrieval module to search for supporting docu-\nments relevant to the generated content, and\nemploys a natural language inference-based ci-\ntation generation module. Once the statements\nin the generated content lack of reference, our\nmodel can regenerate responses until all state-\nments are supported by citations. Note that\nour method is a training-free plug-and-play plu-\ngin that is capable of various LLMs. Experi-\nments on various hallucination-related datasets\nshow our framework outperforms state-of-the-\nart methods in both hallucination detection and\nresponse regeneration on three benchmarks.\nOur code and datasets can be found at https:\n//github.com/Tsinghua-dhy/CEG.\n1\nIntroduction\nLarge Language Models (LLMs) have experienced\nrapid development in recent years, which show\npowerful general intelligence in various scenar-\nios (Yue et al., 2023; Singhal et al., 2023). Current\nLLM-based chatbots, epitomized by ChatGPT and\n\u2020 Weizhi Ma (mawz@tsinghua.edu.cn) and Yang Liu (li-\nuyang2011@tsinghua.edu.cn) are corresponding authors.\nFigure 1: An illustration of our method, which adds\ncitations for the generated content. If there are hallu-\ncinations in the generated content, we prompt LLM to\nregenerate a new response.\nGPT-4, demonstrate impressive capabilities across\ndistinct domains in communicating with humans.\nThere is a growing consensus that LLM-based chat-\nbots can be the next generation of information ac-\nquisition methodology.\nHowever, a critical and unsolved challenge of\nLLM-based chatbots is the hallucination prob-\nlem (Ji et al., 2023), which indicates these chatbots\nmay generate hallucinated content in responses ran-\ndomly. As the underlying mechanisms of hallucina-\ntions remain unclear, this problem has substantial\nconstraints on the deployment of LLM-based chat-\nbots in various sensitive scenarios, such as health-\ncare and education, where reliability is paramount.\nPrevious approaches have attempted to mit-\nigate this issue through retrieval augmenta-\ntion (Borgeaud et al., 2022; Izacard et al., 2022) and\narXiv:2402.16063v4  [cs.CL]  17 Apr 2025\n\nJie Huang and Kevin Chen-Chuan Chang. 2023. Ci-\ntation:\nA key to building responsible and ac-\ncountable large language models.\narXiv preprint\narXiv:2307.02185.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874\u2013880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1\u201338.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nR\u00e9mi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1203\u20131213, Austin,\nTexas. Association for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nJunyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and\nJi-Rong Wen. 2023a. HaluEval: A large-scale hal-\nlucination evaluation benchmark for large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6449\u20136464, Singapore. Association for Com-\nputational Linguistics.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter\nPfister, and Martin Wattenberg. 2023b. Inference-\ntime intervention: Eliciting truthful answers from a\nlanguage model. arXiv preprint arXiv:2306.03341.\nYikang Liu, Ziyin Zhang, Wanyang Zhang, Shisen Yue,\nXiaojing Zhao, Xinyuan Cheng, Yiwen Zhang, and\nHai Hu. 2023. Argugpt: evaluating, understanding\nand identifying argumentative essays generated by\ngpt models. arXiv preprint arXiv:2304.07666.\nPotsawee Manakul, Adian Liusie, and Mark Gales. 2023.\nSelfCheckGPT: Zero-resource black-box hallucina-\ntion detection for generative large language models.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n9004\u20139017, Singapore. Association for Computa-\ntional Linguistics.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. arXiv preprint arXiv:2203.11147.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, et al.\n2023.\nTowards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nGuan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li,\nSen Song, and Yang Liu. 2023a. Openchat: Advanc-\ning open-source language models with mixed-quality\ndata. arXiv preprint arXiv:2309.11235.\nXiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing\nZheng, and Xuanjing Huang. 2023b. Hallucination\n\nvalue alignment (RLHF) (Ouyang et al., 2022; Tou-\nvron et al., 2023) in response generation, but these\noften require additional training and extensive data\nannotation. For example, InstructGPT (Ouyang\net al., 2022) utilize RLHF to alleviate hallucina-\ntions in model output, but needs extra training.\nGao et al. (2023a) attempt to reduce hallucination\nthrough adding retrieved related documents and\ncitations before generation, while the pre-hoc way\nof incorporating citations may potentially harm the\nmodel performance, resulting in poor response re-\nsults with hallucinations.\nIn this work, we propose a novel method to al-\nleviate hallucination in LLMs, which leverages re-\ntrieval augmentation and Natural Language Infer-\nence (NLI) technologies to implement Citation-\nEnhanced Generation (CEG) in a post-hoc way.\nFigure 1 is an illustration. Differing from previous\nstudies, the retrieval augmentation module of the\nCEG framework works after generation (post-hoc),\nand CEG prompts the model to regenerate the an-\nswer when necessary. This approach is effective\nand easy to use, which can reduce the hallucination\nin the model\u2019s output for various LLMs. We con-\nduct experiments on distinct hallucination-related\nbenchmarks, including detection and response re-\ngeneration, where our method achieved state-of-\nthe-art performance. Further analyses demonstrate\nthe usefulness of each module on CEG.\nIn summary, the main contributions of our work\ncan be summarized as follows:\n\u2022 We are the first to propose the use of citation\nto alleviate hallucination in a post-hoc way\nwith regeneration.\n\u2022 We design a novel post-hoc citation-enhanced\ngeneration framework combined with retrieval\naugmentation and NLI to avoid hallucinations,\nwhich is flexible for existing LLMs.\n\u2022 Experimental results show that our CEG\nframework achieves the best performance on\nthree hallucination-related benchmarks.\n2\nRelated Work\n2.1\nHallucination Control in LLMs\nGenerative AI has achieved significant advance-\nments, while still facing the hallucination problem.\nExisting strategies can be categorized into major\ntwo types: mitigation during training and mitiga-\ntion during inference. For the first type, LLMs,\nsuch as LLaMA 2 (Touvron et al., 2023), undergo\nextensive training cycles with high-fidelity data\nsources like Wikipedia to bolster factual consis-\ntency in pre-training. Zhou et al. (2023) alleviate\nhallucination during instruction fine-tuning, which\nadopts high quality manually annotated content to\nregulate hallucination. Some studies (Ouyang et al.,\n2022; Touvron et al., 2023) also introduce penalties\nfor nonfactual responses to alleviate hallucination\nin RLHF. However, all these methods need extra\ntraining and annotations.\nOn the other hand, researchers try to deal\nwith the hallucination challenge during inference.\nInference-Time-Intervention (Li et al., 2023b)\nmitigates hallucination by shifting model acti-\nvations along these factuality-related directions\nduring inference. Retrieval-Augmented Genera-\ntion (RAG) (Lewis et al., 2020) has become a preva-\nlent technique in alleviating hallucination by re-\ntrieving reliable documents before generation (Yu,\n2022). While these methods still generate halluci-\nnations due to the lack of post-hoc verification and\nthey are unable to provide citations for verification.\n2.2\nCitation Augmented LLMs\nIn the realm of LLMs, retrieval technology has be-\ncome a crucial component (Zhang et al., 2023b;\nGao et al., 2023b), as it provides related knowledge\nin generating more reliable results (also mitigates\nthe occurrence of hallucinations). Previous studies\npoint out that citation, generated by retrieval mod-\nels, is key to building responsible and accountable\nLLMs (Huang and Chang, 2023).\nExisting citation augmented strategies can be\ndivided into two types:\nparametric and non-\nparametric.\nParametric methods (Taylor et al.,\n2022) refer to information internalized from the\ntraining data, often leading to inaccurate annotated\ndocuments, as the annotation process itself can\ngive rise to hallucinations. Non-parametric meth-\nods (Gao et al., 2023a; Menick et al., 2022; Izac-\nard and Grave, 2021) involve querying relevant in-\nformation and seamlessly integrating the retrieved\ncontent from outside corpus, which provides more\nreliable citations. Thus, most previous studies are\nnon-parametric, but they are pre-hoc based. For ex-\nample, Gao et al. (2023a) adopt retrieval processes\nto facilitate the annotation of documents within\nmodel-generated outputs. Nevertheless, their pre-\nhoc annotation strategy inadvertently escalates the\ncomplexity of a QA task by converting it into a dual\n",
        "score": "Yes"
    },
    {
        "citation_text": "Rashkin et al., 2021",
        "paper_id": "6",
        "raw_claim": " The difficulty of automatically evaluating attribution is further highlighted by AttributionBench, a comprehensive benchmark revealing that even fine-tuned state-of-the-art LLMs struggle with nuanced information and discrepancies between model access and human annotation (Li et al., 2024).",
        "paper_content": "Measuring Attribution in Natural Language\nGeneration Models\nHannah Rashkin\u2217,\u2663\u2666\nGoogle Research\nVitaly Nikolaev\u2217,\u2663\u2660\nGoogle Research\nMatthew Lamm\u2660\nGoogle Research\nLora Aroyo\u2660\nGoogle Research\nMichael Collins\u2660\nGoogle Research\nDipanjan Das\u2660\u2665\nGoogle Research\nSlav Petrov\u2665\nGoogle Research\nGaurav Singh Tomar\u2666\nGoogle Research\nIulia Turc\u2666\nGoogle Research\nDavid Reitter\u2660\u2665\nGoogle Research\nWith recent improvements in natural language generation (NLG) models for various appli-\ncations, it has become imperative to have the means to identify and evaluate whether NLG\noutput is only sharing veri\ufb01able information about the external world. In this work, we present\na new evaluation framework entitled Attributable to Identi\ufb01ed Sources (AIS) for assessing\nthe output of natural language generation models, when such output pertains to the external\nworld. We \ufb01rst de\ufb01ne AIS and introduce a two-stage annotation pipeline for allowing annotators\nto appropriately evaluate model output according to AIS guidelines. We empirically validate\nthis approach on generation datasets spanning three tasks (two conversational QA datasets, a\nsummarization dataset, and a table-to-text dataset) via human evaluation studies that suggest\nthat AIS could serve as a common framework for measuring whether model-generated statements\nare supported by underlying sources. We release guidelines for the human evaluation studies.\n1. Introduction\nLarge, pretrained neural models have advanced Natural Language Generation (NLG)\nperformance across a variety of use cases, including text summarization, translation,\nand dialogue. Yet, generative neural models are known to hallucinate often, lacking\nfaithfulness to underlying sources, for example in summarization or in grounded dia-\nlogue systems. Accurate evaluation with respect to these issues is important.\n\u2217\nEqual contribution. All authors contributed to all parts of the paper. \u2660Led development of the\nconceptual framework. \u2663Led human annotation study. \u2666Contributed to modeling experiments. \u2665\nProvided project leadership and management. E-mail:\n{hrashkin,vitalyn,mrlamm,loraa,mjcollins,dipanjand,slav,gtomar,reitter}@google.com,\niulia@iuliaturc.com\narXiv:2112.12870v2  [cs.CL]  2 Aug 2022\n\n7. Conclusion\nIn this paper, we de\ufb01ne a new evaluation framework called Attributable to Identi\ufb01ed\nSources which allows us to inspect whether information in generated text can be sup-\nported by source documents. We provide formal de\ufb01nitions of AIS and descriptions of\nhow it can be applied to three different NLG tasks (conversational QA, summarization,\nand table-to-text generation). We validate this evaluation framework quantitatively\non human evaluation studies, in which annotators rated the AIS of model output as\npart of a two-stage annotation pipeline. The results of the human evaluation studies\ndemonstrate that high-quality AIS ratings can be obtained empirically. The results shed\nlight on some of the ongoing challenges in training NLG models; having solid AIS is the\nbasis for addressing them.\n26\n\nIn this paper, we develop a framework for the evaluation of attribution, by which\nwe mean the accurate use of source documents to support generated text. Attribution\nis closely related to issues of hallucination and faithfulness (see \u00a72 for discussion). As a\nkey motivating example, consider a dialog with a system that generates responses to a\nuser\u2019s sequence of questions:\nUSER: what was George Harrison\u2019s \ufb01rst solo album?\nSYSTEM: it was \u201cWonderwall Music\u201d, released in November 1968.\nUSER: how old was he when it was released?\nSYSTEM: he was 25 years old\nIf such a system, in addition to generating responses, could attribute its statements\nto source documents, that is, provide suf\ufb01cient and concise evidence for its claims,\nsystem designers and users alike could more readily ascertain the extent to which\nthe information it provides is supported by underlying sources. Prior work in NLG\nspanning diverse use cases such as summarization, dialogue response generation and\ndata-to-text generation have investigated issues of faithfulness and \u201challucination\u201d,\nbut have not provided a uniform and formally expressed framework to measure these\nerrors. We discuss the relationship of our work to related work in \u00a72.\nIn \u00a73, we introduce our evaluation framework, Attributable to Identi\ufb01ed Sources\n(AIS), that can be used to assess whether statements in natural language made by\na system are derivable from a given underlying source. The de\ufb01nition of AIS (see\n\u00a73.3.1) formalizes the meaning of a sentence s in context using the notion of explica-\ntures (Carston 1988; Wilson and Sperber 2004)1, and de\ufb01nes attribution to some back-\nground information source P in terms of an intuitive test, asking whether \u201cAccording to\nP, s\u201d. It also accommodates system outputs whose meaning is uninterpretable. AIS can\nbe used as a pre-condition or in tandem with other metrics or evaluation frameworks\nto assess overall quality. For example, characteristics of the underlying source (such as\n\u201csource quality\u201d), the \ufb02uency of the generated text, and so forth, can be measured using\ncomplementary metrics that are out of scope in this work.\nWe propose speci\ufb01c instantiations of AIS for three NLG tasks (\u00a74): response gen-\neration in a conversational QA setting (as in the example above; responses must be\nattributable to a provided answer document), text summarization (where the summary\nmust be attributable to the source article), and description generation from structured\ntables, or table-to-text (where the description must be attributable to the source table\nand associated metadata). Each domain involves a number of challenges: for example,\nin dialogue systems a key challenge is that the meaning of system responses is highly\ncontextually dependent.\nNext, we establish the feasibility of AIS evaluations via an empirical study through\nconducting human evaluation experiments. We train annotators to evaluate output\ntext from multiple models per task using task-speci\ufb01c instantiations of AIS. We show\nthat in our human evaluation studies, it is possible to achieve a moderate to high\ndegree of inter-annotator agreement (see \u00a74 for more details). We\u2019re also able to observe\ndifferences in model outputs\u2019 AIS scores, following generally expected trends. As part of\nthis work, we release the detailed guidelines for human evaluation. We believe that AIS\n1 For example, in the above dialogue the explicature of \u201che was 25 years old\u201d is \u201cGeorge Harrison was 25\nyears old when \u2018Wonderwall Music\u2019 was released\u201d: the latter explicature is evaluated for attribution.\nNote that this use of explicatures is closely related to prior work on decontextualization (Choi et al. 2021),\nsee \u00a72 for more discussion.\n2\n",
        "score": "Yes"
    },
    {
        "citation_text": "Huang et al., 2024",
        "paper_id": "7",
        "raw_claim": " Phukan et al. (2024) propose a novel method leveraging LLM hidden states for granular attribution in contextual QA, identifying verbatim copied segments and their sources without extensive retraining.",
        "paper_content": "Advancing Large Language Model Attribution through Self-Improving\nLei Huang1, Xiaocheng Feng1,2*, Weitao Ma1, Liang Zhao1, Yuchun Fan3,\nWeihong Zhong1, Dongliang Xu4, Qing Yang4, Hongtao Liu4, Bing Qin1,2\n1 Harbin Institute of Technology, Harbin, China\n2 Peng Cheng Laboratory, Shenzhen, China\n3 Northeastern University, Shenyang, China\n4 Du Xiaoman Science Technology Co., Ltd., Beijing, China\n{lhuang, xcfeng, wtma, lzhao, whzhong, qinb}@ir.hit.edu.cn\nyuchunfan_neu@outlook.com\n{xudongliang, yangqing, liuhongtao01}@duxiaoman.com\nAbstract\nTeaching large language models (LLMs) to gen-\nerate text with citations to evidence sources\ncan mitigate hallucinations and enhance verifi-\nability in information-seeking systems. How-\never, improving this capability requires high-\nquality attribution data, which is costly and\nlabor-intensive. Inspired by recent advances\nin self-improvement that enhance LLMs with-\nout manual annotation, we present START, a\nSelf-Taught AttRibuTion framework for iter-\natively improving the attribution capability of\nLLMs. First, to prevent models from stagnating\ndue to initially insufficient supervision signals,\nSTART leverages the model to self-construct\nsynthetic training data for warming up. To\nfurther improve the model\u2019s attribution abil-\nity, START iteratively utilizes fine-grained pref-\nerence supervision signals constructed from\nits sampled responses to encourage robust,\ncomprehensive, and attributable generation.\nExperiments on three open-domain question-\nanswering datasets, covering long-form QA\nand multi-step reasoning, demonstrate signif-\nicant performance gains of 25.13% on aver-\nage without relying on human annotations and\nmore advanced models. Further analysis re-\nveals that START excels in aggregating infor-\nmation across multiple sources.\n1\nIntroduction\nThe rapid development of large language models\n(LLMs) (OpenAI, 2023; Zhao et al., 2023) has led\nto their prosperity as indispensable tools for infor-\nmation seeking. Despite their remarkable capabil-\nity to generate fluent and informative responses to\nuser queries, LLMs also struggle with hallucina-\ntions (Huang et al., 2023). To facilitate factuality\nverification, recent research (Bohnet et al., 2022)\nhas explored attributed text generation, a paradigm\nthat enables LLMs to generate responses with cita-\ntions. By attributing models\u2019 output to verifiable\n*Corresponding Author\nsources, it can improve the explainability and cred-\nibility of LLM-generated content (Li et al., 2023).\nWhile beneficial, the ability to attribute con-\ntextual sources is not inherent in LLMs. Most\nwork induces LLMs to generate text with citations\nvia in-context learning (Gao et al., 2023), which\nis far from satisfactory (Liu et al., 2023). The\ncurrent winning recipe for accurate attribution in-\nvolves fine-tuning on high-quality attribution re-\nsponses1 (Li et al., 2024). However, acquiring\nsuch data typically requires either manual cura-\ntion (Malaviya et al., 2023), or distilled from the\nmost advanced LLMs (Huang et al., 2024a,b), both\nof which are costly and not scalable, thus limit-\ning the growth of models\u2019 attribution capability.\nOne promising solution is self-improvement (Yuan\net al., 2023), which has demonstrated the poten-\ntial to boost model performance by learning from\nself-generated high-quality samples.\nInspired by this, we aim to explore the poten-\ntial of self-improvement in bootstrapping the at-\ntribution ability of LLMs. However, achieving\nthis goal presents several challenges.\nOne sig-\nnificant challenge lies in the risk of model stag-\nnation during the self-improvement process, pri-\nmarily due to the insufficient supervision signals\nobtained in the early stage. Concretely, consider-\ning the inferior performance of LLMs in handling\nthe attribution task (Gao et al., 2023), generating\nsufficient high-quality attribution responses solely\nthrough sampling proves difficult. This scarcity of\nhigh-quality samples limits the opportunities for\nLLMs to self-improve effectively. Another chal-\nlenge stems from the limitation of weak supervi-\nsion signals. Current self-improvement approaches\n(Yuan et al., 2023) primarily involve supervised\nfine-tuning on high-quality samples while discard-\ning low-quality ones. When applied to LLM attribu-\n1Attribution responses refers to \u201cresponses with in-line\ncitations, e.g., [1][2]\u201d.\narXiv:2410.13298v1  [cs.CL]  17 Oct 2024\n\ntext with citations via fine-grained rewards. CoRR,\nabs/2402.04315.\nLei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu,\nWeihong Zhong, Xiachong Feng, Weijiang Yu, Wei-\nhua Peng, Duyu Tang, Dandan Tu, and Bing Qin.\n2024b. Learning fine-grained grounded citations for\nattributed large language models.\nIn Findings of\nthe Association for Computational Linguistics, ACL\n2024, Bangkok, Thailand and virtual meeting, Au-\ngust 11-16, 2024, pages 14095\u201314113. Association\nfor Computational Linguistics.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2023. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions. CoRR, abs/2311.05232.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nDongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xin-\nshuo Hu, Xuebo Liu, and Min Zhang. 2024. Improv-\ning attributed text generation of large language mod-\nels via preference learning. CoRR, abs/2403.18381.\nDongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu,\nZiyang Chen, Baotian Hu, Aiguo Wu, and Min\nZhang. 2023. A survey of large language models\nattribution. CoRR, abs/2311.03731.\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023.\nEvaluating verifiability in generative search engines.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 7001\u20137025. Association for Computa-\ntional Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net.\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth\nSieber, Mark Yatskar, and Dan Roth. 2023.\nEx-\npertqa: Expert-curated questions and attributed an-\nswers. CoRR, abs/2309.07852.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2023, Sin-\ngapore, December 6-10, 2023, pages 12076\u201312100.\nAssociation for Computational Linguistics.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2022. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, De-\ncember 7-11, 2022, pages 9844\u20139855. Association\nfor Computational Linguistics.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick S. H. Lewis, Barlas Oguz, Edouard Grave,\nWen-tau Yih, and Sebastian Riedel. 2021. The web\nis your oyster - knowledge-intensive NLP against a\nvery large web corpus. CoRR, abs/2112.09924.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D. Manning, Stefano Ermon, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. In Advances in\nNeural Information Processing Systems 36: Annual\nConference on Neural Information Processing Sys-\ntems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD \u201920: The 26th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, CA, USA, August\n23-27, 2020, pages 3505\u20133506. ACM.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: factoid questions meet\nlong-form answers. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 8273\u20138288.\nAssociation for Computational Linguistics.\nYe Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian\nYu, Haitao Mi, and Dong Yu. 2024. Toward self-\nimprovement of llms via imagination, searching, and\ncriticizing. CoRR, abs/2404.12253.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\n\ntion, these high-quality samples provide only weak\nsupervision signals, mainly teaching LLMs on the\nsurface form of attribution (e.g., proper citation\nformat) (Li et al., 2024). Such practice may ne-\nglect the potential of exploring fine-grained signals\nfrom low-quality samples to learn what constitutes\na desirable attribution response.\nTo address these challenges, we present START,\na Self-Taught AttRibuTion framework designed\nto bootstrap the attribution capabilities of LLMs.\nTo prevent models from stagnating early due to\ninsufficient supervision signals, we first leverage\nthe model to self-construct high-quality synthetic\nattribution data (\u00a73.1). The data synthesis process\nfollows reverse attribution thinking: the model\ninitially generates a response to a given query,\nthen breaks it into atomic claims, and finally ran-\ndomly combines them to create synthetic docu-\nments.\nThis process not only simulates multi-\nsource information-seeking scenarios but also en-\nsures precise attribution, as each document can be\ndirectly traced back to the specific claim it origi-\nnated from. These high-quality synthetic data are\nthen utilized for warming up, providing a good\nstarting point for LLMs to self-improve. Further-\nmore, to better explore fine-grained supervision\nsignals for LLM attribution, we introduce an itera-\ntive self-improving recipe (\u00a73.2). Specifically, the\nframework meticulously designs fine-grained re-\nwards tailored for LLM attribution, covering robust-\nness, comprehensiveness, and attributability. By\nscoring multiple candidates through sampling and\nselecting those with the highest holistic rewards\nfor supervised fine-tuning, the framework subse-\nquently utilizes low-quality samples to construct\nfine-grained preference pairs with diverse optimiza-\ntion rewards for preference optimization. This iter-\native process further fosters the self-improvement\nof attribution capabilities.\nWe conduct extensive experiments across three\nopen-domain question-answering datasets, cover-\ning long-form QA and multi-step reasoning. Re-\nsults indicate that START achieves significant per-\nformance gains of 25.13% on average in citation\nquality. Moreover, START successfully achieves\nself-improvement in LLM attribution, showing pro-\ngressive improvements across iterations. Ablation\nstudies confirm that each component significantly\ncontributes to the improvement. Further analysis\nshows that START not only excels in generating su-\nperior attributable responses but also in effectively\naggregating information across multiple sources.\n2\nRelated Work\n2.1\nLarge Language Model Attribution\nAttribution has gained significant attention for en-\nhancing the interpretability and verifiability of\nLLMs (Gao et al., 2023; Li et al., 2023). Recent\nstudies have focused on improving LLM attribu-\ntion in a supervised way. Asai et al. (2023) first\ndistill GPT-4 to collect high-quality attribution data,\naiming to teach the model to generate grounded an-\nswers with citations through self-reflecting. Simi-\nlarly, Huang et al. (2024a) develop a training frame-\nwork starting with distilling ChatGPT, followed\nby designing reward models to teach the LLM to\ngenerate highly supportive and relevant citations.\nAdditionally, Li et al. (2024) model the attribution\ntask from a preference learning perspective, where\nthey first fine-tune the model on human-labeled at-\ntribution datasets and then perform preference op-\ntimization using synthesized preference data. Fur-\nthermore, Huang et al. (2024b) take this further by\nextending the attribution format to a fine-grained\ncitation level, primarily distilled from ChatGPT. It\nenables the model to first ground the fine-grained\nquotes within the context and then condition the\ngeneration process on them. In contrast to these\nmethods, START aims to bootstrap attribution ca-\npability without relying on human-labeled data or\ndistilling from more capable LLMs.\n2.2\nSelf-Improvement for LLMs\nHigh-quality data either human-crafted or distilled\nfrom advanced LLMs has proven effective in en-\nhancing the performance of LLMs. However, ac-\nquiring such high-quality data can be prohibitively\nexpensive. Recently, self-improvement approaches\n(G\u00fcl\u00e7ehre et al., 2023; Yuan et al., 2024), where\nLLMs learn from self-generated samples have\nemerged as a viable solution to compensate for\nthe scarcity of high-quality data. These methods\ntypically involve employing heuristic rules (Zelik-\nman et al., 2022), self-critique (Tian et al., 2024), or\ntraining additional verifiers (Hosseini et al., 2024)\nto assess the quality of model-generated samples.\nSuch practices are particularly effective in rea-\nsoning tasks, e.g., mathematical reasoning, where\nLLMs already demonstrate capable abilities and\ncan receive precise feedback on correctness. How-\never, these advantages are absent in the attribution\ntask, due to its challenging nature. To bridge the\ngap, we take an initial step towards exploring the\npotential of self-improvement in LLM attribution.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Li et al., 2024",
        "paper_id": "16",
        "raw_claim": " Datasets like HAGRID, which is a human-LLM collaborative dataset, are being developed to foster the creation of generative information-seeking models with improved attribution capabilities (Kamalloo et al., 2023).",
        "paper_content": "AttributionBench: How Hard is Automatic Attribution Evaluation?\nYifei Li\nXiang Yue\nZeyi Liao\nHuan Sun\nThe Ohio State University\n{li.14042, yue.149, liao.629, sun.397}@osu.edu\nAbstract\nModern generative search engines enhance the\nreliability of large language model (LLM) re-\nsponses by providing cited evidence. How-\never, evaluating the answer\u2019s attribution, i.e.,\nwhether every claim within the generated re-\nsponses is fully supported by its cited evi-\ndence, remains an open problem. This veri-\nfication, traditionally dependent on costly hu-\nman evaluation, underscores the urgent need\nfor automatic attribution evaluation methods.\nTo bridge the gap in the absence of standard-\nized benchmarks for these methods, we present\nAttributionBench, a comprehensive bench-\nmark compiled from various existing attri-\nbution datasets.\nOur extensive experiments\non AttributionBench reveal the challenges\nof automatic attribution evaluation, even for\nstate-of-the-art LLMs. Specifically, our find-\nings show that even a fine-tuned GPT-3.5 only\nachieves around 80% macro-F1 under a binary\nclassification formulation. A detailed analysis\nof more than 300 error cases indicates that a\nmajority of failures stem from the model\u2019s in-\nability to process nuanced information, and the\ndiscrepancy between the information the model\nhas access to and that human annotators do. 1\n1\nIntroduction\nThe advent of large language models (LLMs) has\nrevolutionized the field of information retrieval and\ngrounded text generation (Brown et al., 2020; Wei\net al., 2021; Chowdhery et al., 2023; Ouyang et al.,\n2022; Peng et al., 2023), leading to the develop-\nment of advanced generative search engines like\nBing Chat, Google Bard, and perplexity.ai. These\nplatforms excel in generating search results in a\nnatural language format, along with references to\nthe source web pages as evidence to support the\ntruthfulness of the generated response (Dziri et al.,\n2022a; Ji et al., 2023). However, whether the evi-\n1Our\ncode\nand\ndatasets\nare\navailable\nat:\nhttps://github.com/OSU-NLP-Group/AttributionBench\nGPT-3.5 (w/ CoT): \u2026 The reference \ndiscusses the population of Thailand and \nits connection to Theravada Buddhism, \nthe war... However, it does not provide \nspecific information or data to support \nthe claim that the population of Thailand \nis about 63 million people.\nFinal judgment: not attributable \u274c\nGround truth: attributable\nClaim: The population of Thailand is \nabout 63 million people. [1]\nReferences:\n[1] Thailand is home to about 63 million \npeople, 95% of these people follow \nTheravada Buddhism and ... Thailand's \nwar on drugs may potentially explain its \ncurrent retention of the death penalty. \nThe manufacture and distribution of \ndrugs ...\nGPT-3.5 (w/ CoT): \u2026 The reference \ndiscusses the concept of causation in fact \nand legal causation in the context of \ncriminal law. ... However, it does not \nexplicitly mention the division of causation \ninto two components..\nFinal judgment: not attributable \u274c\nGround truth: attributable\nClaim: In delict, causation is typically \ndivided into two components: causation in \nfact and legal causation. [4]\nReferences:\n[1] In this section, causation in fact and \nlegal causation are examined as well as \nsituations where the defendant may be \ninsulated from criminal responsibility. ... In \nthis example, \u2026\nExample 1\nExample 2\nClaim\nReferences\nAttribution\nEvaluator\n\u2705Attributable\n\u274cNot Attributable\nAttributionBench\nFigure 1: The illustration of the attribution evaluation task\nand two typical error examples from AttributionBench\ngenerated by GPT-3.5 (w/ CoT). The references are usu-\nally manually extracted from webpages by human annota-\ntors based on what they think is useful. Left: fine-grained\ninformation insensitivity (i.e., the model disregarded or\noverlooked nuanced details in either the claim or the ref-\nerences, as well as failing to do necessary summarization\nor inference from the given references, tasks that humans\nnaturally perform). Right: human-model accessible in-\nformation mismatch (i.e., human annotators can see the\nwhole webpage while the model is only given the extracted\nevidence, leading to different judgments.)\ndence supports the generated responses or not, also\nknown as the attribution of the response to the evi-\ndence, remains an open problem.\nMany efforts (Liu et al., 2023; Kamalloo et al.,\n2023; Malaviya et al., 2023) have recently been\nmade to conduct human evaluation to examine the\nperformance of attribution of various advanced sys-\ntems like Bing Chat and GPT-4. It turns out such\nsystems often produce attribution errors, making\nthem less faithful and trustworthy for practical use.\nHowever, human evaluation is expensive and time-\narXiv:2402.15089v1  [cs.CL]  23 Feb 2024\n\nconsuming.2 Therefore, there is a pressing need for\nefficient and effective methodologies to automati-\ncally assess attribution and detect possible errors.\nTowards this end, there have been efforts made\nto build automatic attribution evaluation models.\nRashkin et al. (2021) proposed the framework of\nAttributable to Identified Sources (AIS), i.e., eval-\nuating whether model-generated responses can be\nverified against given references. Under this frame-\nwork, several approaches were proposed to build\nthe automatic evaluator, including directly utiliz-\ning natural language inference (NLI) models (Gao\net al., 2023a; Bohnet et al., 2022), directly prompt-\ning LLMs like GPT-3.5 and GPT-4 (Yue et al.,\n2023), and fine-tuning smaller LMs like FLAN-\nT5 (Chung et al., 2022) with repurposed data from\nrelated tasks like NLI, fact-checking, and summa-\nrization (Yue et al., 2023). However, these studies\ndefine the attribution evaluation task differently\n(i.e., using different numbers and types of clas-\nsification labels) and choose different datasets to\ntrain and evaluate models, making it impossible to\ndirectly compare their performance and derive sig-\nnificant insights into the challenges of attribution\nevaluation and how to address them in future work.\nTo this end, we take the first step to present a sys-\ntematic benchmark, AttributionBench, for train-\ning and evaluating cutting-edge automatic attribu-\ntion evaluators. Specifically, we meticulously sam-\nple data from 7 different datasets (Malaviya et al.,\n2023; Liu et al., 2023; Bohnet et al., 2022; Chen\net al., 2023; Dziri et al., 2022b; Yue et al., 2023;\nKamalloo et al., 2023) that cover different domains\nof questions and diverse responses and evidence.\nWe unify them into a binary classification format\nwith a label-balanced setting for fair comparison3.\nWe compile them into a training set and two test\nsets for in-distribution (ID) and out-of-distribution\n(OOD) evaluation, respectively. Table 1 shows the\nstatistics of our benchmark.\nWe conduct extensive experiments and analysis on\nour proposed benchmark. Surprisingly, we find\nthat even fine-tuned GPT-3.5 can only get arount\n80% macro-F1 score under both ID and OOD set-\ntings, which is far away from practical use. To\nbetter understand the challenges of this task, we\n2According to Liu et al. (2023), the cost of annotation is\naround $15 per hour, yielding around 15 data examples.\n3There can be more challenging problem formulations; but\nwe will show that even if in this relatively simple formulation,\nthere is still a large room for models to improve.\ncarefully labeled over 300 error cases from GPT-\n3.5 under chain-of-thought (CoT) prompting (Wei\net al., 2022), which generate rationales for the\nmodel\u2019s prediction that can reveal reasons for an\nerror. We find that 1) over 66% errors are caused\nby the model\u2019s insensitivity to fine-grained infor-\nmation (i.e., ignoring or overlooking details like\nfacts, events, numbers, or lack of necessary summa-\nrization and inference from the evidence, tasks that\nhumans naturally perform), and 2) about 26.8%\nof the errors are caused by the mismatch between\ninformation accessible to the model and that acces-\nsible to human annotators (see examples in Figure\n1). This is because the references given to the\nmodels are manually extracted from web pages by\nhuman annotators based on what they think is use-\nful. However, when making the judgment, human\nannotators may be inherently affected by additional\ncontent since they\u2019ve seen the whole webpage. On\nthe other hand, sometimes the supported evidence\nspreads across the entire webpage, making it im-\npossible to locate a short snippet to serve as the\nevidence. These findings provide valuable insights\ninto 1) the challenges for automatic attribution eval-\nuation, 2) the challenges for humans to evaluate at-\ntribution, and 3) how to develop stronger automatic\nattribution evaluation models.\nWe summarize our contributions as follows:\nBenchmark. We propose AttributionBench, a\ncomprehensive benchmark with a unified formula-\ntion for attribution evaluation, which will enable\nthe community to compare different methods fairly\nand track the progress on this important task.\nMethods. We conduct comprehensive experiments\nand show that existing cutting-edge LLMs like\nGPT-4 and fine-tuned GPT-3.5 still cannot perform\nwell on this task.\nAnalysis. Through a series of in-depth error anal-\nyses, we show insights into why automatic attri-\nbution evaluation is difficult and potential future\nwork.\n2\nAttributionBench\nIn this section, we introduce our task formula-\ntion and data collection pipeline for constructing\nAttributionBench.\n\nA\nImplementation Details\nA.1\nMore Info about Source Datasets\nWe introduce each source dataset used in\nAttributionBench and list the statistics in Ap-\npendix Table 5.\nExpertQA (Malaviya et al., 2023)\nExpertQA is\nconstructed by collecting expert-curated questions\nfrom 484 participants spanning 32 fields of study.\nThe same experts are then engaged to evaluate the\ngenerated responses to their own questions, consid-\nering both informativeness and attributionality.\nStanford-GenSearch (Liu et al., 2023)\nStanford-\nGenSearch utilizes statements extracted from di-\nverse sources, including ELI5 (Fan et al., 2019),\nNaturalQuestions (Kwiatkowski et al., 2019), and\nAllSouls examinations, among others. These state-\nments are input into popular and competent search\nengines to obtain responses. Human annotators\nthen evaluate the precision and recall of the cita-\ntions based on these responses.\nAttributedQA (Rashkin et al., 2021)\nThis\ndataset comprises questions selected from the val-\nidation sets of Natural Questions (Kwiatkowski\net al., 2019). Human evaluators are tasked with\ndetermining their attribution in adherence to the\nprinciples outlined in AIS (Rashkin et al., 2021).\nLFQA (Chen et al., 2023)\nLFQA consists of\nannotations for 100 questions randomly selected\nfrom the ELI-5 test set. Annotations were collected\nfrom six different language models (LM) paired\nwith various document sets.\nBEGIN (Dziri et al., 2022b)\nThe BEGIN bench-\nmark evaluates response attribution in dialogue\nsystems, focusing on grounding responses to pro-\nvided background knowledge. It comprises dia-\nlogue turns generated by advanced dialogue sys-\ntems trained on three knowledge-grounded dia-\nlogue benchmarks and lets human annotators at-\ntribute responses to the background knowledge.\nAttrEval-GenSearch (Yue et al., 2023)\nThis\ndataset is constructed by instructing annotators to\nformulate questions across 12 diverse domains and\nobtaining generated responses with citations from\nNew Bing under balanced mode. To facilitate an-\nnotation, only the first sentence accompanied by a\nreference in the generation is considered.\nHAGRID (Kamalloo et al., 2023)\nHAGRID is\na dataset curated through a joint effort between\nLLMs and human evaluators, with queries sourced\nfrom MIRACL (Zhang et al., 2022). The process\ninvolves obtaining responses from various LLMs\nand subsequently enlisting human evaluators to\nassess their verifiability.\nA.2\nLabel Processing\nWe process each dataset into claim-level binary\nclassification data. For the label space, we consider\nthe example as \u201cattributable\u201d if the original label\nis in (\u201cAttributable\u201d, \u201cFully attributable\u201d or \u201cCom-\npletely supported\u201d), and convert all other labels\nsuch as \u201cPartially support\u201d into \u201cNot attributable\u201d.\nA.3\nModels\nZero-shot\nprompting.\nFor\nGPT-3.5\nand\nGPT-4,\nwe\nuse\nOpenAI\u2019s\nofficial\nAPIs\n(gpt-3.5-turbo-1106,\ngpt-4-1106-preview).\n4\nFor other models\u2019 inference, the inputs are\ntokenized and truncated at a maximum of 2048\ntokens. We generate text with a temperature of 0.\nThe prompts for the task of evaluating attribution\nare provided in Appendix Table 6.\nFine-tuning\nFor fine-tuning GPT-3.5, we use\nOpenAI\u2019s official API. 5 For fine-tuning other mod-\nels, our implementation is based on the Hugging-\nface library (Wolf et al., 2020). The training is\nperformed on 8 A100 80GB GPUs with a maxi-\nmum of 2048 tokens. We use a batch size of 32\nand train 2 epochs for all the models. We set the\nlearning rate as 2e-5 and use a cosine learning rate\ndecay with 0.03 warm-up steps. We used BF16,\nTF32, and FSDP (Zhao et al., 2023) to efficiently\ntrain the models.\nB\nError Distribution in Sub-Datasets\nWe show the detailed error distribution in each\nsub-dataset in Figure 4 and Figure 5. Although\n\u201cfine-grained information insensitivity\u201d is the major\nerror type among all datasets, the distribution of the\nrest error types still varies among 7 test sets. More-\nover, the detailed error reasons within \u201cfine-grained\ninformation insensitivity\u201d are also diverse, indicat-\ning the diversity and difference among different\ntest sets.\n4https://platform.openai.com/docs/api-reference/chat.\n5https://platform.openai.com/finetune.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Phukan et al., 2024",
        "paper_id": "17",
        "raw_claim": " For instance, Chain-of-Thought (CoT) reasoning has been adapted to enhance attribution accuracy, focusing the LLM's reasoning process on generating attribution-centric outputs at various granularities (Berchansky et al., 2024).",
        "paper_content": "Peering into the Mind of Language Models: An Approach for Attribution\nin Contextual Question Answering\nAnirudh Phukan, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami\nBalaji Vasan Srinivasan\nAdobe Research, India\n{phukan, shsomasu, apoorvs, koustavag, balsrini}@adobe.com\nAbstract\nWith the enhancement in the field of generative\nartificial intelligence (AI), contextual question\nanswering has become extremely relevant. At-\ntributing model generations to the input source\ndocument is essential to ensure trustworthiness\nand reliability. We observe that when large lan-\nguage models (LLMs) are used for contextual\nquestion answering, the output answer often\nconsists of text copied verbatim from the in-\nput prompt which is linked together with \"glue\ntext\" generated by the LLM. Motivated by this,\nwe propose that LLMs have an inherent aware-\nness from where the text was copied, likely cap-\ntured in the hidden states of the LLM. We intro-\nduce a novel method for attribution in contex-\ntual question answering, leveraging the hidden\nstate representations of LLMs. Our approach\nbypasses the need for extensive model retrain-\ning and retrieval model overhead, offering gran-\nular attributions and preserving the quality of\ngenerated answers. Our experimental results\ndemonstrate that our method performs on par\nor better than GPT-4 at identifying verbatim\ncopied segments in LLM generations and in at-\ntributing these segments to their source. Impor-\ntantly, our method shows robust performance\nacross various LLM architectures, highlighting\nits broad applicability. Additionally, we present\nVERIFIABILITY-GRANULAR1, an attribution\ndataset which has token level annotations for\nLLM generations in the contextual question\nanswering setup.\n1\nIntroduction\nThe surge in the capabilities of Large Language\nModels (LLMs) has revolutionized natural lan-\nguage understanding. Their ability to comprehend\nand generate human-like text has resulted in their\nwidespread adoption across various industries. A\nprominent and pivotal application of these models\n1Dataset is available at https://github.com/\nAnirudh-Phukan/verifiability-granular.\nis question answering, particularly in contextual\nsettings. The ability of LLMs to parse, interpret,\nand respond to queries within a given context has\nfacilitated efficient information retrieval and com-\nprehension.\nDespite the remarkable strides made in contex-\ntual question answering, challenges persist within\nLLMs. While LLMs excel in generating informa-\ntive responses, they often fall short in providing\nexplicit references or attributions to the specific sec-\ntions or sources within the context from which their\nanswers derive (Liu et al., 2023b). The absence of\nthis attribution impedes the ability to verify the\nauthenticity and accuracy of the generated infor-\nmation. Attribution not only enables verification\n(Rashkin et al., 2023) but also increases user trust\nand confidence in the responses generated by these\nmodels, thus fostering their broader acceptance and\nutilization across diverse domains (Bohnet et al.,\n2022).\nExisting methods for attribution typically align\nwithin three primary categories (Li et al., 2023):\nSystems fine-tuned or trained explicitly for attribu-\ntion tasks (Weller et al., 2023; Gao et al., 2023b),\nretrieve-then-read (Chen et al., 2017; Lee et al.,\n2019) and approaches relying on post-generation\nattribution (Gao et al., 2023a; Huo et al., 2023).\nHowever, each of these approaches encounters sub-\nstantial challenges in achieving accurate and granu-\nlar attribution, thereby limiting their effectiveness.\nThe initial category, which consists of systems\ntailored or explicitly trained for attribution tasks,\nposes a significant challenge (Huang and Chang,\n2023). This challenge primarily arises because\nthe training process demands extensive resources,\nincluding time, computational power, and a large\ncorpus of annotated data.\nFurthermore, the continuous development of\nmore sophisticated models requires retraining, per-\npetuating an unending cycle of adaptation. Addi-\ntionally, the need for regression testing to ensure\narXiv:2405.17980v1  [cs.CL]  28 May 2024\n\n\u2022 Our experimental findings demonstrate the ef-\nficacy of our method across various model\nfamilies. This indicates that leveraging hid-\nden layer representations for attribution can\nbe broadly applied across different LLM ar-\nchitectures, highlighting the wide-ranging ap-\nplicability of our approach.\n\u2022 Additionally, we release the VERIFIABILITY-\nGRANULAR dataset, which contains token-\nlevel attributions for LLM generations in the\ncontextual question answering setup.\n2\nRelated Work\nResearchers have employed different approaches\nfor the attribution of generated or identified text\nspans. A shared task was organized by Thorne\net al. (2018) to encourage researchers to build sys-\ntems capable of performing fact verification and\nattribution to the source texts. Evans et al. (2021)\nemphasized the role of source attribution in fos-\ntering truthful and responsible AI. The growing\nconcern of fake news detection in AI-based news\ngeneration has also been considered as an attri-\nbution task (Pomerleau and Rao, 2017; Ferreira\nand Vlachos, 2016). In addition to automatic at-\ntribution, studies on manual attribution have been\nperformed by domain specific individuals (Borel,\n2023; Li et al., 2015). While we discuss about fact\nverification as an attribution task, it is important to\nnote that user interactions have also been found to\nrequire attribution (Dziri et al., 2022). Petroni et al.\n(2022) has highlighted the fact that wikipedia arti-\ncles needs validation and that the citations need to\nbe attributed. On the other hand Sarti et al. (2023b)\nintroduced a python library based on GPT-2 ca-\npable of identifying feature attributions generated\nfrom the Insequence model; capable of performing\nin multilingual settings (Sarti et al., 2023a). Re-\ncently, researchers performed attribution task on\nmultimodal systems as well (Ancona et al., 2017;\nHolzinger et al., 2021; Zhao et al., 2023).\nHowever, what is missing in prior art is the utili-\nsation of the contextual guidance inherently present\nduring generation for the attribution task. In our\nwork, we tackle this problem by utilising the con-\ntextual information encoded during generation by\nthe LLM and attribute spans to semantically rele-\nvant parts of the source document.\nFigure 2: Semi Extractive answers by LLMs\n3\nProblem Statement\nWe observe a phenomenon in contextual question\nanswering using LLMs which results in a com-\nprehensive answer typically characterized by fac-\ntual spans, replicated verbatim from various seg-\nments of the provided context, interwoven with\n\"glue text\". An example of this phenomenon is\nshown in Figure 2. Yang et al. (2023) also note this\npattern and use references to losslessly speed up\nLLM inference.\nBuilding upon the aforementioned observations,\nwe dissect the challenge of attribution in the con-\ntextual generation setting into two distinct yet inter-\nconnected sub-problems. The first sub-problem in-\nvolves the identification of tokens within the output\nthat have been directly copied from the provided\ncontext.\nThe second sub-problem delves deeper into the\nattribution of these identified tokens. This step in-\nvolves mapping these tokens back to their original\npositions within the document. In essence, this im-\nplies tracing back the tokens to the exact sections\nwithin the document from which they were copied\nverbatim.\nBy doing this, we aim to establish a clear flow\nof information from the source document to the\ngenerated output, thereby enabling a more nuanced\nunderstanding of the context generation process.\nThe task at hand involves two distinct sub-\n\nexclusive to a specific model family. This suggests\nthat our approach can be applied broadly across\ndifferent LLM architectures, underscoring its ver-\nsatility.\n8\nLimitations & Future Work\nOne limitation of our work stems from the spe-\ncific nature of the QuoteSum and VERIFIABILITY-\nGRANULAR datasets we utilized for our experi-\nments. The datasets are constructed such that only\nverbatim spans from the document are annotated\nand attributed in the answers. Our method, thus,\nis primarily evaluated on verbatim spans. How-\never, our method is not specifically designed for\nverbatim spans only; it could potentially work with\nparaphrased information as indicated in our exper-\niments with a synthetically paraphrased version\nof Quotesum presented in the Appendix (Section\nA.5).\nIn the future, we intend to employ datasets that\ninclude paraphrased spans. Testing on such data\nwill allow us to assess the method\u2019s effectiveness\nin attributing paraphrased information, identify po-\ntential challenges, and adapt our methodology ac-\ncordingly.\nWe also see potential for our method to be useful\nbeyond the realm of contextual question answering.\nWe tried mapping an LLM generated text span in\nSpanish to its source document in English (Figure\n12).\nInterestingly, our method shows potential for this\nuse case too. We envision applying our approach\nto other tasks that involve information extraction\nand attribution to a given context. This will not\nonly enhance our understanding of the generation\nprocess of LLMs but also extend the applicability\nand value of our method. We are currently limited\nby the availability of datasets to perform detailed\nanalyses of other applications.\nReferences\nMarco Ancona, Enea Ceolini, A. Cengiz \u00d6ztireli, and\nMarkus H. Gross. 2017. A unified view of gradient-\nbased attribution methods for deep neural networks.\nCoRR, abs/1711.06104.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\n2022. Attributed question answering: Evaluation and\nmodeling for attributed large language models. arXiv\npreprint arXiv:2212.08037.\nBrooke Borel. 2023.\nThe Chicago guide to fact-\nchecking. University of Chicago Press.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. arXiv preprint arXiv:1704.00051.\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David\nReitter. 2022. Evaluating attribution in dialogue sys-\ntems: The BEGIN benchmark. Transactions of the\nAssociation for Computational Linguistics, 10:1066\u2013\n1083.\nOwain Evans, Owen Cotton-Barratt, Lukas Finnve-\nden, Adam Bales, Avital Balwit, Peter Wills, Luca\nRighetti, and William Saunders. 2021. Truthful AI:\ndeveloping and governing AI that does not lie. CoRR,\nabs/2110.06674.\nWilliam Ferreira and Andreas Vlachos. 2016. Emer-\ngent: a novel data-set for stance classification. In\nNAACL HLT 2016, The 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nSan Diego California, USA, June 12-17, 2016, pages\n1163\u20131168. The Association for Computational Lin-\nguistics.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\ncent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\net al. 2023a. Rarr: Researching and revising what\nlanguage models say, using language models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 16477\u201316508.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023b. Enabling large language models to generate\ntext with citations. arXiv preprint arXiv:2305.14627.\nAndreas Holzinger, Bernd Malle, Anna Saranti, and\nBastian Pfeifer. 2021. Towards multi-modal causabil-\nity with graph neural networks enabling information\nfusion for explainable AI. Inf. Fusion, 71:28\u201337.\nJie Huang and Kevin Chen-Chuan Chang. 2023. Ci-\ntation:\nA key to building responsible and ac-\ncountable large language models.\narXiv preprint\narXiv:2307.02185.\nSiqing Huo, Negar Arabzadeh, and Charles Clarke.\n2023. Retrieving supporting evidence for generative\nquestion answering. In Annual International ACM\nSIGIR Conference on Research and Development\nin Information Retrieval in the Asia Pacific Region,\npages 11\u201320.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Kamalloo et al., 2023",
        "paper_id": "18",
        "raw_claim": " THOUGHTSCULPT, for example, employs MCTS to explore a search tree of potential solutions, allowing for iterative revision of intermediate outputs (Chi et al., 2024).",
        "paper_content": "HAGRID: A Human-LLM Collaborative Dataset for Generative\nInformation-Seeking with Attribution\nEhsan Kamalloo\u2217\u2020\nAref Jafari\u2217\u2020\nXinyu Zhang\u2020\nNandan Thakur\u2020\nJimmy Lin\u2020\n\u2020 David R. Cheriton School of Computer Science, University of Waterloo\nekamalloo@uwaterloo.ca\nAbstract\nThe rise of large language models (LLMs)\nhad a transformative impact on search, ush-\nering in a new era of search engines that\nare capable of generating search results in\nnatural language text, imbued with citations\nfor supporting sources.\nBuilding genera-\ntive information-seeking models demands\nopenly accessible datasets, which currently\nremain lacking. In this paper, we introduce\na new dataset, HAGRID (Human-in-the-\nloop Attributable Generative Retrieval for\nInformation-seeking Dataset) for building\nend-to-end generative information-seeking\nmodels that are capable of retrieving candi-\ndate quotes and generating attributed expla-\nnations. Unlike recent efforts that focus on\nhuman evaluation of black-box proprietary\nsearch engines, we built our dataset atop\nthe English subset of MIRACL, a publicly\navailable information retrieval dataset. HA-\nGRID is constructed based on human and\nLLM collaboration. We first automatically\ncollect attributed explanations that follow\nan in-context citation style using an LLM,\ni.e.\nGPT-3.5.\nNext, we ask human an-\nnotators to evaluate the LLM explanations\nbased on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst\nfor the development of information-seeking\nmodels with better attribution capabilities.1\n1\nIntroduction\nLarge Language Models (LLMs) have paved the\nway for the emergence of generative information-\nseeking search engines such as Bing Chat, Google\nBard, and perplexity.ai, where search results are\nformulated in natural language text, incorporat-\ning references to the relevant web pages from\nwhich they are derived.\nThis approach aims to\n\u2217Equal Contribution\n1HAGRID is released at https://github.com/\nproject-miracl/hagrid.\nQuestion\nWhat was Octavia E. Butler\u2019s first novel?\nQuotes\n[1] Survivor is a science fiction novel by American\nwriter Octavia E. Butler. First published in 1978 as\npart of Butler\u2019s \u201cPatternist series\u201d...\n[2] Butler\u2019s first work published was \u201cCrossover\u201d\nin the 1971 Clarion Workshop anthology... Starting\nin 1974, Butler worked on a series of novels\nthat would later be collected as the Patternist\nseries... The first novel, \u201cPatternmaster\u201d (1976),\neventually became the last installment in the series\u2019\ninternal chronology...\nAnswer\nOctavia E. Butler\u2019s first novel was \u201cPatternmaster\u201d\nwhich was published in 1976 and was also the first\ninstallment in her \u201cPatternist series\u201d [2].\nInformative? Yes\nAttributable? Yes\nTable 1: An example taken from HAGRID that\nincludes a question along with a list of relevant\npassages (quotes), an answer generated by GPT-\n3.5 (\u00a73.3), and informativeness and attributability\nevaluated by human annotators (\u00a73.4).\nprovide users with contextually rich responses.\nYet, LLMs are known to generate text lacking\nsufficient grounding to knowledge sources (Dziri\net al., 2022; Ji et al., 2023), thereby posing risks\nof misinformation and even worse, hallucina-\ntion (Maynez et al., 2020; Raunak et al., 2021).\nThis problem becomes particularly critical within\nsearch engines where such inaccuracies can erode\nuser trust and potentially spread misinformation\n(Metzler et al., 2021; Shah and Bender, 2022).\nBuilding models that are capable of incorporating\ncitations that link to some supporting evidence is\na vital step toward understanding the behaviour of\nLLMs, allowing users to easily verify the factu-\narXiv:2307.16883v1  [cs.CL]  31 Jul 2023\n\nality of model outputs. The development of such\nmodels further fosters interpretable LLMs and at-\ntributable outputs (Rashkin et al., 2023), thus rein-\nforcing the transparency and reliability of LLMs.\nA significant obstacle in building generative\nsearch models equipped with citations is the lack\nof accessible and openly available datasets. The\ndata used by commercial search engines for train-\ning their generative information-seeking models\nare typically proprietary and not accessible to the\npublic, thereby hindering their widespread use in\nthe open-source community.\nIn this paper, we introduce a new dataset for\ngenerative information-seeking scenarios to ad-\ndress these limitations.\nOur dataset is con-\nstructed on top of MIRACL (Zhang et al., 2022),\nan information retrieval dataset that consists of\ninformation-seeking questions along with a set of\nmanually labeled relevant passages (quotes). We\ncollect attributed explanations for each question\nby eliciting prompts from an LLM, i.e., GPT-\n3.5 (Ouyang et al., 2022), based on the given rel-\nevant passages. The explanations adhere to an in-\ncontext citation style, similar to scientific articles,\nthat references the supporting quotes. We next ask\nhuman annotators to judge the explanations based\non two criteria, (i) informativeness: whether the\nexplanation provides a direct answer to the ques-\ntion, and (ii) attributability: whether the expla-\nnation is attributable to the source passages. We\nname our dataset HAGRID, representing Human-\nin-the-loop Attributable Generative Retrieval for\nInformation-seeking Dataset. An example ques-\ntion along with its relevant passage and the gener-\nated answer is presented in Table 1.\nHAGRID consists of two subsets: training and\ndevelopment, enabling researchers to train and\nevaluate future information-seeking models with\nattribution capabilities. In particular, we seek to\nestablish a dataset for building open-source end-\nto-end search models capable of retrieving can-\ndidate quotes and generating attributable answers\nbased on input queries, which are key ingredients\nin retrieval-augmented generative models (Lewis\net al., 2020; Izacard and Grave, 2021; Borgeaud\net al., 2022). In contrast to existing datasets (Liu\net al., 2023a; Gao et al., 2023), our emphasis on\nboth openness and the integration of human anno-\ntations makes HAGRID a valuable and unique re-\nsource in this area. HAGRID is publicly released\nunder the Apache 2.0 License. We hope that open-\nsourcing of the dataset spurs innovation and fur-\nther advancements in the rapidly growing area of\ngenerative search.\n2\nRelated Work\nExplainability.\nUnderstanding why models be-\nhave in certain ways is crucial in deploying them\nin real-world applications (Doshi-Velez and Kim,\n2017). A common approach for explainability in\nNLP is to provide human-understandable explana-\ntions for particular outputs of a black-box model\n(Camburu et al., 2018). Numerous attempts were\nmade in many language understanding tasks in-\ncluding text classification (Camburu et al., 2018;\nLiu et al., 2019), question answering (Abujabal\net al., 2017; Rajani et al., 2019), fact verification\n(Atanasova et al., 2020; Kotonya and Toni, 2020),\nand summarization (Li et al., 2021) to generate ra-\ntionales that explain models\u2019 outputs. While these\nexplanations are in line with our goal in this pa-\nper, they are not necessarily attributable (Jacovi\nand Goldberg, 2020). Moreover, several bench-\nmarks (DeYoung et al., 2020; Mathew et al., 2021)\nwere proposed to evaluate the generated ratio-\nnales. Towards this goal, Narang et al. (2020) built\na general-purpose T5 model that generates expla-\nnations for its predictions.\nAttributability.\nRashkin et al. (2023) formalize\nan attributable statement to identified sources such\nthat it can be entailed from some underlying cor-\npus by a generic hearer. Thus, attributability is\na specific form of explainability within the con-\nstraints of a given source. WebGPT (Nakano et al.,\n2021) and GopherCite (Menick et al., 2022) are\ntwo recent closed-source models that are capable\nof generating references to their supporting ev-\nidence.\nFrom the data perspective, several QA\ndatasets (Geva et al., 2021; Bohnet et al., 2022)\nprovide pointers to text snippets supporting the\ngold answer.\nMoreover, two recent works (Liu\net al., 2023a; Gao et al., 2023) focus on verifying\ncitations in generated text based on a given set of\nquotes, which closely aligns with our objective in\nthis paper. Specifically, Liu et al. (2023a) focus on\nclosed-source proprietary search engines, whereas\nour goal is to use publicly available data to allow\nfor building open-source end-to-end search mod-\nels. Similarly, ALCE (Gao et al., 2023), a con-\ncurrent work to HAGRID, shares a similar goal,\nalbeit with two notable differences.\nFirst, Gao\net al. (2023) derive questions from QA datasets\n\n2022.\nReframing human-AI collaboration\nfor generating free-text explanations.\nIn\nProceedings of the 2022 Conference of the\nNorth American Chapter of the Association\nfor Computational Linguistics:\nHuman Lan-\nguage Technologies, pages 632\u2013658, Seattle,\nUnited States. Association for Computational\nLinguistics.\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and\nEunsol Choi. 2023.\nA critical evaluation of\nevaluations for long-form question answering.\nIn Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguis-\ntics (Volume 1:\nLong Papers), pages 3225\u2013\n3245, Toronto, Canada. Association for Com-\nputational Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua\nBengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. 2018. HotpotQA:\nA dataset for diverse, explainable multi-hop\nquestion answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2369\u20132380, Brus-\nsels, Belgium. Association for Computational\nLinguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kil-\nian Q. Weinberger, and Yoav Artzi. 2020.\nBERTScore: Evaluating text generation with\nBERT. In International Conference on Learn-\ning Representations.\nXinyu Zhang, Nandan Thakur, Odunayo Ogun-\ndepo,\nEhsan\nKamalloo,\nDavid\nAlfonso-\nHermelo, Xiaoguang Li, Qun Liu, Mehdi\nRezagholizadeh, and Jimmy Lin. 2022. Mak-\ning\na\nMIRACL:\nMultilingual\ninformation\nretrieval across a continuum of languages.\narXiv preprint arXiv:2210.09984.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Berchansky et al., 2024",
        "paper_id": "9",
        "raw_claim": " The concept of process supervision and reward modeling, as seen in OmegaPRM, which uses MCTS to efficiently collect high-quality process supervision data for mathematical reasoning, further underscores the utility of MCTS and fine-grained feedback in guiding LLMs (Luo et al., 2024).",
        "paper_content": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\nGranularity\nMoshe Berchansky\nDaniel Fleischer\nMoshe Wasserblat\nPeter Izsak\nIntel Labs\n{moshe.berchansky,daniel.fleischer,moshe.wasserblat,peter.izsak}@intel.com\nAbstract\nState-of-the-art performance in QA tasks is cur-\nrently achieved by systems employing Large\nLanguage Models (LLMs), however these mod-\nels tend to hallucinate information in their re-\nsponses. One approach focuses on enhancing\nthe generation process by incorporating attribu-\ntion from the given input to the output. How-\never, the challenge of identifying appropriate\nattributions and verifying their accuracy against\na source is a complex task that requires signifi-\ncant improvements in assessing such systems.\nWe introduce an attribution-oriented Chain-of-\nThought reasoning method to enhance the accu-\nracy of attributions. This approach focuses the\nreasoning process on generating an attribution-\ncentric output. Evaluations on two context-\nenhanced question-answering datasets using\nGPT-4 demonstrate improved accuracy and cor-\nrectness of attributions. In addition, the combi-\nnation of our method with finetuning enhances\nthe response and attribution accuracy of two\nsmaller LLMs, showing their potential to out-\nperform GPT-4 in some cases.1\n1\nIntroduction\nText generation from sources, such as informa-\ntion obtained through retrieval, is a key aspect\nof grounded question answering. One way to en-\nsure the credibility of these models is through at-\ntributed text generation. This method pairs the\ngenerated text with supporting evidence, enhanc-\ning the model\u2019s trustworthiness and allowing for\neasier detection of errors.\nThe pairing process can be done in several levels.\nFirstly, we can pair each sentence in our response\nwith a set of passage identifiers (Gao et al., 2023).\nOn a model detailed level, individual spans in the\nanswer can correspond to a specific passage, hence\nattributing specific portions to various passages\n1Our code is publicly available for reproduction: https:\n//github.com/mosheber/cotar.git\nwhat is the title of tears for fears song?\n[1] Johnny Panic and the Bible of Dreams...\n[3] ... Songs from the Big Chair: ...\n[2] Human eyes produce tears...\nQuestion\nLets analyze the relevant spans:\nFrom passage [1]:\n\u00a0 * Johnny Panic and the ...\n\u00a0 * is a song by the British band ...\nFrom passage [3]:\n\u00a0 * international hit singles\n\u00a0 * \"Mothers Talk\", \"Shout\", ....\nThus, the final answer is:\nCoT\n[ 1 Johnny Panic and the Bible of Dreams ] ...\u00a0\nThey also have [ 3 international hit singles ] such as\n[ 3 \"Mothers Talk\", \"Shout\", ... ].\nAnswer\nFigure 1: Usage of CoT for attribution-based answers.\nWe either instruct the model, using fewshot examples,\nor finetune the model, to produce a detailed list of the\nsalient information from each passage. Each entry can\nbe either on the passage, sentence, and even span level.\nFinally, the model produces a coherent and faithful an-\nswer.\n(Schuster et al., 2023). These spans are designed\nto be direct copies from the passages, assuring that\nthe content provided is fully supported. Since not\nall the spans are copied, the model has a sufficient\ndegree of flexibility to produce both a coherent and\nfactually correct answer.\nHowever, these approaches fail at times in two\nprimary ways to cite accurately. On one hand, the\nmodel might focus only a very specific portion of\nthe input, therefore missing relevant sections when\nit constructs the answer. On the other hand, it might\ncite too many passages, some of which might not\nbe relevant to the answer.\nTo solve these, we propose utilizing a Chain-of-\nThought approach (CoT) (Wei et al., 2022b), which\nwe denote as CoTAR, which allows the model to\nperform reasoning over the input passages before\ngenerating the output. In our approach, we instruct\nthe model to extract relevant information from the\npassages as well as specify the form of attribution\narXiv:2404.10513v2  [cs.CL]  26 Nov 2024\n\nto generate. The information can be in the span,\nsentence, or passage level. We inspect how the\nvarious CoT methods affect the model\u2019s output,\nin each of the citation levels. In addition to few-\nshot instruction, we also finetune various models\nusing our method, resulting in models that are com-\npetitive with and even outperform GPT-4 in some\ncases.\nWe can summarize our contributions in this pa-\nper as follows:\n\u2022 We perform rigorous measurements of both the\nanswer quality and citation quality, across multi-\nple models and citation levels.\n\u2022 We show that utilizing CoT reasoning improves\nthe ability of an LLM to produce both better qual-\nity answers, and more precise and faithful cita-\ntions from the source, demonstrated on multiple\nmodels.\n\u2022 We demonstrate that using by finetuning, smaller\nmodels can be competitive with or outperform\nGPT-4 in some cases in answer and citation qual-\nity metrics.\n2\nMethod\n2.1\nAttribution-Oriented Question Answering\nAttribution-oriented question answering can be de-\nfined as a task where, given a query and a set of\nrelevant contextual resources, the objective is to\naccurately answer the question while attributing\nspecific portions or the entire answer to the appro-\npriate contextual sources.\nWe can identify three levels of attribution with\ndifferent granularity levels: Span, Sentence and\nPassage. Attribution levels refer to how citations\nare displayed within an answer, indicating whether\ncomplete passages, individual sentences, or spe-\ncific text segments are credited back to their origi-\nnal sources. Detailed examples for each level are\navailable in Table 5 in the appendix.\n2.2\nChain-of-Thought Attribution Reasoning\nWe propose a multi-step CoT (Wei et al., 2022b)\nreasoning scheme with varying levels of attribution,\nsimilar to the three levels presented in Section 2,\nhypothesizing that this could encourage the model\nto generate more accurate answers. The process\ninvolves identifying the most crucial aspects of the\ngiven context for answering the question, by incor-\nporating direct citations to the referenced parts. We\ndenote the process of granular attribution as CoT\nattribution guidance, or CoT method for short. The\nthree levels are as follows:\n\u2022 Span Guidance: Produce the relevant spans of\ninformation per passage.\n\u2022 Sentence Guidance: For every passage, write\nsentences that summarize how the passage an-\nswers the question.\n\u2022 Passage Guidance: State which passages are\nrelevant for the question.\nWe explore all combinations of CoT methods\nwith citation levels, and asses how each one ef-\nfects the generated answers. A example of all three\nlevels of attribution-guidance is in Table 6 in the\nappendix.\n3\nEvaluation Metrics\nIn order to evaluate the answers, we specify metrics\nof two types. Answer Quality, measuring simi-\nlarity between the predicted answer and the gold\nanswer. Citation Quality, assessing the similarity\nbetween the cited text, the cited passage, and the\ncitations present in the gold answer.\nFor answer quality , we use the n-grams based\nmethod ROUGE-L (RL), and the semantic method\nBERTScore (BERT) (Zhang* et al., 2020). For\nhallucination evaluation, we use the HEM2 model,\nwhich was finetuned on NLI datasets, and predict\nwhether two texts are factually consistent. For ci-\ntation quality, we measure the quality of the cited\ncontent; we propose specific metrics for each of\nthe citation levels. We note that span citations must\nmatch the source passages precisely, while sen-\ntence and passage attributions are not required to\ndo so. We use the SEM-F1 metric proposed by\nSchuster et al. (2023) for n-gram token level sim-\nilarity between the cited content. In addition, we\ninclude the Citation Precision/Recall introduced in\nGao et al. (2023), and combine them into ALCE\nF1, by using the harmonic mean. We also measure\nthe F1 score over passage indices between the cited\npassages and the passages cited by the expected an-\nswer, denoted as DOC F1, and the Correct Span Ci-\ntation Attribution (CSCA) which indicates whether\na predicted span is a direct span from the attributed\npassage.\n4\nExperimental Setup\nIn order to properly manage our experimental setup,\nwe utilized RAGFoundry, as introduced in Fleis-\ncher et al. (2024). This allowed us to create the data\n2vectara/hallucination_evaluation_model\n\nAviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster,\nand Ido Dagan. 2024. Attribute first, then gener-\nate: Locally-attributable grounded text generation.\nPreprint, arXiv:2403.17104.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog appli-\ncations. Preprint, arXiv:2201.08239.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Preprint,\narXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and\nDenny Zhou. 2022b. Chain of thought prompting\nelicits reasoning in large language models. ArXiv,\nabs/2201.11903.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nA\nImplementation Details\nThe prompt is built using 4 fewshot examples, each\ncomposed of a question, a list of passages, and the\nexpected answer in the task\u2019s format. As done by\nSchuster et al. (2023), the prompt for the models is\nthe same as they have specified, with an example\nin Table 7, and the fewshot examples are chosen\nwith the question similarity between the test ques-\ntion and the training example questions. As stated\nabove, the expected answers cite their passages of\norigin in one of the levels described in Table 5. For\nthe CoT setting, a prefix is attached in one of the\nmethods showcased in Table 6. Text generation is\ndone without sampling, with a maximum length\nof 2k tokens. For QuoteSUM, the train and test\nsets are identical to the ones used by Schuster et al.\n(2023). For MSMARCO, we utilize 2750 samples\nfrom the created dataset for training, and a 1000\nfor the test set. We specify the training parameters\nin Table 4.\nB\nModels\nThe weights for the models we have used have\nall been retrieved from HuggingFace, aside from\nGPT-4, for which we use Azure OpenAI:\n\u2022 Mistral 7B: mistralai/Mistral-7B-Instruct-v0.2,\ndistributed under the Apache License 2.0.\n\u2022 Flan-T5: google/flan-t5-xxl, distributed under\nthe Apache License 2.0.\n\u2022 GPT-4: API access using Azure OpenAI, model\ngpt-4-32k-0613.\nC\nAdditional Results\nParameter\nDecoder-Only\nEncoder-Decoder\nMax Target Len.\n-\n4096\nMax Seq. Len.\n8192\n32768\nLora R\n16\n16\nLora \u03b1\n32\n32\nLora Dropout\n0.05\n0.05\nLora Bias\nNone\nNone\nLora Modules\ngate, down, up, q, v, k, o\nq, v\nLR\n5e-5\n2e-4\nLR Scheduler\nLinear\nLinear\nWeight Decay\n0\n0.01\nPrecision\nbfloat16\nbfloat16\nBatch Size\n1\n1\nEpochs\n3\n3\nWarmup Ratio\n0\n0.1\nTable 4: The training parameters used for Lora finetun-\ning for the decoder-only and encoder-decoder models.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Chi et al., 2024",
        "paper_id": "14",
        "raw_claim": " LLMRefine proposes an inference-time optimization method that uses a learned fine-grained feedback model to pinpoint defects and guide iterative refinement via simulated annealing (Xu et al., 2023).",
        "paper_content": "THOUGHTSCULPT: Reasoning with Intermediate Revision and Search\nYizhou Chi and Kevin Yang and Dan Klein\nUC Berkeley\n{yizhouchi,yangk, klein}@berkeley.edu\nAbstract\nWe present THOUGHTSCULPT, a general rea-\nsoning and search method for tasks with out-\nputs that can be decomposed into components.\nTHOUGHTSCULPT explores a search tree of\npotential solutions using Monte Carlo Tree\nSearch (MCTS), building solutions one ac-\ntion at a time and evaluating according to\nany domain-specific heuristic, which in prac-\ntice is often simply an LLM evaluator. Crit-\nically, our action space includes revision ac-\ntions: THOUGHTSCULPT may choose to revise\npart of its previous output rather than continu-\ning to build the rest of its output. Empirically,\nTHOUGHTSCULPT outperforms state-of-the-\nart reasoning methods across three challenging\ntasks: Story Outline Improvement (up to +30%\ninterestingness), Mini-Crosswords Solving (up\nto +16% word success rate), and Constrained\nGeneration (up to +10% concept coverage).\n1\nIntroduction\nWhile large language models (LLMs) such as GPT\n(Brown et al., 2020; OpenAI, 2024), LLaMA (Tou-\nvron et al., 2023a,b), and Claude (Anthropic, 2024)\nare increasingly capable at performing a variety of\nreasoning tasks, recent studies have revealed that\nthe utilization of distinct prompting strategies and\ninstructional guidance can have a notable influence\non the performance of LLMs when tackling identi-\ncal tasks.\nChain-of-Thought (CoT) is a prompting strategy\ndetailed in Wei et al. (2023) that directs LLMs to\nproduce the final task output through intermedi-\nate steps of reasoning, referred to as \"intermedi-\nate thoughts.\" Notably, CoT has demonstrated a\nsubstantial enhancement in the problem-solving\nproficiency of LLMs without necessitating any\nmodel updates. Self-consistency with CoT (CoT-\nSC) (Wang et al., 2023a) proposes to improve out-\nput consistency by generating multiple CoTs and\nselecting the best outcome. Recently, extending\nCoT and CoT-SC, Tree-of-Thoughts (Yao et al.,\n2023a) and Graph-of-Thoughts (Besta et al., 2024)\npropose to shape the reasoning process of LLMs\nas a tree or an arbitrary graph structure. These ap-\nproaches enable LLMs to explore different paths\nof thought and find better outputs by utilizing back-\ntracking and graph-search algorithms. However,\nthese approaches\u2019 reasoning capabilities are often\nlimited by the set of candidates they generate at\nearlier steps. They cannot revise and edit their\noriginal answers continuously in later steps. As\na result, these methods may not be as effective in\naddressing problems that require frequent revision\nand modifications.\nWe propose THOUGHTSCULPT, a tree-based\nframework that emulates human reasoning by en-\nabling LLMs to create interconnected thought net-\nworks. A key feature is its self-revision mech-\nanism, which iteratively improves outputs while\ngenerating new thought nodes. To address the vast\nsearch space in text generation, we use Monte Carlo\nTree Search (MCTS), which efficiently navigates\nthe search space and provides high-quality solu-\ntions, though not necessarily globally optimal. Our\nmethod includes three core modules: the thought\nevaluator, which gives textual and numerical feed-\nback; the thought generator, which produces so-\nlutions based on initial instructions and feedback;\nand the decision simulator, which simulates lines\nof thought within the MCTS process to assess the\npotential value of different paths.\nWe evaluate THOUGHTSCULPT on three chal-\nlenging tasks for state-of-the-art language mod-\nels: Story Outline Improvement, Mini-Crosswords\nSolving, and Constrained Generation.\nThese\ntasks require advanced reasoning skills, varying\ndegrees of exploration, and the ability for self-\nrevision to achieve optimal results.\nCompared\nto state-of-the-art reasoning strategies as base-\nlines, THOUGHTSCULPT exhibits an up to 30%\ninterestingness increase in Story Outline Improve-\narXiv:2404.05966v2  [cs.CL]  15 Feb 2025\n\nwithin the allocated search budget. Figure 5 illus-\ntrates how THOUGHTSCULPT approaches to solve\na crossword puzzle.\nResults\nAs shown in Table 8, THOUGHTSCULPT\nwith MCTS attains the highest letter success rate\nusing GPT-3.5 and the highest word and game suc-\ncess rate using GPT-4; it is also always at least\ncomparable to the best in all cases. With limited\nsearch steps, it is surprising that ToT using GPT-\n4 performs worse than even Self-refine; it turns\nout that a self-revision mechanism is important in\nthis task. THOUGHTSCULPT with MCTS achieves\ncomparable performance to that reported by ToT\n(Yao et al., 2023a) using 100 search steps, despite\nemploying just 20 search steps in our experiment.\n4.3\nConstrained Generation\nCommonGen is a benchmark dataset and a con-\nstrained text generation task designed to evaluate\nLMs\u2019 abilities in generative commonsense reason-\ning (Lin et al., 2020). An example instruction for\nthe task is shown in Appendix A.3. However, cur-\nrently, the coverage test of CommonGen can be\ncompleted with 90% or higher accuracy by many\nLLMs with one-shot prompting. Therefore, we\ninstead test on CommonGen-Hard as introduced\nby (Madaan et al., 2023). Rather than just four\nconcepts, CommonGen-Hard requires models to\ngenerate a sentence with 20-30 concepts.\nMethod Setup\nIn this task, we first provide the\nset of concepts required and the task description for\nthe LM to generate an initial thought node. During\nthe thought evaluation, the LM will be prompted\nto give feedback about the quality of the concepts\nused and whether there are any missing concepts.\nA child node will be generated using the feedback\nalong with the current solution. We set a maxi-\nmum depth of 3 for this task. For each node, both\nTHOUGHTSCULPT and ToT will generate a maxi-\nmum of 3 child candidates.\nMethods\nGPT3.5\nGPT4\nCoT\n44.1\n96.1\nSelf-refine\n70.0\n98.5\nToT\n54.8\n98.8\nTHOUGHTSCULPT (DFS)\n79.6\n99.1\nTHOUGHTSCULPT (MCTS)\n77.9\n99.0\nTable 3: Constrained Generation Results (% Coverage of\nConcepts). THOUGHTSCULPT outperforms all baselines on\nboth base LMs.\nResults\nTable 3 shows that THOUGHTSCULPT\noutperforms all other baselines when using ei-\nther GPT-3.5 or GPT-4 as the base LM. While\nTHOUGHTSCULPT with DFS achieves the highest\ncoverage of 79.6% (GPT-3.5) and 99.1% (GPT-4),\nTHOUGHTSCULPT with MCTS also demonstrates\ncomparable concept coverage of 77.9% using GPT-\n3.5 and 99.0% using GPT-4. While MCTS exhibits\nnotable exploration capabilities, it fails to surpass\nDFS due to the task\u2019s nature, where effective solu-\ntions are abundant as long as generated sentences\ncorrectly integrate assigned concepts. DFS, em-\nploying a greedy approach prioritizing nodes with\nthe highest concept coverage, outperforms MCTS\nin this context. However, solely relying on con-\ncept coverage does not ensure appropriate concept\nutilization. Hence, we conduct an additional evalu-\nation using GPT-4 to determine the preferred out-\nput based on concept coverage and appropriate-\nness. Figure 6, comparing THOUGHTSCULPT with\nMCTS against THOUGHTSCULPT with DFS and\na third baseline (intuitively, representing the case\nwhere neither THOUGHTSCULPT version\u2019s output\nis good), indicates that THOUGHTSCULPT with\nMCTS is significantly favored.\n5\nConclusion\nWe introduce THOUGHTSCULPT, a framework de-\nsigned to empower LLMs to handle complex tasks\nrequiring continuous refinement and reasoning ca-\npabilities, all without necessitating any modifica-\ntions or updates to the underlying model architec-\nture.\nBy harnessing Monte Carlo Tree Search\n(MCTS), THOUGHTSCULPT enables LLMs to ef-\nfectively explore vast search spaces while manag-\ning computational resource costs efficiently. More-\nover, THOUGHTSCULPT facilitates a seamless self-\nrevision process, allowing LLMs to iteratively re-\nfine and improve their outputs without the need\nfor extensive prompt engineering. Through our\nexperiments, we illustrate THOUGHTSCULPT\u2019s po-\ntential across diverse tasks, highlighting its ver-\nsatility and broad applicability. The results un-\nderscore THOUGHTSCULPT\u2019s capacity to enhance\nLLM performance in challenges requiring continu-\nous thought iteration, such as open-ended genera-\ntion, multi-step reasoning, and creative ideation.\n\nLimitations\nWhile THOUGHTSCULPT presents a promising\napproach for reasoning during inference, its re-\nliance on multiple calls to the base language\nmodel incurs a higher computational cost than\nmost sampling methods. Consequently, in scenar-\nios where base language models already demon-\nstrate satisfactory performance, the adoption of\nTHOUGHTSCULPT may not be advisable. However,\nTHOUGHTSCULPT proves beneficial for tasks re-\nquiring intricate reasoning, potential for continual\nimprovement, or when the base language model\u2019s\nperformance is suboptimal. Furthermore, the in-\ncorporation of MCTS enables THOUGHTSCULPT\nto navigate complex search spaces, striking a bal-\nance between exploration and exploitation, and han-\ndling scalability concerns, thereby offering com-\nputational advantages over alternative search algo-\nrithms.\nEthics Statement\nWe affirm that all datasets utilized in our experi-\nments have been appropriately sourced and cited,\nadhering to principles of academic integrity and\nproper attribution.\nOur experiments primarily leverage GPT-3.5 and\nGPT-4 as the base LLMs. These models possess re-\nmarkable capabilities in generating human-like text\nbased on prompts. However, we acknowledge the\nethical concerns surrounding their potential misuse\nfor spreading misinformation, generating harmful\ncontent, or impersonating individuals. We recog-\nnize the imperative for ethical considerations to\ninclude robust mechanisms aimed at preventing\nmisuse and fostering responsible use of these mod-\nels.\nThe purpose of THOUGHTSCULPT is to enhance\nthe reasoning and complex problem-solving capa-\nbilities of Language Models (LMs). However, it is\nessential to acknowledge that THOUGHTSCULPT\ndoes not inherently include mechanisms to prevent\nLMs from generating harmful content. Therefore,\nwe strongly advise anyone utilizing our model to\nexercise caution and be mindful of the potential\nfor misuse. Users must take proactive measures to\nmitigate the risk of harmful content generation by\nimplementing effective safeguards and appropriate\ncontrols.\nReproducibility\nIn our experiments, we aim for transparency and\nreproducibility by utilizing publicly accessible\ndatasets. Furthermore, for the content evaluator\nutilized in the story outline improvement task, we\nemployed Flan-T5, an open-source model. To fa-\ncilitate reproducibility, our codebase will also be\nmade available for reference and validation upon\npublication. However, as we access GPT-3.5 and\nGPT-4 through the OpenAI API, we acknowledge\nthat reproducibility may be affected subject to Ope-\nnAI changing their API.\nReferences\nAnthropic. 2024. [link].\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022.\nTraining\na helpful and harmless assistant with reinforce-\nment learning from human feedback.\nPreprint,\narXiv:2204.05862.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Michal Podstawski, Lukas Gianinazzi,\nJoanna Gajda, Tomasz Lehmann, Hubert Niewiadom-\nski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph\nof thoughts: Solving elaborate problems with large\nlanguage models. Preprint, arXiv:2308.09687.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. Preprint, arXiv:2005.14165.\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan.\n2024. Alphamath almost zero: Process supervision\nwithout process. Preprint, arXiv:2405.03553.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. Preprint, arXiv:2304.05128.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\n",
        "score": "Yes"
    },
    {
        "citation_text": "Luo et al., 2024",
        "paper_id": "10",
        "raw_claim": " Similarly, Self-RAG enhances LLM quality and factuality through adaptive retrieval and self-reflection, using \"reflection tokens\" to guide generation and critique (Asai et al., 2023).",
        "paper_content": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision\nFigure 1 | Example tree structure built with our proposed OmegaPRM algorithm. Each node in the\ntree indicates a state of partial chain-of-thought solution, with information including accuracy of\nrollouts and other statistics. Each edge indicates an action, i.e., a reasoning step, from the last state.\nYellow edges are correct steps and blue edges are wrong.\napproaches remains limited for complex multi-step reasoning problems. Reward models offer a\npromising alternative to verifiers, enabling the reranking of candidate outcomes based on reward\nsignals to ensure higher accuracy. Two primary types of reward models have emerged: Outcome\nReward Models (ORMs; Cobbe et al., 2021; Yu et al., 2024), which provide feedback only at the end of\nthe problem-solving process, and Process Reward Models (PRMs; Li et al., 2023; Lightman et al., 2023;\nUesato et al., 2022), which offer granular feedback at each reasoning step. PRMs have demonstrated\nsuperior effectiveness for complex reasoning tasks by providing such fine-grained supervision.\nThe primary bottleneck in developing PRMs lies in obtaining process supervision signals, which\nrequire supervised labels for each reasoning step. Current approaches rely heavily on costly and labor-\nintensive human annotation (Lightman et al., 2023; Uesato et al., 2022). Automating this process\nis crucial for scalability and efficiency. While recent efforts using per-step Monte Carlo estimation\nhave shown promise (Wang et al., 2024a,b), their efficiency remains limited due to the vast search\nspace. To address this challenge, we introduce OmegaPRM, a novel divide-and-conquer Monte Carlo\nTree Search (MCTS) algorithm inspired by AlphaGo Zero (Silver et al., 2017) for automated process\nsupervision data collection. For each question, we build a Monte Carlo Tree, as shown in Fig. 1,\nwith the details explained in \u00a73.3. This algorithm enables efficient collection of over 1.5 million\nhigh-quality process annotations without human intervention. Our PRM, trained on this dataset\nand combined with weighted self-consistency decoding, significantly improves the performance of\ninstruction-tuned Gemini Pro from 51% to 69.4% on MATH500 (Lightman et al., 2023) and from\n86.4% to 93.6% on GSM8K (Cobbe et al., 2021). We also boosted the success rates of Gemma2 27B\nfrom 42.3% to 58.2% on MATH500 and from 74.0% to 92.2% on GSM8K.\n2\n\n2024-12-10\nImprove Mathematical Reasoning in Language\nModels by Automated Process Supervision\nLiangchen Luo1*, Yinxiao Liu1*, Rosanne Liu1, Samrat Phatale1, Meiqi Guo1, Harsh Lara1, Yunxuan Li2, Lei\nShu1, Yun Zhu1, Lei Meng2, Jiao Sun2 and Abhinav Rastogi1\n1Google DeepMind, 2Google\nComplex multi-step reasoning tasks, such as solving mathematical problems or generating code, remain\na significant hurdle for even the most advanced large language models (LLMs). Verifying LLM outputs\nwith an Outcome Reward Model (ORM) is a standard inference-time technique aimed at enhancing\nthe reasoning performance of LLMs. However, this still proves insufficient for reasoning tasks with a\nlengthy or multi-hop reasoning chain, where the intermediate outcomes are neither properly rewarded\nnor penalized. Process supervision addresses this limitation by assigning intermediate rewards during\nthe reasoning process. To date, the methods used to collect process supervision data have relied on\neither human annotation or per-step Monte Carlo estimation, both prohibitively expensive to scale,\nthus hindering the broad application of this technique. In response to this challenge, we propose a\nnovel divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named OmegaPRM for\nthe efficient collection of high-quality process supervision data. This algorithm swiftly identifies the\nfirst error in the Chain of Thought (CoT) with binary search and balances the positive and negative\nexamples, thereby ensuring both efficiency and quality. As a result, we are able to collect over 1.5 million\nprocess supervision annotations to train Process Reward Models (PRMs). This fully automated process\nsupervision alongside the weighted self-consistency algorithm is able to enhance LLMs\u2019 math reasoning\nperformances. We improved the success rates of the instruction-tuned Gemini Pro model from 51%\nto 69.4% on MATH500 and from 86.4% to 93.6% on GSM8K. Similarly, we boosted the success rates\nof Gemma2 27B from 42.3% to 58.2% on MATH500 and from 74.0% to 92.2% on GSM8K. The entire\nprocess operates without any human intervention or supervision, making our method both financially\nand computationally cost-effective compared to existing methods.\n1. Introduction\nDespite the impressive advancements achieved by scaling Large Language Models (LLMs) on es-\ntablished benchmarks (Wei et al., 2022a), cultivating more sophisticated reasoning capabilities,\nparticularly in domains like mathematical problem-solving and code generation, remains an active\nresearch area. Chain-of-thought (CoT) generation is crucial for these reasoning tasks, as it decom-\nposes complex problems into intermediate steps, mirroring human reasoning processes. Prompting\nLLMs with CoT examples (Wei et al., 2022b) and fine-tuning them on question-CoT solution pairs\n(Ouyang et al., 2022; Perez et al., 2021) have proven effective, with the latter demonstrating superior\nperformance. Furthermore, the advent of Reinforcement Learning with Human Feedback (RLHF;\nOuyang et al., 2022) has enabled the alignment of LLM behaviors with human preferences through\nreward models, significantly enhancing model capabilities.\nBeyond prompting and further training, developing effective decoding strategies is another crucial\navenue for improvement. Self-consistency decoding (Wang et al., 2023) leverages multiple reasoning\npaths to arrive at a voted answer. Incorporating a verifier, such as an off-the-shelf LLM (Huang\net al., 2022; Luo et al., 2023), can further guide LLMs in reasoning tasks by providing a feedback\nloop to verify final answers, identify errors, and suggest corrections. However, the gain of such\n*Equal contributors. Correspondence to {luolc,canoee}@google.com\n\u00a9 2024 Google DeepMind. All rights reserved\narXiv:2406.06592v2  [cs.CL]  11 Dec 2024\n",
        "score": "Yes"
    },
    {
        "citation_text": "Xu et al., 2023",
        "paper_id": "13",
        "raw_claim": " The challenge of semantic drift, where LLMs generate correct facts initially before drifting to incorrect ones, highlights the need for continuous monitoring and guidance during generation (Spataru et al., 2024).",
        "paper_content": "LLMRefine: Pinpointing and Refining Large Language Models via\nFine-Grained Actionable Feedback\nWenda Xu,\u2217\u2020 Daniel Deutsch,\u2021 Mara Finkelstein,\u2021 Juraj Juraska,\u2021 Biao Zhang,\u2021\nZhongtao Liu,\u2021 William Yang Wang,\u2020 Lei Li,\u00b6 and Markus Freitag\u2021\n\u2020University of California, Santa Barbara, \u2021Google, \u00b6Carnegie Mellon University\nwendaxu@cs.ucsb.edu, {dandeutsch,freitag}@google.com\nAbstract\nRecent large language models (LLM) are\nleveraging human feedback to improve their\ngeneration quality. However, human feedback\nis costly to obtain, especially during inference.\nIn this work, we propose LLMRefine, an\ninference time optimization method to refine\nLLM\u2019s output.\nThe core idea is to use\na learned fine-grained feedback model to\npinpoint defects and guide LLM to refine\nthem iteratively.\nUsing original LLM as a\nproposal of edits, LLMRefine searches for\ndefect-less text via simulated annealing, trading\noff the exploration and exploitation.\nWe\nconduct experiments on three text generation\ntasks, including machine translation, long-\nform question answering (QA), and topical\nsummarization.\nLLMRefine consistently\noutperforms all baseline approaches, achieving\nimprovements up to 1.7 MetricX points on\ntranslation tasks, 8.1 ROUGE-L on ASQA, 2.2\nROUGE-L on topical summarization.\n1\nIntroduction\nIn recent years, large language models (LLMs)\nhave shown impressive performance on various text\ngeneration tasks (Brown et al., 2020; Anil et al.,\n2023). Critical to their success has been the ability\nto incorporate human feedback into the learning\nprocess (Ouyang et al., 2022).\nNevertheless, human feedback is costly to\ncollect, especially at inference time when the model\nprovides new, unseen input. In the meanwhile,\nautomatic text generation evaluation metrics for\na variety of tasks are rapidly improving (Sellam\net al., 2020; Xu et al., 2022b; Rei et al., 2020; Xu\net al., 2023a,b). Can we use one of these metrics to\nrectify LLM\u2019s generation?\nIn this work, we propose LLMRefine, an\ninference-time optimization method to improve the\nquality of generated text. Our LLMRefine starts\n\u2217Work done during a Google internship\nInput: Translate \"\u4e00\u4e2a\u9910\u7b49\u4e86\u4e00\u4e2a\u534a\u5c0f\u65f6\u3002\" into English.\nLLM's output:\nA meal had been waiting for an hour and a half\nExisting feedback methods:\n\"Improve\" feedback: Improve current translation\nScalar feedback: Translation is 70/100\nBinary feedback: Translation contains errors\nLLMRefine's fine-grained feedback:\n\"A meal has been waiting\" is a major mistranslation error\nLLM's proposal:\nA meal waited an hour and a half.\nRevised\nGeneration\nReject: obtain a new sample from\nprevious generation and feedback\nAccept the revised generation\nRepeat above steps for n iterations\nLLM's final output:\nI've waited one and half hours for one meal.\nFigure 1: An overview of our LLMRefine: We start\nfrom LLM\u2019s initial generation and iteratively refine the\ngeneration, based on fine-grained actionable feedback.\nWe use a simulated annealing technique to accept or\nreject the proposed revision at each step.\nwith LLM\u2019s initial output, then uses a learned error\npinpoint model to provide fine-grained feedback\nabout the location and type of defects in the\ntext. We then use a refinement model (same or\nanother LLM) to follow the feedback instruction\nand generate candidate text.\nThe fine-grained\nfeedback provides more much precise information\nabout what exactly is wrong in the generated text,\nresulting in higher quality revision.\nHowever, due to the large search space, the\nrefinement model is imperfect; it often fails to\ncorrect all of the errors identified by the feedback\nmodel in one iteration (Madaan et al., 2023). We\nformulate the iterative refinement procedure into\na local search problem. It alternates between the\narXiv:2311.09336v5  [cs.CL]  25 Oct 2024\n\nfeedback generation and refinement in multiple\niterations, with the goal of searching for the highest\nscoring output according to the feedback model.\nTo this end, we develop a simulated annealing\ntechnique in LLMRefine to trade off between\nexploring many possible edits and quickly reaching\noptimal text.\nFigure 1 shows overview of our\napproach.\nWe evaluate LLMRefine on three text generation\ntasks,\nincluding machine translation (WMT\n(Kocmi\net\nal.,\n2022)),\nlong-form\nquestion\nanswering (ASQA (Stelmakh et al., 2022)) and\ntopic summarization (Saunders et al., 2022),\nbecause they have a large number of annotated\noutputs with fine-grained error spans (Freitag et al.,\n2021a; Saunders et al., 2022; Wu et al., 2023).\nWe use those ratings to train an automatic error\npinpoint model that generates a list of error spans\nalong with error categories and severities without\nthe aid of a reference text (which is unavailable\nduring inference) (Fernandes et al., 2023; Xu et al.,\n2023b). This model serves as our substitute for\nhuman feedback.\nOur experiments show that\nLLMRefine results in higher-quality text compared\nto baseline methods using other feedback (scalar\nor binary score) or other search techniques. Our\ncontributions are:\n\u2022 We propose LLMRefine, an inference time\noptimization method to iteratively refine\nLLM\u2019s output with fine-grained actionable\nfeedback, achieving best trade-off between\nsearch space and optimal quality.\n\u2022 We demonstrate that LLMRefine consistently\noutperforms\nall\nbaseline\napproaches,\nachieving improvements up to 1.7 MetricX\npoints on translationn tasks, 8.1 ROUGE-L on\nASQA and 2.2 ROUGE-L improvements on\ntopical summarization. Humans demonstrate\na significant preference for the output of\nLLMRefine over the baseline outputs.\n2\nRelated Work\nInference-time Optimization Approach\nWe\ndivide techniques for incorporating feedback at\ninference time into two main techniques (Pan et al.,\n2023): generate-then-ranking and feedback-guided\ngeneration.\nThe reranking framework involves\ngenerating a large set of candidate text outputs from\nthe base model and utilizing a critic model to select\nthe best output. The integration of the critic model\ncan be achieved through chain-of-thoughts (Wei\net al., 2023; Huang et al., 2022), binary verifier (Li\net al., 2023), or a utility function (Freitag et al.,\n2022a; Fernandes et al., 2022). Our approach is\ncomplementary to re-ranking or minimum bayes\nrisk decoding (MBR) strategies, offering additional\nperformance beyond these techniques.\nIncorporating Fine-Grained Feedback\nRecent\nstudies have highlighted the benefits of fine-grained\nerror annotation by demonstrating that it can reduce\nnoise in human ratings and increase inter-rater\nagreement (Freitag et al., 2021b) as well as increase\nautomatic metric correlation to human judgments\nXu et al. (2022a, 2023a,b).\nOne approach to\nleveraging these benefits is through the use of large\nlanguage models to self-correct their own output\n(Madaan et al., 2023). Building on this, Chen et al.\n(2023) demonstrate that iterative self-improvement\nfurther enhances translation quality.\nHowever,\ndespite the unsupervised nature of the self-refine\npipeline, the feedback signal is dominated by the\nlarge language model\u2019s own evaluation capability,\nwhich has been shown to be biased towards\nsentence ordering and its own output (Liu et al.,\n2023; Xu et al., 2024). To address this limitation,\nWu et al. (2023) propose a fine-grained reward\nmodel that distinguishes rewards at the span-\nlevel associating with different error categories.\nOrthogonal to this work, we propose an inference\ntime optimization approach to iteratively refine\nmodel\u2019s output with fine-grained feedback.\n3\nRefinement with Fine-Grained\nFeedback\nThere are three main components to our framework:\na generation model, a feedback model, and a\nrefinement model, each described next.\nThe generation model produces an initial\ncandidate output yi given the input x. x and yi\nare the source text and a candidate output that is\ngenerated by the model. The feedback model F\ntakes x and yi and generates some form of feedback\nfi that represents the quality of yi, which can be\nin any form\u2014a scalar value, Boolean, free form\nnatural language, or more.\nWe assume fi can\nalways be converted into a scalar quality score via\nfunction s(\u00b7) (Details of our scoring scheme can\nbe found in Appendix A). Finally, the refinement\nmodel uses x, yi, and fi and generates a new,\nimproved output yi+1.\nAs we will discuss in\nSection 4, the loop between the feedback and\n\nrefinement model can repeat for multiple iterations\nto further evaluate and update the generated output.\nFor most of this work, we assume that both the\ngeneration and refinement models are an LLM that\nis 0-shot prompted to perform the respective task\n(See example prompt in Table 8, although we do\nexperiment with different generation models). The\nspecific prompt for the refinement model depends\non the type of feedback being used (See Figure\n1). Since our focus is on the value of fine-grained\nfeedback in the form of an error pinpoint model\nfor text generation, we next describe our feedback\nmodel in more detail.\n3.1\nAn Error Pinpoint Model\nWhile the majority of text generation evaluation\nresearch focuses on predicting a scalar quality\nscore for a text, we instead train an error pinpoint\nthat produces fine-grained feedback on translation\nquality, similar to InstructScore (Xu et al., 2023b).\nThis is based on the assumption that more specific,\nactionable feedback will enable the refinement\nmodel to generate better output.\nThe input to our feedback model is the source\ntext x and a hypothesis generation yi. The feedback\nmodel then generates a list of error locations,\ntypes, and severities in natural language that are\ncontained in yi. We model this task as a sequence-\nto-sequence model and finetune an LLM. Further\nimplementation details are provided in Section 5.\nTraining our feedback model requires a set\nof text with human-annotated error locations,\ncategories, and severities. For each task that we\nexperiment on, the training data and feedback\nmodels are different since the types of errors are\ntask-dependent. For machine translation, we use\nMQM annotated data (Mariana, 2014; Freitag et al.,\n2021a). For long form QA, we use data collected\nby Wu et al. (2023). For topical summarization, we\nuse data collected by Saunders et al. (2022).\nThe finegrained feedback model pinpoints the\nerror location and provides detailed error type\ninformation and severity level.\nThis stands in\ncontrast to more traditional evaluation metrics like\nBLEU, ROUGE or BLEURT that assign scalar\nscores that represent text generation quality. Note\nthat because the feedback model operates during\ninference, our feedback model does not use a\nreference to evaluate the text. The specific input\nand output examples for our feedback model can\nbe found in the Table 10, 11 and 12.\nOnce feedback fi is generated, it is passed to\nthe refinement model via prompting (See Figure 1\nfor example inputs and outputs to the feedback and\nrefinement model). Specific implementation and\nevaluation details of our error pinpoint model are\ndescribed in Section 5.1.\n4\nIterative Refinement as Search\nAlthough the refinement model receives the output\nyi and feedback fi, it is not always guaranteed\nto generate the best new output in a single step.\nTherefore, we experiment with different methods\nfor iterative refinement in which the feedback and\nrefinement loop is repeated until some stopping\ncondition is met.\nIterative refinement can be viewed as a search\nprocedure that is trying to find the optimal yi for\na given x, where \u201coptimal\u201d is measured by the\nfeedback model. Specifically, we model iterative\nrefinement as a local search algorithm in which\nevery possible output is a state in the search space,\nand each step of the search algorithm starts at some\nstate represented by yi and moves to yi+1. The\ngoal is to find the highest scoring state.\nWe\nexplore\nthree\ndifferent\nlocal\nsearch\nalgorithms, described next.\n4.1\nLocal Search Algorithms\nGiven a current output yi, the local search\nalgorithms begin by sampling a new candidate\noutput ci from the refinement model given feedback\nfi. Then, each algorithm makes a decision about\nwhether it will accept or reject ci based on some\ncriteria. If the decision is made to accept ci, then\nci becomes yi+1 and the search loop repeats unless\nthe feedback model detects no errors in yi+1. If ci\nis rejected, then yi becomes yi+1 and the algorithm\nrepeats (i.e., a second candidate is sampled from\nthe refinement model for the same output). Each\nof the three following algorithms differs in how it\ndecides whether to accept or reject the candidate\noutput.\nAlways Accept.\nThe \u201calways accept\u201d algorithm\n(AA) will attempt to explore the search space as\nmuch as possible by always accepting ci.\nGreedy Uphill.\nThe greedy uphill (GREEDY)\nalgorithm will only accept ci if the score from the\nfeedback model for ci is better the score for yi. In\nthis case, we ensure that the output does not get\nworse according to the feedback model.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Asai et al., 2023",
        "paper_id": "19",
        "raw_claim": " REC, a suite of fine-tuned LLM auto-evaluators, provides detailed explanations and verifiable citations for assessing generated text quality across multiple dimensions (Hsu et al., 2024).",
        "paper_content": "Preprint.\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens to evaluate its own prediction after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n3.3\nSELF-RAG INFERENCE\nGenerating reflection tokens to self-evaluate its own output makes SELF-RAG controllable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\nAdaptive retrieval with threshold. SELF-RAG dynamically decides when to retrieve text passages by\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\nCritique token type. For each critique token group G (e.g.,\nISREL ), we denote its score at timestamp\nt as sG\nt , and we compute a segment score as follows:\nf(yt, d, Critique ) = p(yt|x, d, y<t)) + S( Critique ), where\n(3)\nS( Critique ) =\nX\nG\u2208G\nwGsG\nt for G = { ISREL , ISSUP , ISUSE },\n(4)\nwhere sG\nt =\npt(\u02c6r)\nPNG\ni=1 pt(ri) stands for the generation probability of the most desirable reflection token\n\u02c6r (e.g.,\nISREL =Relevant) for the critique token type G with N G distinct tokens (that represent\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\ny is mostly supported by evidence, we can set a weight term for the\nISSUP score higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\nfilter out a segment continuation when the model generates an undesirable\nCritique token (e.g.,\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\u2019\nbehaviors. SELF-RAG tailors an LM with no additional training.\n4\nEXPERIMENTS\n4.1\nTASKS AND DATASETS\nWe conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\nClosed-set tasks include two datasets, i.e., a fact verification dataset about public health (PubHealth;\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams (ARC-\n6\n\nPreprint.\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND\nCRITIQUE THROUGH SELF-REFLECTION\nAkari Asai\u2020, Zeqiu Wu\u2020, Yizhong Wang\u2020\u00a7, Avirup Sil\u2021, Hannaneh Hajishirzi\u2020\u00a7\n\u2020University of Washington\n\u00a7Allen Institute for AI\n\u2021IBM Research AI\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\nABSTRACT\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\neration (SELF-RAG) that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called reflection tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models.1\n1\nINTRODUCTION\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n1Our code and trained models are available at https://selfrag.github.io/.\n1\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023\n\nPreprint.\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\nshot fine-tuning on task datasets (Izacard et al., 2022b).\nWhile prior work often retrieves only\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\nConcurrent RAG work.\nA few concurrent works2 on RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\non retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n3\nSELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\nor confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n3.1\nPROBLEM FORMALIZATION AND OVERVIEW\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\n2All work is arXived within a week of this preprint.\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n3\n",
        "score": "Yes"
    },
    {
        "citation_text": "Spataru et al., 2024",
        "paper_id": "12",
        "raw_claim": " While some methods focus on cost-effective extrinsic refinement (Cai et al., 2024), our work emphasizes intrinsic guidance through self-reflection and reward modeling within the generation process.",
        "paper_content": "large improvements. This can further be improved\nby using a resample-then-rerank pipelines where\nfor each sentence, we generate several versions\nand choose the best based on sentence similarity\nmeasures.\nOverall, our methods offer a practi-\ncal compromise, balancing computation with per-\nformance, and build a foundation for further re-\nsearch. Importantly, they are directly applicable to\nany probabilistic auto-regressive language models.\n10\nLimitations\nModel specifics.\nWe have applied our meth-\nods to LLaMa2-70B model and we trust that in-\ncentivising the EOS token and the SelfCheck-\nBERTScore methods will work similarly well for\nother models. However, we note that the thresh-\nolds are likely not directly transferable to other\nmodels and that in order to employ similar strate-\ngies, model owners will have to tweak the thresh-\nolds to figure out the correct numbers for their\ncase.\nSuitable tasks.\nEven though our methods can be\napplied to any long-form text generation task, they\nare perhaps most relevant for tasks where factual\naccuracy is paramount (such as long form ques-\ntion answering or factual text generation). Early-\nstopping methods specifically are more suitable\nfor tasks where generating false information is\nmore harmful than not generating it (for example\ngiving false medical advice). Our oracle for early-\nstopping removes 92% of incorrect facts from the\ngenerated text, but this comes with the cost of re-\nmoving 58% of correct facts. These measurements\n(as can be seen in appendix F.1) should be used for\nindividual applications to debate trade-offs.\nTextual diversity.\nAs this study is focused on\nfactually-dense text, we did not take into account\ndiversity of generated text, which may be rele-\nvant for more creative tasks such as story gener-\nation. For early stopping via sentence similarity,\nwe chose to use SelfCheck-BERTScore which is\nsensitive to stylistic variations, as well as factual\nvariations. However, there is no reason for which\nthis metric cannot be replaced with other sentence\nsimilarity-metrics which account for style, thus re-\ntaining the creative factor of text generation.\nAutomated\nevaluation.\nWe\nhave\nused\nthe\nFActScore pipeline, which is an automated eval-\nuation pipeline for validating truthfulness of facts.\nWe have validated the pipeline with human anno-\ntations (as detailed in Appendix B.2), but as any\nautomated pipeline it has an error margin. The re-\nliability of the pipeline is heavily dependant on the\nreliability of its knowledge source, which in this\ncase is Wikipedia - one of the most commonly-\nused, accessible, large-scale, good quality, un-\nstructured knowledge sources (Lee et al., 2022).\nFuture direction.\nOne can imagine many pos-\nsible avenues of future directions for further un-\nderstanding and mitigating semantic drift. For ex-\nample, models could be further fine-tuned specif-\nically to end generation when there is too much\nvariability in the generation, critique models could\nbe trained to identify the drift point based on\nmodel\u2019s internal states etc. We hope that with our\nwork we have sufficiently highlighted the problem\nand set the first stepping stones for addressing it.\n11\nAcknowledgments\nWe would like to thank Sewon Min for answer-\ning our questions about the FactScore (Min et al.,\n2023) pipeline and accompanying code.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2021. Evaluation of text generation: A survey.\nWoon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xi-\nujun Li, Michel Galley, Chris Brockett, Mengdi\nWang, and Jianfeng Gao. 2019. Towards coherent\nand cohesive long-form text generation. In Proceed-\nings of the First Workshop on Narrative Understand-\ning, pages 1\u201311, Minneapolis, Minnesota. Associa-\ntion for Computational Linguistics.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James Glass, and Pengcheng He. 2023. Dola:\nDecoding by contrasting layers improves factuality\nin large language models.\nDavid Dale, Elena Voita, Loic Barrault, and Marta R.\nCosta-juss`a. 2023.\nDetecting and mitigating hal-\nlucinations in machine translation: Model internal\nworkings alone do well, sentence similarity Even\nbetter.\nIn Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 36\u201350, Toronto,\nCanada. Association for Computational Linguistics.\n\nKnow When To Stop:\nA Study of Semantic Drift in Text Generation\nAva Spataru1\nEric Hambro2\u2020\nElena Voita1\nNicola Cancedda1\n1FAIR, Meta\n2Anthropic\n{avaspataru, lenavoita, ncan}@meta.com\neric.hambro@gmail.com\nAbstract\nIn this work, we explicitly show that modern\nLLMs tend to generate correct facts first, then\n\u201cdrift away\u201d and generate incorrect facts later:\nthis was occasionally observed but never prop-\nerly measured. We develop a semantic drift\nscore that measures the degree of separation\nbetween correct and incorrect facts in gener-\nated texts and confirm our hypothesis when\ngenerating Wikipedia-style biographies. This\ncorrect-then-incorrect generation pattern sug-\ngests that factual accuracy can be improved\nby knowing when to stop generation. There-\nfore, we explore the trade-off between infor-\nmation quantity and factual accuracy for sev-\neral early stopping methods and manage to im-\nprove factuality by a large margin. We further\nshow that reranking with semantic similarity\ncan further improve these results, both com-\npared to the baseline and when combined with\nearly stopping. Finally, we try calling exter-\nnal API to bring the model back to the right\ngeneration path, but do not get positive results.\nOverall, our methods generalize and can be ap-\nplied to any long-form text generation to pro-\nduce more reliable information, by balancing\ntrade-offs between factual accuracy, informa-\ntion quantity and computational cost.\n1\nIntroduction\nDifferently from the earlier approaches to gener-\nating natural language with explicit content plan-\nning (Mann, 1983; Reiter and Dale, 1997), modern\nautoregressive language models make predictions\ntoken-by-token, without pre-established text struc-\nture. One of the consequences of this methodolog-\nical shift is that newer models lack the capabil-\nity of maintaining high-level structure throughout\ngeneration and overly focus on local coherence.\nThis was noted in the form of repetition (Fu et al.,\n2021) and semantic drift (Li et al., 2021).\n\u2020Work done while at FAIR, Meta.\nThe term \u201csemantic drift\u201d emerged to describe\nthe decrease in text generation quality when in-\ncreasing generation length and has been classified\nas a sub-type of hallucinations (Ji et al., 2023).\nBefore that, semantic drift (or topic shift) was\nbriefly mentioned when talking about question\ngeneration (Zhang and Bansal, 2019) and story\ngeneration (Wang et al., 2021; Sun et al., 2020).\nIn factual evaluation, recent works also mention\na decline in factual accuracy for longer genera-\ntions (Min et al., 2023; Qian et al., 2023). While\nquality decrease for longer generations hints at\nspecific order in generation quality (high-quality\nfirst, low-quality later), this ordered behavior has\nnot been neither formally defined nor thoroughly\nstudied and measured. In this work, we refer to\n\u201csemantic drift\u201d as the strength of the order in gen-\neration quality and, for the first time, provide tools\nfor understanding this phenomenon.\nWe propose to measure semantic drift by con-\nsidering the change in truthfulness of a sequence\nof facts when a model generates a fact-rich text\naround a topic. Intuitively, we measure the degree\nof separation between correct and incorrect facts\nin a paragraph: if the model starts by generating\ncorrect facts and then switches to systematically\ngenerating incorrect ones, we consider this as a se-\nmantic drift. To quantify the severity of semantic\ndrift, we use the FActScore task which provides\ncorrect/incorrect labels for individual facts (Min\net al., 2023). We find that, indeed, several LLaMa2\nvariants have high semantic drift score: they tend\nto generate correct facts first, then \u201cdrift away\u201d\nfrom the topic and generate incorrect facts later.\nThis correct-then-incorrect separation suggests\nthat factual accuracy can be improved by stop-\nping generation early. We show that even a simple\nmethod that encourages generating EOS leads to\nlarge improvements in factuality. We then propose\nto use resample-then-rerank pipelines where for\neach sentence, we generate several versions and\narXiv:2404.05411v1  [cs.CL]  8 Apr 2024\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\nNir Ratner, Yonatan Belinkov, Omri Abend, Kevin\nLeyton-Brown, Amnon Shashua, and Yoav Shoham.\n2023. Generating benchmarks for factuality evalua-\ntion of language models. ArXiv, abs/2307.06908.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight,\nBenjamin Chess, and John Schulman. 2022.\nWe-\nbgpt: Browser-assisted question-answering with hu-\nman feedback.\nOpenAI. 2023. Gpt-4 technical report.\nHongjin Qian, Zhicheng Dou, Jiejun Tan, Haonan\nChen, Haoqi Gu, Ruofei Lai, Xinyu Zhang, Zhao\nCao, and Ji-Rong Wen. 2023. Optimizing factual\naccuracy in text generation through dynamic knowl-\nedge selection.\nVipula Rawte, Amit Sheth, and Amitava Das. 2023. A\nsurvey of hallucination in large foundation models.\nEhud Reiter and R. Dale. 1997. Building applied natu-\nral language generation systems. Natural Language\nEngineering, 3:57 \u2013 87.\nTimo Schick,\nJane Dwivedi-Yu,\nRoberto Dess`\u0131,\nRoberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. 2023. Tool-\nformer: Language models can teach themselves to\nuse tools.\nDan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, and Pascale Fung. 2022. Read be-\nfore generate! faithful long form question answering\nwith machine reading.\nRuixiao Sun, Jie Yang, and Mehrdad Yousefzadeh.\n2020. Improving language generation with sentence\ncoherence objective.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809\u2013819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan,\nBinh Tang, Ross Taylor, Adina Williams, Jian Xi-\nang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur,\nSharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. 2023. Llama\n2: Open foundation and fine-tuned chat models.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020.\nFact or fiction: Veri-\nfying scientific claims. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7534\u20137550, On-\nline. Association for Computational Linguistics.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries.\nBen Wang and Aran Komatsuzaki. 2021.\nGPT-\nJ-6B:\nA\n6\nBillion\nParameter\nAutoregressive\nLanguage Model.\nhttps://github.com/\nkingoflolz/mesh-transformer-jax.\nCunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-\ngru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi\nYao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong\nWang, Linyi Yang, Jindong Wang, Xing Xie, Zheng\nZhang, and Yue Zhang. 2023. Survey on factuality\nin large language models: Knowledge, retrieval and\ndomain-specificity.\nWei Wang, Piji Li, and Hai-Tao Zheng. 2021. Sentence\nsemantic regression for text generation.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\nXiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-\nwei Zhang, Fei Wu, and Guoyin Wang. 2023. In-\nstruction tuning for large language models: A sur-\nvey.\nShiyue Zhang and Mohit Bansal. 2019.\nAddress-\ning semantic drift in question generation for semi-\nsupervised question answering.\nIn Conference on\nEmpirical Methods in Natural Language Process-\ning.\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona\nDiab, Paco Guzman, Luke Zettlemoyer, and Marjan\nGhazvininejad. 2021. Detecting hallucinated con-\ntent in conditional neural sequence generation.\n",
        "score": "Yes"
    },
    {
        "citation_text": "Hsu et al., 2024",
        "paper_id": "4",
        "paper_content": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in\nAutomatic Evaluation by Large Language Models\nAliyah R. Hsu 1 2 James Zhu 2 Zhichao Wang 2 Bin Bi 2 Shubham Mehrotra 2 Shiva K. Pentyala 2\nKatherine Tan 2 Xiang-Bo Mao 2 Roshanak Omrani 2 Sougata Chaudhuri 2 Regunathan Radhakrishnan 2\nSitaram Asur 2 Claire Na Cheng 2 Bin Yu 1\nAbstract\nLLMs have demonstrated impressive proficiency\nin generating coherent and high-quality text,\nmaking them valuable across a range of text-\ngeneration tasks. However, rigorous evaluation\nof this generated content is crucial, as ensuring\nits quality remains a significant challenge due\nto persistent issues such as factual inaccuracies\nand hallucination. This paper introduces three\nfine-tuned general-purpose LLM auto-evaluators,\nREC-8B, REC-12B and REC-70B, specifically\ndesigned to evaluate generated text across sev-\neral dimensions: faithfulness, instruction follow-\ning, coherence, and completeness. These mod-\nels not only provide ratings for these metrics\nbut also offer detailed explanation and verifiable\ncitation, thereby enhancing trust in the content.\nMoreover, the models support various citation\nmodes, accommodating different requirements\nfor latency and granularity. Extensive evalua-\ntions on diverse benchmarks demonstrate that our\ngeneral-purpose LLM auto-evaluator, REC-70B,\noutperforms state-of-the-art LLMs, excelling in\ncontent evaluation by delivering better quality ex-\nplanation and citation with minimal bias. Our\nREC dataset and models are available at https:\n//github.com/adelaidehsu/REC.\n1. Introduction\nLarge Language Models (LLMs) have demonstrated impres-\nsive capabilities in generating high quality coherent text\nand are deployed in applications for various text-generation\ntasks (Brown et al., 2020; Chowdhery et al., 2022; OpenAI,\n2023; Touvron et al., 2023). In order for LLMs to provide\nWork done during an internship at Salesforce.\n1UC\nBerkeley\n2Salesforce\nAI\nPlatform.\nCorrespondence\nto:\nAliyah\nR.\nHsu\n<aliyahhsu@berkeley.edu>,\nJames\nZhu\n<james.zhu@salesforce.com>.\nunder review.\nup-to-date information or to perform knowledge-intensive\ntasks (Lewis et al., 2020), Retrieval Augmented Genera-\ntion (RAG) system (Chen et al., 2017; Borgeaud et al., 2021;\nIzacard et al., 2022; Guu et al., 2020) has been widely used.\nRAG involves first retrieving documents that are relevant to\nthe generation task from an external knowledge source, and\nperforming the generation using a knowledge-augmented\nprompt. However, it is of paramount importance to ensure\nthat the generated text is reliable and can be trusted by a\nhuman, as LLMs often suffer from factual inaccuracies and\nhallucination of knowledge (Ji et al., 2022; Zhang et al.,\n2023; Shuster et al., 2021). Towards this goal, we propose\nthat the generated text needs to be evaluated along various\ndimensions shown below:\n\u2022 Faithfulness:Is the LLM generated response factually\ncorrect given the context?\n\u2022 Instruction Following: Does the LLM generated re-\nsponse follow the instructions provided in the prompt?\n\u2022 Coherence: Is the LLM generated response coherent?\n\u2022 Completeness: Does the LLM generated response\ninclude all the necessary details?\n\u2022 Citation: If a LLM generated response is factually cor-\nrect, can we provide evidence for where that response\ncame from?\nIn this paper, we introduce fine-tuned models for the eval-\nuation tasks listed above that can form the basis for all\ntrust-related evaluations for generative AI applications. The\nmodels were fine-tuned to not only provide ratings for the\nmetrics but also explanations + citation for why it rated the\ngeneration so. An example of how our models operate for\ncontent quality evaluation can be found in Figure 1.\nOur models are the first to enable citation in both automatic\nevaluation (i.e., content quality citation) and general task\noutput (i.e., general RAG citation) with scalability and ef-\nficiency, while prior work often focus on providing solely\nrating and explanation in automatic evaluation (Zhu et al.,\n1\narXiv:2411.02448v3  [cs.CL]  20 May 2025\n\nRate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by LLMs\nS., and Zhang, Y. Pandalm: An automatic evaluation\nbenchmark for llm instruction tuning optimization. Inter-\nnational Conference on Learning Representations (ICLR),\n2024b.\nWang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert,\nD., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Help-\nsteer2: Open-source dataset for training top-performing\nreward models. CoRR, abs/2406.08673, 2024c. URL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr2406.html#abs-2406-08673.\nXu, W., Wang, D., Pan, L., Song, Z., Freitag, M., Wang,\nW., and Li, L. INSTRUCTSCORE: Towards explainable\ntext generation evaluation with automatic feedback. In\nBouamor, H., Pino, J., and Bali, K. (eds.), Proceedings\nof the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 5967\u20135994, Singapore, Decem-\nber 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.365. URL https://\naclanthology.org/2023.emnlp-main.365.\nYe, X., Sun, R., Arik, S. \u00a8O., and Pfister, T.\nEffective\nlarge language model adaptation for improved ground-\ning and citation generation. In North American Chap-\nter of the Association for Computational Linguistics,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:265220884.\nYuchen Lin, B., Deng, Y., Chandu, K., Brahman, F.,\nRavichander, A., Pyatkin, V., Dziri, N., Le Bras, R., and\nChoi, Y. Wildbench: Benchmarking llms with challeng-\ning tasks from real users in the wild. arXiv e-prints, pp.\narXiv\u20132406, 2024.\nZhang, J., Li, Z., Das, K., Malin, B., and Sricha-\nran,\nK.\nSac3:\nReliable hallucination detection\nin black-box language models via semantic-aware\ncross-check consistency.\nIn Conference on Em-\npirical Methods in Natural Language Processing,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:265019095.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H.,\nGonzalez, J., and Stoica, I. Judging llm-as-a-judge with\nmt-bench and chatbot arena.\nArXiv, abs/2306.05685,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:259129398.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\nwith mt-bench and chatbot arena. In Proceedings of the\n37th International Conference on Neural Information\nProcessing Systems, NIPS \u201923, Red Hook, NY, USA,\n2024. Curran Associates Inc.\nZhu, L., Wang, X., and Wang, X. Judgelm: Fine-tuned large\nlanguage models are scalable judges. arXiv, 2023.\n14\n\nRate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by LLMs\nFor instance, the post-fix citation mode would be the fastest\nas there is no need for our models to generate claims or\nsnippets. It simply has to generate the reference to the cited\ncontext. On the other hand, the inline citation with con-\ntext snippet mode has to place the citation inline with the\ngenerated response and also point to a snippet within the\ncorresponding cited context. This mode is the most granular,\nbut can increase latency as it needs to generate more output\ntokens.\n3. Related Work\nVerifiable Text Generation\nProviding a reference to the\nattributable source of information (Liu et al., 2023a; Gao\net al., 2023) has been proposed as one of the approaches to\nmitigate hallucination and improve the factuality of LLM\ngeneration. Such work fall into the general RAG citation\nsetting defined in our work, where the aim is to gener-\nate verifiable content through citation of provided articles.\nInitial efforts fine-tune LLMs using human-written exam-\nples (Nakano et al., 2021) or machine-generated examples\nverified by humans (Menick et al., 2022), but their privately\nmaintained training data limits further research. With the\nadvent of more capable LLMs (OpenAI, 2023; Jiang et al.,\n2023a), most existing work rely on zero-shot prompting or\nfew-shot prompting to cite articles during generation (Ka-\nmalloo et al., 2023; Gao et al., 2023; Liu et al., 2023a),\nalthough the quality of their generated citation leaves much\nroom for improvement (Malaviya et al., 2024). Other work\nutilize an additional natural language inference (NLI) model\nto add citation (Gao et al., 2022; Chen et al., 2023). To im-\nprove upon prior work, we propose a fine-tuned LLM that\ncan generate better-grounded responses supported with cita-\ntion of various modes, with the fine-tuning dataset released\nto the public to facilitate future research and no additional\nNLI models needed.\nGeneral-purpose auto-evaluators\nCollecting human an-\nnotations to evaluate LLM is not only costly and time con-\nsuming, but hard to replicate (Ouyang et al., 2022; Zheng\net al., 2023; Chiang & Lee, 2023), and as a result, LLMs\nbecome a natural automated proxy to evaluate LLM capa-\nbilities on various benchmarks (Bai et al., 2024; Bubeck\net al., 2023; Chiang et al., 2023; Fu et al., 2023; Liu et al.,\n2023b; Wang et al., 2023a). Existing LLM auto-evaluators\noften judge LLM outputs by expressing \u201cpreference\u201d over\noutputs from a reference model (Dubois et al., 2024; Li\net al., 2023b; Yuchen Lin et al., 2024), or by providing rat-\ning and explanation according to a user-defined metric as\na direct assessment (Li et al., 2023a; Wang et al., 2023b;\n2024b). Closer to our work, Jiang et al. (2024) and Xu et al.\n(2023) fine-tune LLMs to generate rating, explanation, and\na detailed analysis to pinpoint errors in a response evaluated.\nUnlike prior work, our fine-tuned model is a more generaliz-\nable auto-evaluator since it provides content quality citation\nfor both good and bad evaluated responses, in addition to\nrating and explanation. This not only allows users to be\nable to diagnose where errors are from but provides sup-\nporting evidence when a generation is good, which are both\nimportant to establish trust in a model.\n4. Data Collection\nTo fine-tune our general-purpose LLM auto-evaluator, we\nmeticulously collect a mixture of data encompassing a broad\nspectrum of LLM capabilities (Section 4.1), and we denote\nour collected dataset as REC-Data. Due to a lack of pub-\nlicly available content quality citation datasets, we lever-\nage synthetic data generation using an LLM to curate such\ndata (Section 4.2) to facilitate the research community. We\nperform rule-based automatic quality check followed by a\nunified task formatting to post-process our data for instruc-\ntion fine-tuning (Section 4.3).\n4.1. Task Types\nTo enhance the explainability in the automatic evaluation\nof our LLM auto-evaluator, while maintaining its general\ninstruction-following capability, we gather datasets from\na diverse range of task types (See detailed REC-Data dis-\ntribution in Appendix C). These tasks represent essential\ncapabilities that an advanced auto-evaluator LLM should\npossess:\n\u2022 Pairwise Evaluation: Compare two responses at the\nsame time and express a preference according to evalu-\nation criteria.\n\u2022 Pointwise Evaluation: Evaluate specific aspects of a\nresponse independently according to evaluation criteria\nand provide a rating.\n\u2022 Open-ended Evaluation: Evaluate a response indepen-\ndently and provide a free-form explanation, usually to\nsupport either pairwise or pointwise evaluation.\n\u2022 Citation: Evaluate a response alongside the context,\nand provide citation for verifiable answer attribution.\n\u2022 General Instruction:\nGenerate a response as in-\nstructed (no evaluation tasks in this type), such as\nsummarization and QA.\n4.2. Synthetic Data Generation\nWe leverage synthetic data generation for some of the tasks,\nincluding pointwise evaluation and citation. To ensure the\nquality of synthetic data, we leverage a powerful instruction-\ntuned model based on Llama-3.1-70B to generate both point-\nwise evaluation data and citation data for RAG and content\n3\n",
        "score": "Yes"
    },
    {
        "citation_text": "Cai et al., 2024",
        "paper_id": "8",
        "paper_content": "CERET: Cost-Effective Extrinsic Refinement for Text Generation\nJason Cai, Hang Su, Monica Sunkara, Igor Shalyminov, Saab Mansour\nAWS AI Labs\n{cjinglun, shawnsu, sunkaral, shalymin, saabm}@amazon.com\nAbstract\nLarge Language Models (LLMs) are power-\nful models for generation tasks, but they may\nnot generate good quality outputs in their first\nattempt. Apart from model fine-tuning, ex-\nisting approaches to improve prediction accu-\nracy and quality typically involve LLM self-\nimprovement / self-reflection that incorporate\nfeedback from models themselves. Despite\ntheir effectiveness, these methods are hindered\nby their high computational cost and lack of\nscalability. In this work, we propose CERET, a\nmethod for refining text generations by consid-\nering semantic stability, entailment and inter-\nsample uncertainty measures. Experimental\nresults show that CERET outperforms Self-\nconsistency and Self-rerank baselines consis-\ntently under various task setups, by 1.6% in\nRouge-1 for abstractive summarization and\n3.5% in hit rate for question answering. Com-\npared to LLM Self-rerank method, our ap-\nproach only requires 9.4% of its latency and\nis more cost-effective. 1\n1\nIntroduction\nLarge Language Models (LLMs) like GPT (Brown\net al., 2020), Claude, PaLM (Chowdhery et al.,\n2022; Anil et al., 2023), and Llama (Touvron et al.,\n2023) have showcased unprecedented capabilities\nin natural language understanding and generation.\nThese models, with parameter counts reaching into\nthe hundreds of billions, have become pivotal in\nadvancing the frontier of natural language process-\ning (NLP). Despite their impressive fluency and\ncoherence, language models frequently generate\ncontent that is incomplete, biased, or misleading in\ntheir initial attempts across a variety of language\ngeneration tasks.\nThe key challenge is that while pre-training\nequips base models with broad linguistic knowl-\n1The source code and data samples are released\nat\nhttps://github.com/amazon-science/\nCERET-LLM-refine.\nedge, it does not necessarily impart the spe-\ncialized skills needed for particular downstream\ntasks. Current methodologies for enhancing LLM\ngeneration largely involve resource-intensive ap-\nproaches such as supervised fine-tuning (SFT),\nwhich relies heavily on domain-specific training\ndata, or reinforcement learning from human feed-\nback (RLHF), which necessitates extensive hu-\nman annotations.\nHowever, curating large vol-\numes of high-quality domain-specific data and hu-\nman feedback often proves prohibitively expensive\nand time-consuming in practice, severely limiting\nthe applicability of SFT and RLHF. By integrat-\ning feedback derived from the generated outputs,\nself-improvement / self-reflection approaches en-\nhance generations in an iterative manner (Madaan\net al., 2023; Yao et al., 2023a). These approaches\nempower the LLM to adapt to specific tasks and\ndomains by learning from its own mistakes and suc-\ncesses. Nevertheless, the substantial cost linked to\niterative inference poses challenges for scalability\nand applicability real-time systems.\nThis paper introduces CERET, a novel method\ndesigned to refine text generation in a rapid, low-\nresource manner to reduce the need for domain-\nspecific training data or expensive human annota-\ntions. The cornerstone of CERET lies in its ability\nto enhance generated content by holistically con-\nsidering three key scoring dimensions - semantic\nstability, entailment, and inter-sample uncertainty\nmeasures.\nSemantic stability scoring quantifies the linguis-\ntic invariance among multiple candidate outputs\ngenerated by the base model for the same input,\nindicating higher confidence for more stable can-\ndidates. Entailment scoring leverages natural lan-\nguage inference (NLI) models to quantify the logi-\ncal entailment relations between candidate outputs,\npreferring candidates that maximally entail others.\nInter-sample uncertainty scoring penalizes candi-\ndates that are semantically similar to outputs for\narXiv:2406.05588v2  [cs.CL]  2 Nov 2024\n\nwell as Natural Language Understanding tasks. Se-\nlecting high-confidence pseudolabels is also a key\naspect of the co-training technique proposed by\nLang et al. (2022), where both partial access and\nfull access settings are studied. All these meth-\nods explore uncertainty/confidence from a certain\nperspective, while our approach combines the un-\ncertainty/confidence with semantic stability and\nentailment, and we further proposed a framework\nfor these three dimensions and investigated their\nsynergies.\n6\nConclusions\nOur proposed CERET is an efficient framework\nto enhance text generation without the need for\ndomain-specific training data or expensive anno-\ntations. By considering semantic stability, entail-\nment, and inter-sample uncertainty measures, our\napproach significantly improves the quality of text\ngeneration across multiple natural language pro-\ncessing tasks. The efficiency and cost-effectiveness\nof our approach suggest its potential for wide adop-\ntion in real-world applications.\n7\nLimitations\nCERET can be potentially applied to a wide range\nof NLP problems, including dialogue response gen-\neration, open-ended common sense reasoning, and\nNatural Language Understanding (NLU) by text\nfilling for text continuation. These topics require\ndedicated investigation and are not yet covered by\nthis paper.\nOur experiments show that beam search sam-\npling almost always provides sufficient room for\nrefinement, according to the oracle performance in\nTable 1 and Table 2. Nevertheless, in certain task\nor data scenarios, performances of no-refinement\nbaseline and oracle prediction may be close to each\nother. In that case, the performance of CERET will\nbe limited by oracle results.\nReferences\nAfra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan,\nPeter Clark, Derry Tanti Wijaya, and Niket Tandon.\n2023. RL4F: Generating natural language feedback\nwith reinforcement learning for repairing model out-\nputs. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 7716\u20137733, Toronto,\nCanada. Association for Computational Linguistics.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz - A large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018, pages 5016\u20135026. Association\nfor Computational Linguistics.\nJinglun Cai, Mingda Li, Ziyan Jiang, Eunah Cho, Zheng\nChen, Yang Liu, Xing Fan, and Chenlei Guo. 2023a.\nKg-eco: Knowledge graph enhanced entity correc-\ntion for query rewriting. In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nJinglun Cai, Monica Sunkara, Xilai Li, Anshu Bhatia,\nXiao Pan, and Sravan Bodapati. 2023b. Masked au-\ndio text encoders are effective multi-modal rescorers.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2023, pages 10718\u201310730, Toronto,\nCanada. Association for Computational Linguistics.\nYulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\n2021. DialogSum: A real-life scenario dialogue sum-\nmarization dataset. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 5062\u20135074, Online. Association for Computa-\ntional Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\n\ndifferent inputs, a signal of greater uncertainty.\nOur approach operates in a rapid, zero-shot man-\nner without any domain-specific training data, re-\nward modeling, or human feedback.\nThe pro-\nposed scoring and refinement process encapsu-\nlates an efficient way to improve text generation\nacross a diverse spectrum of NLP tasks, includ-\ning abstractive summarization, dialogue response\ngeneration, and open-domain question answering.\nThrough a rigorous series of experiments on stan-\ndard datasets, CERET is empirically validated to\nsignificantly outperform baseline methods such as\nSelf-consistency and Self-reranking across both\nsummarization and QA tasks. Beyond its superior\nperformance, CERET stands out for its practical-\nity and cost-effectiveness, making it a promising\nsolution for real-world applications where domain-\nspecific resources and annotations are limited or\nunavailable. This paper not only presents CERET\nas a valuable novel contribution to the growing\nfield of NLP but also underscores its potential im-\npact on advancing the practical deployment of text\ngeneration across a myriad of domains.\nThe main contributions are summarized as fol-\nlows:\n\u2022 CERET is proposed as a holistic framework\nfor enhancing generation quality, encompass-\ning semantic stability, entailment, and inter-\nsample uncertainty measures.\n\u2022 The refinement process is data efficient and\ncost-effective, without the requirement for\ndomain-specific training data or expensive an-\nnotations.\n\u2022 The proposed approach can be applied across\nvarious natural language processing tasks,\nsuch as text summarization, dialogue response\ngeneration and question-answering systems.\n\u2022 CERET is highlighted for its practicality and\nefficiency, presenting only a minor fraction\nof the usual latency associated with a single\ngeneration call, which positions it as a feasible\nsolution for real-world applications.\n2\nApproach\n2.1\nSystem Architecture\nCERET consists of three scoring methods, namely\nSemantic Stability Scoring, Entailment Scoring\nand Inter-sample Uncertainty Scoring, for calibrat-\ning the quality of LLM predictions. The overview\nof the proposed system is illustrated in Figure 1.\nFirstly, a diverse set of candidates are sampled from\nLLMs. Then each individual scoring method will\nproduce a separate score from a certain perspec-\ntive. Based on the scores in three dimensions, a\nlinear weighted final confidence score is computed\nto measure the quality of each prediction. The pre-\ndiction with the highest confidence score is selected\nas the final model prediction.\n2.2\nSemantic Stability Scoring\nWe first introduce an intra-sample scoring method,\nSemantic Stability Scoring, which is motivated\nby the need to enhance the confidence and reli-\nability of sample generations produced by LLMs.\nThe scientific rationale is inspired by Kuhn et al.\n(2023) and Yin et al. (2022), where it was shown\nthat a sample generation exhibits higher confidence\nwhen it demonstrates considerable semantic stabil-\nity or linguistic invariance among other generations.\nHowever, the semantic stability measured in Kuhn\net al. (2023) involves clustering sampled genera-\ntions for each sample, which is computationally\nexpensive for real world applications at large scale.\nIn contrast, we propose a cluster-free method for\nsemantic stability modeling. Specifically, Seman-\ntic Stability Scoring is formulated as the following:\nGiven input data sample x, the model generates\nk predictions (y1, ..., yk). For each yi, a fixed pre-\ntrained language model produces its correspond-\ning embedding e(yi).\nIn practice, we leverage\nRoBERTa (A Robustly Optimized BERT Pretrain-\ning Approach) (Liu et al., 2019) as the pre-trained\nlanguage model, and the final hidden representa-\ntion of \u201c<s>\u201d token from RoBERTa, is regarded\nas e(yi). To aggregate all intra-sample representa-\ntions, we treat the average-pooled embedding \u00afe as\na stability reference point:\n\u00afe = mean(e(y1), ..., e(yk))\n(1)\nA lower distance between an embedding and the\nreference point implies a higher stability. We can\nemploy Euclidean distance or cosine distance as\nthe distance metric || \u00b7 ||. The stability score si\nsta is\ndefined as the negative distance between e(yi) and\nthe stability reference point \u00afe:\nsi\nsta = \u2212||e(yi) \u2212\u00afe||\n(2)\n2.3\nEntailment Scoring\nEntailment scoring is another intra-sample scoring\nmethod, fully powered by entailment relation: \u201cp\n",
        "score": "No"
    },
    {
        "citation_text": "\u015eahinu\u00e7 et al., 2024",
        "paper_id": "11",
        "paper_content": "Systematic Task Exploration with LLMs:\nA Study in Citation Text Generation\nFurkan \u00b8Sahinu\u00e71, Ilia Kuznetsov1, Yufang Hou2,3, Iryna Gurevych1\n1Ubiquitous Knowledge Processing Lab (UKP Lab)\nDepartment of Computer Science and Hessian Center for AI (hessian.AI)\nTechnical University of Darmstadt\n2IBM Research Europe - Ireland\n3Technical University of Darmstadt\nwww.ukp.tu-darmstadt.de\nAbstract\nLarge language models (LLMs) bring unprece-\ndented flexibility in defining and executing\ncomplex, creative natural language generation\n(NLG) tasks. Yet, this flexibility brings new\nchallenges, as it introduces new degrees of free-\ndom in formulating the task inputs and instruc-\ntions and in evaluating model performance. To\nfacilitate the exploration of creative NLG tasks,\nwe propose a three-component research frame-\nwork that consists of systematic input manipu-\nlation, reference data, and output measurement.\nWe use this framework to explore citation text\ngeneration \u2013 a popular scholarly NLP task that\nlacks consensus on the task definition and evalu-\nation metric and has not yet been tackled within\nthe LLM paradigm. Our results highlight the\nimportance of systematically investigating both\ntask instruction and input configuration when\nprompting LLMs, and reveal non-trivial rela-\ntionships between different evaluation metrics\nused for citation text generation. Additional\nhuman generation and human evaluation exper-\niments provide new qualitative insights into the\ntask to guide future research in citation text gen-\neration. We make our code1 and data2 publicly\navailable.\n1\nIntroduction\nThanks to their instruction-following abilities, large\nlanguage models (LLMs) allow specifying and ex-\necuting NLP tasks with unprecedented flexibility\nand speed, while reducing the need for task-specific\narchitecture design, data annotation, and model\ntraining (Touvron et al., 2023a,b; Taori et al., 2023;\nOuyang et al., 2022; OpenAI, 2023; Chung et al.,\n2022). This has led to a surge of new, complex,\ncreative natural language generation (NLG) tasks\nlike peer review generation (Robertson, 2023) or\nstory and poetry generation (Chakrabarty et al.,\n2023), that push the boundary of what was deemed\nfeasible for NLP systems just a few years ago.\n1GitHub: UKPLab/acl2024-citation-text-generation\n2Data: TUdatalib\n\u201cPrevious studies in related work \ngeneration cast the task as text \nsummarization [REF#B], however, in this \nwork we explore alternative approaches\u201d\ninstruction\nabstract\nabstract\n+intent\n+example\nprompt\nSurface metrics\nROUGE\nBLEURT \nBERTScore\nTRUE\nSummaC\ncites\nA\nB\nx6\nLlama 2\ninputs \noutput \nGPT 3.5\n2\n3\n4\n1\nFigure 1: Citation text generation with LLMs. The\ntask (1) is to generate a paragraph of related work from\nthe citing paper (A) about a cited paper (B). The instruc-\ntion combined with task inputs constitutes a prompt (2)\nthat is communicated to the model. The model\u2019s re-\nsponse (3) is evaluated using a range of measurements,\nfrom word count to NLI-based factuality metrics (4).\nThe flexibility comes at a cost, as it introduces\nnew degrees of freedom into the analysis. LLMs\ngenerate output in response to a prompt, which\nconsists of a natural-language task instruction sup-\nplemented by additional bits of information about\nan instance, which we term input components (Fig-\nure 2). LLM-powered creative NLG tasks often\nfeature a complex input component space, and the\ntask instruction wording can affect model behavior\nin non-intuitive ways. The output space is varied\nas well, as there might exist infinitely many accept-\nable generations. This overall variability brings\nthe risk of creative NLG tasks being defined and\nevaluated ad hoc, hindering systematic comparison\nof NLP systems and leading to anecdotal accounts\nof LLM capabilities.\nAlthough optimizing model instructions to max-\nimize performance of LLMs is an active research\narea (Section 2.1), prompt engineering mostly tar-\narXiv:2407.04046v1  [cs.CL]  4 Jul 2024\n\nMain paper abstract: This paper proposes a combined model for POS tagging <...>\nRelevant paper abstract: In this paper, we propose a novel decoding algorithm for <...>\nIntent: To compare the results of the proposed model with the results of the previous work.\nExample: [REF#1] proposed a joint decoder for word segmentation, POS tagging and \nword-based constituent parsing, although they trained models for the three tasks separately.\nYour aim is to generate an exactly single paragraph to be used in related work section in a \nmain paper. You will be given main paper's abstract, a relevant paper's abstract and the \nintent of the paragraph. The paragraph should reflect the intent and you need to refer the \nrelevant paper in the same paragraph by using citation mark [REF#1]. You can inspire from \nthe given example. Your output must strictly consist of the related work paragraph only, \nnothing else.\nInput components\n\u2705 Source abstract\n\u2705 Target abstract\n\u2705 Intent\n\u2705 Example \nInstruction \u2192\n+\n+\nFigure 2: Prompt manipulation combines the instruction (top) with input components (left) and the corresponding\ndata (bottom) incl. free-form citation intent and example sentence. The result serves as LLM prompt as in Figure 1.\ngets the tasks where input and output spaces are\nwell-defined (e.g., question answering). However,\nsome creative NLG tasks need a step of exploration\nof what inputs are required and how the evaluation\nof outputs will be carried out before deeply explor-\ning the best way to introduce the task to LLMs.\nOur work addresses task variability in citation\ntext generation \u2013 a widely studied scholarly NLG\ntask aiming to increase efficiency of scientific work\n(Li and Ouyang, 2022; Funkquist et al., 2023). Ci-\ntation text generation is a good example of creative\nNLG, as it features a complex input component\nspace combined with multiple plausible outputs.\nPrior work on citation text generation lacks consen-\nsus on the required inputs, explores only a limited\nnumber of measurements to characterize the out-\nputs, and does not investigate the use of instruction-\ntuned LLMs to tackle the task (Table 1).\nTo address this gap, we design a framework to\nsystematically explore the task of citation text gen-\neration with LLMs (Figure 1). We systematically\nmanipulate the input components and instructions\ncommunicated to the model via a prompt, and study\nthe effects of these manipulations on the model\noutput using a wide range of measurements, sup-\nplemented by a novel reference dataset for citation\ntext generation based on the ACL Anthology, and\nfeaturing novel use of free-form citation intents to\nguide generation (Section 3). Our experiments with\ntwo state-of-the-art LLMs \u2013 Llama 2-Chat (Tou-\nvron et al., 2023b) and GPT 3.5 Turbo (Ouyang\net al., 2022) reveal that input components and task\ninstructions both impact the generations, and their\neffects add up. Free-form citation intents, as illus-\ntrated in Figure 2, show promise as an alternative\nto categorical intents used in prior citation text gen-\neration work. Our results (Section 5) imply that\nthe relative performance of alternative task input\nconfigurations can be estimated on a small set of\ninstructions, while the best absolute performance\nneeds experimentation with a wide array of instruc-\ntion wordings. Through correlation analysis, we\nobserve that the NLG metrics in our measurements\nare complementary, motivating the use of wide-\nspanning measurement sets for NLG tasks beyond\ncitation text generation. Our human studies (Sec-\ntion 6) reveal both quantitative and qualitative in-\nsights about input components and task instructions\nfrom both generation and evaluation perspectives.\nIn summary, this work contributes:\n\u2022 A framework for exploring the task of citation\ntext generation with LLMs;\n\u2022 A new reference corpus of citation texts based\non the ACL Anthology enriched with novel free-\nform citation intents;\n\u2022 Experimental results on the impact of task in-\nputs and instructions on citation text generation\noutputs, and an examination of the relationships\nbetween the measurements;\n\u2022 Human evaluation and generation studies provid-\ning additional insights to shape future work in\ncitation text generation and creative NLG.\nWe stress that our work neither seeks nor claims\nstate-of-the-art citation text generation, as the dif-\nferences in pre-trained model capabilities would\nhinder a fair comparison and likely lead to con-\nfounding (Nityasya et al., 2023). Instead, the ob-\njective of our work is to explore prompting as a\ntool for systematic task manipulation in the LLM\nage. We believe our approach to be general and\nadaptable to other creative NLG tasks.\n2\nBackground\n2.1\nLLMs and Prompting\nInstruction-tuned LLMs demonstrate competitive\nzero-shot performance across a wide range of NLP\ntasks (Touvron et al., 2023a,b; Taori et al., 2023;\n",
        "score": "No"
    }
]