{"related_works": "## Related Work\n\nVariational Autoencoders (VAEs) have emerged as a powerful framework for natural language generation, yet they are frequently plagued by two persistent challenges: information under-representation and posterior collapse. Information under-representation arises when the latent space, typically derived from only the final hidden state of an encoder, fails to adequately summarize the input data. Posterior collapse, on the other hand, occurs when the Kullback-Leibler (KL) divergence term in the VAE objective function dominates the reconstruction loss, leading to an uninformative latent space that is ignored by the decoder. Our work, which proposes a discrete variational attention model with a categorical distribution over the attention mechanism and an auto-regressive prior, directly addresses these issues by leveraging the discrete nature of language and enhancing the latent space for improved generation.\n\nA significant body of research has focused on mitigating posterior collapse in VAEs for text generation. Many approaches explore modifications to the training objective or the latent space itself. For instance, Ding et al. (2023) experiment with techniques like KL warm-up, word dropout, and KL coefficient adjustments within a continuous latent space to address posterior collapse in conditional VAEs for neural machine translation. Similarly, Long et al. (2019) investigate posterior collapse and encoder feature dispersion in sequence VAEs, while Mansbridge et al. (2018) propose AutoGen, an RNN-based VAE that adds high-fidelity reconstructions to the objective function to force latent variable utilization, also discussing KL annealing as a common technique. Shen et al. (2019) introduce a density gap-based regularization method to combat posterior collapse and the \"hole problem\" in continuous VAEs. While these works primarily operate within continuous latent spaces and employ various regularization or training strategies, our approach distinguishes itself by leveraging the inherent discreteness of language to inherently prevent posterior collapse.\n\nThe concept of discrete latent variables has gained traction as a promising avenue for addressing VAE limitations in text generation. Our work builds upon and extends this idea by proposing a discrete variational attention model. Fang et al. (2020) directly address information under-representation and posterior collapse by proposing a discrete variational attention model with a categorical distribution over the attention mechanism and an auto-regressive prior, a model that is nearly identical to the one presented in our abstract, indicating a strong foundational relationship. Similarly, Li et al. (2020) propose the Discrete Auto-regressive Variational Attention Models (DAVAM) which also leverages discrete latent space to avoid posterior collapse and an auto-regressive prior to capture sequential dependencies, further validating the efficacy of our chosen approach. Park et al. (2018) also explore discrete latent variables for text generation, specifically proposing a discretized bottleneck (DB-VAE) to enforce implicit latent feature matching and alleviate posterior collapse, and investigating the impact of codebook size. These works collectively underscore the benefits of discrete latent spaces for language generation and provide strong support for our choice of a discrete approach.\n\nBeyond discrete latent variables, other approaches have explored alternative latent space structures to improve VAE performance. Celotti et al. (2020) propose APo-VAE, which utilizes hyperbolic latent space and wrapped normal distributions to achieve robustness against posterior collapse and capture hierarchical structures in text. While our work focuses on discrete attention, APo-VAE offers a contrasting method for enhancing the latent space. Serban et al. (2017) investigate the use of flexible, multi-modal piecewise constant distributions for continuous latent variables in VAEs to overcome the limitations of simplistic Gaussian priors and better capture complex, multi-modal latent factors. Their work, while focusing on continuous variables, acknowledges the promise of discrete latent variables, providing a valuable comparative perspective for our discrete approach.\n\nAddressing information under-representation, where only the last hidden state is used to form the latent space, is another critical aspect of VAE improvement. Wang et al. (2017) propose HR-VAE, which tackles latent variable collapse by imposing a standard normal prior on all hidden states of the RNN encoder, rather than just the last one. This directly relates to our concern about information under-representation, as both works aim to improve the latent space's ability to summarize data. Bowman et al. (2015) also explore generating sentences from a continuous space, a foundational work in VAEs for text generation that highlights the challenges of effectively utilizing the latent space. Our approach, by incorporating an attention mechanism, aims to capture more comprehensive information from the input sequence, thereby mitigating information under-representation.\n\nThe utility of the latent space for specific generation tasks, such as controllable text generation, has also been a focus of research. Shu et al. (2020) address the \"latent vacancy\" problem, where manipulated latent codes fall into low-density regions, leading to poor decoding, by constraining the posterior mean to a learned probability simplex for unsupervised controllable text generation. While our work focuses on general language generation, their efforts to improve latent space utility for specific applications are relevant. Similarly, Li et al. (2022) explore Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation, aiming to create more interpretable and controllable latent spaces.\n\nSome research has explored alternative architectural components or pre-training strategies to enhance VAEs for text generation. Wang et al. (2019) propose PoDA, a denoising autoencoder approach for pre-training seq2seq models for text generation, which could potentially complement our VAE-based approach by providing a robust pre-training method for the underlying sequence-to-sequence architecture. Pang et al. (2021) introduce OPTIMUS, a large-scale VAE model combining BERT and GPT-2 to create a universal latent space for sentences, demonstrating the benefits of pre-training on massive datasets for mitigating posterior collapse. While OPTIMUS uses continuous latent variables, its success in creating a well-structured latent space offers a contrasting yet valuable perspective.\n\nThe role of attention mechanisms in VAEs for text generation has also been investigated. Bahuleyan (2018) addresses the \"bypassing phenomenon\" in Variational Encoder-Decoders (VEDs), where a deterministic attention mechanism can render the variational latent space ineffective. They propose a variational attention mechanism where the attention vector itself is modeled as random variables, aiming to alleviate the bypassing problem and increase diversity. This work is particularly relevant to ours, as we also employ an attention mechanism, but our focus is on making the attention mechanism discrete to align with the discrete nature of language and prevent posterior collapse.\n\nFinally, other related works address different aspects of improving text generation models. Tu and Li (2022) provide an overview of controllable text generation via VAEs, offering a broader context for our work. Zhang et al. (2023) introduce DEM-VAE to address the mode-collapse problem in VAEs with mixture priors, aiming for a well-structured and interpretable latent space, which aligns with our interest in discrete representations for language, albeit for a different type of collapse. McCarthy et al. (2019) introduce Variational State Space Models (VSSMs) for parallel autoregressive generation, focusing on efficient generation through a different architectural approach. While their primary focus is on parallelization, their use of a VAE framework and aim for improved performance in language generation suggests a potential future intersection with our research. Jhamtani et al. (2020) propose a deep latent variable model for story generation that learns a sequence of discrete anchor words as a latent plan, highlighting the utility of discrete latent variables for inducing useful latent spaces. These diverse approaches collectively contribute to the ongoing efforts to enhance the capabilities of language generation models.", "citations": ["Zhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu, Tianyang Zhan, Gholamreza Haffari (2022). Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. arXiv:2202.13363. https://arxiv.org/abs/2202.13363", "Haoqin Tu, Yitong Li (2022). An Overview on Controllable Text Generation via Variational Auto-Encoders. arXiv:2211.07954. https://arxiv.org/abs/2211.07954", "Lei Shu, Alexandros Papangelis, Yi-Chia Wang, Gokhan Tur, Hu Xu, Zhaleh Feizollahi, Bing Liu, Piero Molino (2020). Controllable Text Generation with Focused Variation. arXiv:2009.12046. https://arxiv.org/abs/2009.12046", "Yookoon Park, Jaemin Cho, Gunhee Kim (2018). A Hierarchical Latent Structure for Variational Conversation Modeling. arXiv:1804.03424. https://arxiv.org/abs/1804.03424", "Xianghong Fang, Haoli Bai, Zenglin Xu, Michael Lyu, Irwin King (2020). Discrete Variational Attention Models for Language Generation. arXiv:2004.09764. https://arxiv.org/abs/2004.09764", "Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, Tat-Seng Chua (2023). MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space. arXiv:2305.12785. https://arxiv.org/abs/2305.12785", "Bo Pang, Erik Nijkamp, Tian Han, Ying Nian Wu (2021). Generative Text Modeling through Short Run Inference. arXiv:2106.02513. https://arxiv.org/abs/2106.02513", "Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang, Jianfeng Gao, Lawrence Carin (2019). Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models. arXiv:1902.00154. https://arxiv.org/abs/1902.00154", "Luca Celotti, Simon Brodeur, Jean Rouat (2020). AriEL: volume coding for sentence generation. arXiv:2003.13600. https://arxiv.org/abs/2003.13600", "Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, Jianfeng Gao (2020). Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space. arXiv:2004.04092. https://arxiv.org/abs/2004.04092", "Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, Andr\u00e9 Freitas (2023). LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces. arXiv:2312.13208. https://arxiv.org/abs/2312.13208", "Hareesh Bahuleyan (2018). Natural Language Generation with Neural Variational Models. arXiv:1808.09012. https://arxiv.org/abs/1808.09012", "Alex Mansbridge, Roberto Fierimonte, Ilya Feige, David Barber (2018). Improving latent variable descriptiveness with AutoGen. arXiv:1806.04480. https://arxiv.org/abs/1806.04480", "Heng Wang, Zengchang Qin, Tao Wan (2017). Text Generation Based on Generative Adversarial Nets with Latent Variable. arXiv:1712.00170. https://arxiv.org/abs/1712.00170", "Teng Long, Yanshuai Cao, Jackie Chi Kit Cheung (2019). On Posterior Collapse and Encoder Feature Dispersion in Sequence VAEs. arXiv:1911.03976. https://arxiv.org/abs/1911.03976"], "selected_papers": [{"paper_id": 33, "text": ["Conditional Variational Autoencoder for Neural Machine Translation\nArtidoro Pagnoni\u22171 Kevin Liu\u22171 Shangyan Li\u22171\nAbstract\nWe explore the performance of latent variable\nmodels for conditional text generation in the con-\ntext of neural machine translation (NMT). Sim-\nilar to (Zhang et al., 2016), we augment the\nencoder-decoder NMT paradigm by introduc-\ning a continuous latent variable to model fea-\ntures of the translation process. We extend this\nmodel with a co-attention mechanism motivated\nby (Parikh et al., 2016) in the inference network.\nCompared to the vision domain, latent variable\nmodels for text face additional challenges due\nto the discrete nature of language, namely pos-\nterior collapse (Bowman et al., 2015). We ex-\nperiment with different approaches to mitigate\nthis issue. We show that our conditional varia-\ntional model improves upon both discriminative\nattention-based translation and the variational\nbaseline presented in (Zhang et al., 2016). Fi-\nnally, we present some exploration of the learned\nlatent space to illustrate what the latent variable\nis capable of capturing. This is the \ufb01rst reported\nconditional variational model for text that mean-\ningfully utilizes the latent variable without weak-\nening the translation model.\n1. Introduction\nMachine translation is a classic, conditional language mod-\neling task in NLP, and was one of the \ufb01rst in which deep\nlearning techniques trained end-to-end have been shown\nto outperform classical phrase-based pipelines.\nCurrent\nNMT models generally use the encoder-decoder frame-\nwork (Sutskever et al., 2014) where an encoder transforms\na source sequence to a distributed representation, which the\ndecoder then uses to generate the target sequence.\nAd-\nditionally, attention mechanisms (Bahdanau et al., 2014)\nallow the model to focus on relevant parts of the source\nsequence when decoding. However, these attention-based\n*Equal contribution\n1School of Engineering and Applied Sci-\nences, Harvard University. Correspondence to: Artidoro Pagnoni\n<apagnoni@college.harvard.edu>.\nmodels may be insuf\ufb01cient in capturing all alignment and\nsource sentence information (Tu et al., 2016).\nTo attempt to more fully capture holistic semantic infor-\nmation in the translation process, we explore latent vari-\nable models. Latent variable models are a class of statisti-\ncal models that seek to model the relationship of observed\nvariables with a set of unobserved, latent variables, and can\nallow for modeling of more complex, generative processes.\nHowever, inference in these models can often be dif\ufb01cult\nor intractable, motivating a class of variational methods\nthat frame the inference problem as optimization. Varia-\ntional Autoencoders (Kingma & Welling, 2013), in partic-\nular, have seen success in tasks such as image generation\n(Gregor et al., 2015), but face additional challenges when\napplied to discrete tasks such as text generation (Bowman\net al., 2015).\nWe experiment with a conditional latent variable model ap-\nplied to the task of translation. (Zhang et al., 2016) intro-\nduce a framework and baseline for conditional variational\nmodels and apply it to machine translation.\nWe extend\ntheir model with a co-attention mechanism, motivated by\n(Parikh et al., 2016), in the inference network and show\nthat this change leads to a more expressive approximate\nposterior. We compare our conditional variational model\nwith a discriminitive, attention-based baseline, and show\nan improvement in BLEU on German-to-English transla-\ntion. We also present our experiments testing various meth-\nods of addressing common challenges of applying VAEs to\ntext (Bowman et al., 2015), namely posterior collapse. Fi-\nnally, we demonstrate some exploration of the learned la-\ntent space in our conditional variational model.\n2. Background\nThis section discusses recent efforts in neural machine\ntranslation, variational autoencoders (VAE), and their ex-\ntension to the conditional case (CVAE).\n2.1. RNN-Attention Sentence Encoding\nIn the standard Recurrent Neural Net (RNN)-based\nencoder-decoder setting, the encoder RNN represents the\nsource sentence by learning sequentially from the previous\nsource word xi and an evolving hidden state, while the de-\narXiv:1812.04405v1  [cs.CL]  11 Dec 2018\n", "Conditional Variational Autoencoder for Neural Machine Translation\nFigure 1. Experiment 3: Ranked generation of samples by log probabilities\nGenerated sentences from sampling from the prior. Ranked by log likelihood.\n", "Conditional Variational Autoencoder for Neural Machine Translation\nWe trained each of our models end-to-end with Adam\n(Kingma & Ba, 2014) with initial learning rate 0.002, de-\ncayed by a scheduler on plateau.\nOur variational mod-\nels used Monte Carlo sampling and the reparameterization\ntrick for gradient estimation (Kingma & Welling, 2013;\nRezende et al., 2014). Latent variables are sampled from\nthe approximate posterior during training, but from the\nprior during generation.\nFor our variational models, we use a KL warm-up schedule\nby training a modi\ufb01ed objective:\nJ = RE + \u03b1KL\nAlpha is set to 0 for the \ufb01rst \ufb01ve training epochs, then an-\nnealed linearly over the next ten epochs.\nWe compared three models: vanilla sequence-to-sequence\nwith dot-product attention, VNMT (Zhang et al., 2016),\nand our Conditional VAE with co-attention. All models\nused 300 dimensional word embeddings, 2 layer encoder\nand decoder LSTMs with hidden dimensions of size 300.\nVariational models used 32 dimensional latent variables.\n6. Results\nOur main results comparing discriminative attention-based\ntranslation with a few of our CVAE models are shown in\nTable1.\n6.1. Experiment 1: Expressiveness of Co-attention\nInference\nTo assess the contribution of our co-attention based approx-\nimate posterior, we compare the reconstruction losses of\nour model and the VNMT model (Zhang et al., 2016) with\nthe KL term of the ELBO objective zeroed out. Here, all\ngradients will only be backpropagated through the recon-\nstruction error, eliminating the KL regularization of the ap-\nproximate posterior to resemble the prior. Hence, the re-\nconstruction error here is a measure of the ability of the\napproximate posterior to encode information relevant to re-\nconstructing the target sequence. Results are shown below\nin Table 2.\nModel\nRE\nVNMT\n1.5771\nCo-attention\n1.3572\nTable 2. Experiment 1: Expressiveness of Co-attention Inference\n6.2. Experiment 2: Addressing Posterior Collapse\nNext we explore three methods of addressing posterior col-\nlapse: Word Dropout, KL Minimum, and KL Coef\ufb01cient.\nResults are shown in Table 3.\n6.2.1. WORD DROPOUT\nExtending word dropout as used in (Bowman et al., 2015),\nwe weaken the encoder-decoder portion of the model to\nsteer the model to make greater use of the latent variable\nwhen translating. We mask words with <unk> in both\nthe source and target sequences before feeding them into\nthe encoder and decoder, respectively. However, we do\nnot mask words fed into the inference networks, hoping to\nmore strongly incentive use of the the latent variable.\n6.2.2. KL MINIMUM\nWe set a minimum KL penalty in the objective, forcing the\nmodel to take at least a \ufb01xed KL regularization cost.\nJ = RE + max(KL, KLMIN)\n6.2.3. KL COEFFICIENT\nWe \ufb01x a constant coef\ufb01cient \u03b1KL to the KL objective, al-\nlowing us to adjust the weighting of the KL penalty relative\nto reconstruction error.\nJ = RE + \u03b1KLKL\n6.3. Experiment 3: Generation and Interpolation\nTo explore the latent space learned by the model, we sam-\nple and generate multiple sequences. To verify that the la-\ntent space is smooth, we interpolate across the latent space\nand observe sentences generated. Figure 1 shows 20 sam-\npled sentences for each example, ranked by log probability.\nFigure 2 shows examples of linear interpolations between\ntwo sampled latent variables. These experiments are done\nwith the CVAE model trained with KL coef\ufb01cient of 0.25.\n7. Discussion\nFrom our main results, our variational models are able to\noutperform a vanilla sequence-to-sequence model with at-\ntention by both BLEU and perplexity measures. However,\nas expected with VAEs for text, we ran into the challenge\nof posterior collapse for our standard CVAE model. By set-\nting KL coef\ufb01cient to 0.25 (described above), we are able\nto train a model that utilizes the latent variable model much\nmore, and still outperform sequence-to-sequence in terms\nof BLEU.\nIn experiment 1, we show that the addition of our co-\nattention mechanism signi\ufb01cantly improves the expressive-\nness of the approximate posterior network. This indicates\nthe potential for our CVAE model to improve on previ-\nous variational baselines for translation. Furthermore, this\nresult also con\ufb01rms that capturing interactions between\nsource and target sentences through co-attention helps pro-\n", "Conditional Variational Autoencoder for Neural Machine Translation\nIn the same spirit as the co-attention technique described\nin (Parikh et al., 2016), we compute pairwise dot attention\ncoef\ufb01cients between the words of the source sentence and\neach word of the target sentence, and vice versa. Notice\nhere that instead of applying the co-attention mechanism\ndirectly to the word embeddings as it is done in (Parikh\net al., 2016), we apply it to the annotation vectors produced\nby running the encoder LSTM on both source and target\nsentences. We found that this approach lead to a more rep-\nresentative posterior network, which gave better results.\nThe source and target attention coef\ufb01cients are therefore\ngiven by:\n\u03b1x\nt =\nexp(hy\nt hx\u22a4\ni\n)\nP\ni exp(hy\nt hx\u22a4\ni\n) = softmax(hy\nt hx\u22a4)\n\u03b1y\nt =\nexp(hx\nt hy\u22a4\ni\n)\nP\ni exp(hx\nt hy\u22a4\ni\n) = softmax(hx\nt hy\u22a4)\nWhere the softmax is take over the second dimension. We\nthen use these coef\ufb01cients to get context vectors, which are\nconvex combinations of the annotation vectors:\ncx\nt = \u03b1x\nt hx\ncy\nt = \u03b1y\nt hy\nWe combine the previous with a mean-pool to obtain a\n\ufb01xed dimensional vector, and concatenate it with the mean-\npool of the annotation vectors from both the source and tar-\nget sentences (similar to what is done in the prior).\n\u00afcx =\n1\nTf\nPTf\nt=1 cx\nt\n\u00afcy =\n1\nTs\nPTs\nt=1 cy\nt\n\u00af\nhx =\n1\nTs\nPTs\nt=1 hx\nt\n\u00af\nhy =\n1\nTf\nPTf\nt=1 hy\nt\nFinally, we add a linear projection layer and a non linearity,\nand get the \ufb01nal \ufb01xed dimensional vector.\nh\u2032\nz = tanh(W \u2032\nz( \u00afcx; \u00af\nhx; \u00afcy; \u00af\nhy) + b\u2032\nz)\nThis will be projected to the mean vector and variance ma-\ntrix just like in the prior network:\n\u00b5\u2032\n=\nW \u2032\n\u00b5h\u2032\nz + b\u2032\n\u00b5\nlog \u03a3\u20322\n=\nI(W \u2032\n\u03c3h\u2032\nz + b\u2032\n\u03c3)\nThrough the use of the co-attention network, the mean and\nvariance parameters of the posterior capture interactions\nbetween source and target sentences.\n4.3. Neural Decoder\nThe decoder models the probability of a target sentence y\ngiven a source sentence x and a latent variable z by de-\ncomposing the generation process in a left to write process.\nAt each time step given y<j, the words that were already\ntranslated, x and, z the decoder outputs a probability dis-\ntribution over the vocabulary.\np(y|x, z) =\nTf\nY\nj=1\np(yj|y<j, x, z)\nWe use Bahdanau\u2019s attention decoder (Bahdanau et al.,\n2014) with the incorporation of the dependence on the la-\ntent variable z. In particular we can parametrize the proba-\nbility of decoding each word as:\np(yj|y<j, x, z) = softmax(Wv tanh(hj; cj; z) + bv)\nWhere Wv is a linear projection to a vocabulary-sized vec-\ntor, hj is the output of the LSTM at step j, cj is the context\nvector for time step j, and z is the sentence level latent\nvariable.\nThe context vector cj is the result of a convex combina-\ntion of the annotation vectors hx produced by the encoder\napplied to the source sentence x.\ncj = \u03b1jhx\nWhere \u03b1j is the vector of normalized attention weights ob-\ntained by taking the softmax of the dot product of annota-\ntion vectors and the LSTM output hj.\n\u03b1j = softmax(hjhx\u22a4)\nThe hidden state hj is produced at each step by a LSTM\nthat takes as input z and the word embedding of word yj.\nhj = LSTM((Ey(yj); z), hj\u22121)\nThe decoder network that we present differs from Bah-\ndanau\u2019s architecture in that we include the dependency on\nthe latent variable z. The vector z is concatenated before\nthe last projection layer to the context vector and the LSTM\nhidden state. We also included it as a skip connection in the\nLSTM input by concatenating it to the word embedding of\nthe target words at each time step.\n5. Methods\nWe use the IWSLT 2016 German-English dataset for our\nexperiments, consisting of 196k sentence pairs. We prepro-\ncess by \ufb01ltering out pairs containing sentences longer than\n100 words and replacing all words that appear less than\n\ufb01ve times with an \u201dunk\u201d token, yielding vocabulary sizes\nof 26924 German words and 20489 English words. Note:\nfor BLEU score calculation in our current results, we retain\nthe \u201dunk\u201d tokens and thus may not be directly comparable\nto other published results.\n"], "summary": "The cited paper, \"Conditional Variational Autoencoder for Neural Machine Translation,\" explores latent variable models for conditional text generation, specifically in neural machine translation. It addresses the challenge of posterior collapse in VAEs for discrete text, a problem also tackled in your work. While your paper proposes a discrete variational attention model to mitigate posterior collapse and enhance latent space, the cited paper experiments with methods like KL warm-up, word dropout, and KL coefficient to address this issue in a continuous latent space. Both works aim to improve VAE performance for text generation by addressing common VAE limitations, but they explore different approaches to the latent space and its properties.", "citation": "Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, Tat-Seng Chua (2023). MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space. arXiv:2305.12785. https://arxiv.org/abs/2305.12785"}, {"paper_id": 0, "text": ["Discrete Variational Attention Models for Language Generation\nXianghong Fang1 \u2217, Haoli Bai1 \u2217, Zenglin Xu2 , Michael Lyu1 , Irwin King1\n1The Chinese University of Hong Kong\n2Harbin Institute of Technology, Shenzhen\nxianghong fang@163.com, {hlbai,lyu,king}@cse.cuhk.edu.hk, zenglin@gmail.com\nAbstract\nVariational autoencoders have been widely applied\nfor natural language generation, however, there are\ntwo long-standing problems: information under-\nrepresentation and posterior collapse. The former\narises from the fact that only the last hidden state\nfrom the encoder is transformed to the latent space,\nwhich is insuf\ufb01cient to summarize data. The latter\ncomes as a result of the imbalanced scale between\nthe reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in\nthis paper we propose the discrete variational at-\ntention model with categorical distribution over the\nattention mechanism owing to the discrete nature\nin languages. Our approach is combined with an\nauto-regressive prior to capture the sequential de-\npendency from observations, which can enhance\nthe latent space for language generation. Moreover,\nthanks to the property of discreteness, the training\nof our proposed approach does not suffer from pos-\nterior collapse. Furthermore, we carefully analyze\nthe superiority of discrete latent space over the con-\ntinuous space with the common Gaussian distribu-\ntion. Extensive experiments on language genera-\ntion demonstrate superior advantages of our pro-\nposed approach in comparison with the state-of-\nthe-art counterparts.\n1\nIntroduction\nAs one of the representative of deep generative mod-\nels, variational autoencoders (VAEs) [Kingma and Welling,\n2013] have been widely applied in natural language gener-\nation [Wang and Wang, 2019; Fu et al., 2019; Li et al.,\n2019]. Given input text x, VAE learns the variational pos-\nterior q(z|x) through the encoder, and reconstructs the out-\nput from latent variables z via the decoder p(x|z). To gen-\nerate diverse sentences, the decoder p(x|z) heavily relies on\nthe samples drawn from the prior p(z) that controls the con-\ntents, topics and semantics for generation. However, despite\nthe successful applications of VAEs for language generation,\n\u2217Equal contribution in random order.\nthey majorly suffers from two long-standing challenges, i.e.,\ninformation underrepresentation and posterior collapse.\nFirst of all, in most variational language generation meth-\nods [Fu et al., 2019; He et al., 2019; Wang and Wang, 2019;\nLi et al., 2019], the latent space is derived from only the last\nhidden state of the encoder, and is therefore insuf\ufb01cient to\nsummarize the input. We call this challenge as information\nunderrepresentation. Intuitively, given an observed sentence,\nthe corresponding sequence of hidden states should be se-\nmantically correlated and representative during the phase of\nlanguage generation. Thus a potential solution is to enhance\nthe representation power via the attention mechanism [Bah-\ndanau et al., 2014], which can build the correlation between\nthe hidden states of the encoder and decoder. However, little\nefforts have been paid towards the utilization of attention in\nvariational language generation.\nThe next challenge is posterior collapse, a long stand-\ning phenomenon troubling the training of VAEs especially\nin the case of language generation.\nWhen the posterior\nfails to encode any knowledge from the observations, the\ndecoder therefore receives no signal at each time step dur-\ning generation.\nMany approaches have been proposed to\nalleviate the issue, for instance, annealing the KL diver-\ngence term [Bowman et al., 2015b; Kingma et al., 2017;\nFu et al., 2019], revising the model [Yang et al., 2017;\nSemeniuta et al., 2017; Xu and Durrett, 2018] and modifying\nthe training procedure [He et al., 2019; Li et al., 2019]. De-\nspite the effectiveness of these methods, the trade-off between\nthe reconstruction loss and the KL divergence is inevitable,\nand the phenomenon could still happen when the two terms\nare not properly scaled.\nAiming to address the above challenges, we propose the\nDiscrete Variational Attention Model (DVAM) with categor-\nical distributions over the attention mechanism. As shown in\nFigure 1, the proposed DVAM adopts two different RNNs as\nthe encoder network and the decoder network, respectively.\nIn order to better explore the prior context information for the\nlanguage generation phase, we introduce the latent stochas-\ntic variable z to build the semantic connection between the\nencoder hidden states and the decoder hidden states, via the\nattention [Bahdanau et al., 2014]. Considering that text in-\nputs x are more naturally modeled as a sequence of discrete\nsymbols rather than continuous ones, we explore the potential\nof quantized latent space in the attention mechanisum. The\narXiv:2004.09764v4  [cs.LG]  16 Jun 2021\n", "Table 3: Sampled Sentences on Yahoo Dataset.\nMethod\nSamples\npretraining+\n\u2022 i hate wandering, i just wan na know when the skies in the sky and the winds. [/s]\nFBP\n\u2022 where is it that morning when snow on thanksgiving ? what\u2019s the next weekend ? dress it!!!! my\nVAE\nmother was the teen mom and i love her and she just is going to be my show. [/s]\n\u2022 are they allowed to join (francisco) in UNK. giants in the \ufb01rst place.? check out other answers.\ndo you miss the economy and not taking risks in the merchant form, what would you tell? go to\nthe yahoo home page and ask what restaurants follow this one. [/s]\nGVAM\n\u2022 didn\u2019t i still worry, he loves porn and feels awful??? [/s]\n\u2022 if i aint divorced b4 the prom, and i wont worry, worry, i realy worry, and nobody feels awful,\nand i realy UNK sometime, i wont worry, and eventually. [/s]\n\u2022 what is the worst and worst moment and the worst and worst, and a stranger and hugs, and nobody,\nand i deserve it, and nobody feels awful and i wont worry and worry. [/s]\nDVAM\n\u2022 i need to start a modeling company ! any suggestions on what is a reliable topic? [/s]\n(K=512)\n\u2022 does anyone agree, there is a global warming of the earth? in general. there are several billion things,\nincluding the earth, solar system. [/s]\n\u2022 is anyone willing to donate plasma if you are allergic to cancer or anything else? probably you can.\ni\u2019ve never done any thing but it is only that dangerous to kill bacteria. i have heard that it doesn\u2019t have\nany effect on your immune system. [/s]\n(a) Code book size K\n(b) Maximum regularizer \u03b2max\n(c) Latent dimension of ezt\nFigure 3: Ablation studies of DVAM.\ngests the samples from the prior indeed contain sequential\ndependency that bene\ufb01ts the generation.\n4.3\nAblation Studies\nBy default, all the ablation studies are conducted on Yahoo\ndataset with the default parameter settings.\nCode Book Size K\nWe begin with the effect of different code book size K on\nthe reconstruction loss and KL divergence for language mod-\nelling.\nWe vary K \u2208{128, 256, 512, 1024}, and the re-\nsults are shown in Figure 3(a). It can be observed that as\nK increases, the Rec loss decreases while KL increases, both\nmonotonically. The results are also consistent to Table 2 by\nincreasing K from 128 to 512. Such phenomenons are in-\ntuitive, since a larger K improves model capacity but poses\nmore challenges for training the auto-regressive prior. Conse-\nquently, one should properly choose the code book size, such\nthat the prior can approximate the posterior well, and yet the\nposterior is representative enough for the sequential depen-\ndency.\nMaximum Regularizer \u03b2max\nThen we tune the maximum regularizer \u03b2max, which con-\ntrols the distance of the continuous hidden state he\n1:T to the\ncode books {ek}K\nk=1. Recall that a small \u03b2max loosely re-\nstricts the continuous space he\n1:T to the code book, making\nthe quantization process hard to converge. On the other hand,\nif \u03b2max is too large, he\n1:T could easily get stuck in some lo-\ncal minimal during the training. Therefore, it is necessary to\n\ufb01nd a proper trade-off between the two situations. We vary\n\u03b2max \u2208{0.1, 0.2, 0.5, 1, 5, 10, 20}, and the results is shown\nin Figure 3(b). We can \ufb01nd that when \u03b2max = 5, the algo-\nrithm achieves the lowest Rec, while smaller or larger \u03b2max\nboth lead to higher Recs.\nDimension of Code Book Vectors\nFinally,\nwe\nvary\nlatent\ndimension\nof\n{ek}K\nk=1\nin\n{8, 16, 32, 64, 128, 256},\nand the results are shown in\nFigure 3(c). We \ufb01nd that the performance of language mod-\nelling is relatively robust to the choice of latent dimension.\nIntuitively, in the continuous space the dimension of latent\nvariables is closely related to the model capacity. However,\nin the discrete case, the capacity of the model is largely\ndetermined by the code book size K instead of the latent\ndimension, which is also veri\ufb01ed in Table 2 and Figure 3(a).\n5\nConclusion\nIn this paper, we propose discrete variational attention model,\na new algorithm for natural language generation. Our pro-\nposed approach can address the issues of information under-\nrepresentation and posterior collapse. Moreover we also care-\nfully analyze the advantages of discreteness over continuity\nin variational attention models. Extensive experiment results\non benchmark language modelling datasets demonstrate the\nsuperiority of our proposed approach. As a future direction,\nour approach can be applied for more applications of natu-\nral language processing such as text summarization, dialogue\nsystems and poetry generation.\n", "advantages of quantized representation have also been em-\npirically justi\ufb01ed in the recently proposed vector quantized\nvariational autoencoder (VQVAE) [van den Oord et al., 2017;\nRoy et al., 2018] in learning the sequentially correlated prior\nfor image generation. We further show that the quantized rep-\nresentation can avoid DVAM trapping in posterior collapse\n(when KL divergence DKL(q(z|x)||p(z)) \u21920)\u2014since the\nvariational posterior q(z|x) is a discrete distribution, the KL\ndivergence is not differentiable with its parameters and hence\nnot involved during model training. This can also let us learn\nthe variational posterior and prior separately. We \ufb01rst train\nDVAM until convergence where the posterior successfully\nencodes the sequential dependency from observations. Then\nwe deploy informative context priors, such as a separate auto-\nregressive prior [van den Oord et al., 2016b], to learn the\nsequential dependency from the well-trained posterior, after\nwhich we can sample diverse and representative latent se-\nquences from the prior for sentence generation. Furthermore,\nwe provide detailed analysis on the advantages of the quan-\ntized latent space over continual latent space. Finally, we\nevaluate the proposed model on several benchmark datasets\nfor language modelling, and experimental results demonstrate\nthe superiority of DVAM in generating languages over its\ncounterparts.\nOur contributions can thus be summarised as:\n1. We propose the discrete variational attention model with\nan auto-regressive prior to capture the sequential depen-\ndency in the latent space, such that issues of information\nunderrepresentation and posterior collapse can be effec-\ntively tackled.\n2. We carefully analyze reasons why discrete latent space\nwith categorical distribution in our model is preferred\nthan the continuous space with the commonly Gaussain\ndistribution for language generation.\n3. Experimental results on benchmark datasets demon-\nstrate the advantages of our DVAM against state-of-the-\nart baselines in language generation.\n2\nBackground\n2.1\nVariational Antoencoders for Language\nGeneration\nVariational Autoencoders (VAEs) [Kingma and Welling,\n2013] are well known class of generative models.\nGiven\nobservations x, we seek to infer latent variables z from\nwhich new observations \u02c6x can be generated. To achieve this,\nwe need to maximize the marginal log likelihood log p\u03b8(x),\nwhich is usually intractable due to the complex posterior\np(z|x). Consequently an approximate posterior q\u03c6(z|x) (i.e.\nthe encoder) is introduced, and the evidence lower bound\n(ELBO) of the marginal likelihood is maximized as follows:\nlog p\u03b8(x) \u2265Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)]\n|\n{z\n}\nreconstruction loss\n\u2212DKL(q\u03c6(z|x)\u2225p(z))\n|\n{z\n}\nKL divergence\n,\n(1)\nwhere p\u03b8(x|z) represents likelihood function conditioned on\nlatent codes z, also known as the decoder. \u03b8 and \u03c6 are the\ncorresponding parameters.\nIn language generation, VAEs are used to learn the map-\nping from the latent space of z to the observations x. Based\non this mapping, new sentences can be effectively generated.\nVAEs are usually armed with an RNN encoder and decoder,\nwhere the input sentences x are given to the encoder, the la-\ntent variables z are derived from the last hidden states he\nT\nvia the reparameterization trick [Kingma and Welling, 2013]\nz = \u00b5(he\nT ) + \u03c3(he\nT )\u03f5 with \u03f5 \u223cN(0, I). To generate new\nsentences, we sample latent variables z from the prior that\nare forwarded to the decoder to generate sequences \u02c6x by\np(\u02c6x1:T |z) =\nT\nY\nt=2\np(\u02c6xt|\u02c6xt\u22121, z) \u00b7 p(\u02c6x1|z).\n(2)\n2.2\nDiminishing Effect of Latent Variables\nModeling RNN-based language models with VAEs is prone\nto suffer from the diminishing effect of latent variables, i.e.,\nthe generated sequences \u02c6x are loosely dependent on z and\nthereon the learned latent space plays no role in generation.\nThe diminishing effect largely comes from two aspects:\nInformation Underrepresentation.\nWhile sentences x are\nfed into the encoder, the corresponding latent variables z are\nobtained through the transformation of only last hidden state\nhe\nT of the encoder. The resulting latent space, however, is\nusually insuf\ufb01cient to summarize the observations x. The\nrich semantics in the whole sequences are thereon lost in z,\nknown as the information underrepresentation. During gen-\neration, such latent variables z are forwarded to the decoder,\nwhich cannot effectively guide the decoder to generate sen-\ntences with high correlation and quality.\nSome recent at-\ntempts [Bahuleyan et al., 2018; Deng et al., 2018] borrow\nideas from the attention mechanism [Bahdanau et al., 2014],\nand introduce variational distributions over the context vec-\ntor, however, they use uninformative prior that may fail to\nsample sequentially correlated latent variables for sentence\ngeneration.\nPosterior Collapse.\nPosterior collapse usually arises as\nDKL(q\u03c6(z|x)\u2225p(z)) in Equation (1) diminishes to zero,\nwhere the local optimal gives q\u03c6(z|x) = p(z). When pos-\nterior collapse happens, we can verify that x are independent\nof z by p(x)p(z) = p(x)q\u03c6(z|x) = p(x) p(x,z)\np(x)\n= p(x, z).\nTherefore, the encoder learns a data-agnostic posterior (i.e.,\nthe standard Normal distribution) without any information\nfrom the observations x, while the decoder learns to gener-\nate itself without actually relying on the latent variable z.\nPosterior collapse happens inevitably as the ELBO con-\ntains both the reconstruction loss Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)] and\nthe KL-divergence DKL(q\u03c6(z|x)\u2225p(z)). When the scales be-\ntween the two terms are not properly balanced, it could easily\nmake the KL divergence over optimized. Moreover, given a\npowerful and auto-regressive decoder, the decoder itself can\nlearn to generate sentences \u02c6x without actually relying on z.\n3\nMethods\nIn order to tackle the diminishing effect of latent variables\nin variational language models, we \ufb01rst present the Discrete\nVariational Attention Model (DVAM), and then introduce\nmeaningful priors for language generation.\n", "differentiable, we adopt the widely used straight through es-\ntimator (STE) [Bengio et al., 2013]] to copy gradients from\nezt to he\nt, as is shown in Figure 1, In terms of the code books\n{ek}K\nk=1, since Equation (7) is non-differentiable with them,\nwe \ufb01rst apply the K-means algorithm to calculate the average\nover all latent variables he\n1:T that are closest to {ek}K\nk=1, then\nwe take exponential moving average over the code books so\nas to stabilize the mini-batch update.\nAuto-regressive Prior for Language Generation\nDespite\nthe optimization of Equation (7) does not rely on the choice\nof the prior, however, the form of prior does affect the process\nof language generation. Recall that in the training phase, la-\ntent variables z1:T \u223cq\u03c6(z1:T |x) are sampled conditioned on\nthe input x, and therefore the posterior learns to encode the\nsequential dependency in the latent space. Nevertheless, to\ngenerate new sentences after network training, we \ufb01rst sam-\nple z1:T unconditionally from the prior and then pass it to the\ndecoder for generation, where the sequential dependency can\nhardly be guaranteed. As a result, the decoder cannot receive\nthe structured supervision from the latent space for valid gen-\neration.\nTo solve the problem, we seek to \ufb01nd a auto-regressive\nprior p\u03c8(z1:T ) = p\u03c8(z1) QT\nt=2 p\u03c8(zt|z1:t\u22121) parameterized\nby \u03c8 such that it has enough capacity to capture the underly-\ning sequential structures in the posterior q\u03c6(z1:T |x). Towards\nthat end, we adopt a PixelCNN [van den Oord et al., 2016a]\nto learn the prior. Unlike PixelCNN models on images, we\nuse a 16-layer residual 1-dimensional convolutional network\nto swap over the latent sequence. In order to learn the se-\nquential dependency in the posterior q\u03c6(z1:T |x), we \ufb01rst train\nDVAM using Equation (7) until convergence, and then min-\nimize the KL divergence P\nt DKL(q\u03c6(zt|x)||p\u03c8(zt|z1:t\u22121))\nw.r.t \u03c8, which reduces to the cross entropy loss according to\nEquation (6).\n3.3\nDiscussion on the Distribution of z\nAs mentioned earlier in Section 3.1, a simple idea is to di-\nrectly assign Gaussian distributions over the attention, i.e.\nzt = \u00b5t(he\nt) + \u03c3t(he\nt)\u03f5 for \u03f5 \u223cN(0, I), leading to the Gaus-\nsian Variational Attention Model (GVAM).\nTo analyze the defects of GVAM, we \ufb01rst recall the KL\ndivergence between two Gaussian distributions, which can be\nwritten as follows:\nT\nX\nt=1\nDKL(q\u03c6(zt|x)\u2225p\u03c8(zt|z1:t\u22121))\n=\nT\nX\nt=1\nD\nX\nd=1\n1\n2(log \u02c6\u03c32\ntd\n\u03c32\ntd\n\u22121 + \u03c32\ntd + (\u02c6\u00b5td \u2212\u00b5td)2\n\u02c6\u03c32\ntd\n),\n(8)\nwhere D is the latent dimension of zt. We denote {\u00b5td, \u03c3td}\nand {\u02c6\u00b5td, \u02c6\u03c3td} as the parameters of the posterior and prior\ndistributions, respectively.\nUnlike the objective of DVAM\nin Equation (6), we \ufb01nd that Equation (8) is differentiable\nw.r.t to variational parameters \u03c6, and therefore should be in-\ncluded for the minmization of ELBO. As we also need an\nauto-regressive prior during generation, such differentiation\nbrings the challenge for GVAM. Unlike DVAM that learns\nthe posterior q\u03c6(z1:T |x) in advance to teach the prior, GVAM\nneeds to involve the posterior and prior jointly during the op-\ntimization, i.e.,\nmin\n\u03c6,\u03b8,\u03c8 \u2212Ez1:T \u223cq\u03c6 log p\u03b8(x|z1:T )+DKL(q\u03c6(z1:T |x)\u2225p\u03c8(z1:T )).\n(9)\nSuch optimization can be easily troubled by posterior col-\nlapse due to two aspects. Firstly, the scale of the KL diver-\ngence increases linearly to the length of the sequence, and a\nlarge scale usually overlooks the minimization of the recon-\nstruction loss. The training thereon is unstable across vari-\nous observations x with different lengths. Secondly and more\nseriously, both \u03c6 and \u03c8 are used to minimize the KL diver-\ngence. Whenever q\u03c6(z1:T |x) collapses to p\u03c8(z1:T ) before the\nq\u03c6(z1:T |x) learns any sequential dependency from the obser-\nvations, both q\u03c6(z1:T |x) and p\u03c8(z1:T ) are trapped to the local\noptimal that cannot provide structural supervision to the de-\ncoder during both training and generation.\n4\nExperiments\nIn this section, we verify the advantages of our proposed\nDVAM for language generation. We \ufb01rst perform language\nmodelling on three benchmark datasets, which measures the\nmodel capacity of different approaches. Then we investigate\nthe training of the auto-regressive prior, and evaluate the gen-\nerated sentences from these approaches. Finally we conduct a\nset of ablation studies to shed more lights into DVAM. Codes\nimplemented by Pytorch will be released on Github.\n4.1\nExperimental Setup\nWe take three benchmark datasets of language modelling for\nveri\ufb01cation: Yahoo Answers [Xu and Durrett, 2018], Penn\nTree [Marcus et al., 1993], and a down-sampled version of\nSNLI [Bowman et al., 2015a]. A summary of dataset statis-\ntics is shown in Table 1.\nTable 1: Dataset statistics.\nDatasets\nTrain Size\nVal Size\nTest Size\nAvg Len\nYahoo\n100,000\n10,000\n10,000\n78.7\nPTB\n42,068\n3,370\n3,761\n23.1\nSNLI\n100,000\n10,000\n10,000\n9.7\nWe compare the proposed DVAM against a number\nof baselines, including the classical LSTM-LM, vanilla\nVAE [Kingma and Welling, 2013], as well as its advanced\nvariants, e.g. annealing VAE [Bowman et al., 2015b], cyclic\nannealing VAE1 [Fu et al., 2019], lagging VAE2 [He et\nal., 2019], Free Bits (FB) [Kingma et al., 2017] and pre-\ntraining+FBP VAE3 [Li et al., 2019]. We also compare to\nour closest counterpart, i.e., Gaussian Variational Attention\nModel (GVAM) so as to directly verify the advantages of dis-\ncreteness in the variational attention model.\nWe evaluate the performance of language generation mod-\nels using three metrics, i.e., the reconstruction loss (Rec) cal-\nculated as Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)] for measuring the ability\nto recover data from latent space (the lower the better), the\n1https://github.com/haofuml/cyclical annealing\n2https://github.com/jxhe/vae-lagging-encoder\n3https://github.com/bohanli/vae-pretraining-encoder\n"], "summary": "The cited paper, \"Discrete Variational Attention Models for Language Generation,\" directly addresses the core problems of information under-representation and posterior collapse in VAEs for natural language generation, which are also central to your research. It proposes a novel discrete variational attention model with a categorical distribution over the attention mechanism, combined with an auto-regressive prior, to overcome these issues. The paper further analyzes the benefits of discrete latent space over continuous space, providing a strong foundation and comparative analysis for your work. Your abstract is nearly identical to the cited paper's abstract, indicating a direct and strong relationship between your work and this paper.", "citation": "Xianghong Fang, Haoli Bai, Zenglin Xu, Michael Lyu, Irwin King (2020). Discrete Variational Attention Models for Language Generation. arXiv:2004.09764. https://arxiv.org/abs/2004.09764"}, {"paper_id": 48, "text": ["Encoder\n\ud835\udc61= 0\nEncoder\n\ud835\udc61= 1\nEncoder\n\ud835\udc61= \ud835\udc41\n\ud835\udc64'\n\ud835\udc64(\n\ud835\udc64)\n\u210e'\n\u210e(\nDecoder\n\ud835\udc64'\n+ \ud835\udc64(\n+ \u2026\ud835\udc64)\n+\n\u210e)\n\ud835\udca9(0,1)\n\ud835\udc3e\ud835\udc3f\n\ud835\udc67\n\ud835\udf07\n\ud835\udf0e7\n(a)\nEncoder\n\ud835\udc61= 0\nEncoder\n\ud835\udc61= 1\nEncoder\n\ud835\udc61= \ud835\udc41\n\ud835\udc64'\n\ud835\udc64(\n\ud835\udc64)\n\u210e'\n\u210e(\n\u210e)\nDecoder\n\ud835\udc64'\n+ \ud835\udc64(\n+ \u2026\ud835\udc64)\n+\n\ud835\udca9(0,1)\n\ud835\udc3e\ud835\udc3f\n\ud835\udf07\n\ud835\udf0e6 \ud835\udc67\n\ud835\udf07\ud835\udf0e6\n\ud835\udca9(0,1)\n\ud835\udc3e\ud835\udc3f\n\ud835\udf07\ud835\udf0e6\n\ud835\udca9(0,1)\n\ud835\udc3e\ud835\udc3f\n(b)\nFigure 1: (a) The typical architecture of RNN-based VAE; (b) the proposed HR-VAE architecture.\nis generic and can be applied to any existing VAE-\nRNN-based architectures.\nWe evaluate our model against several strong\nbaselines\nwhich\napply\nVAE\nfor\ntext\nmod-\nelling (Bowman et al., 2016; Yang et al., 2017;\nXu and Durrett, 2018).\nWe conducted experi-\nments based on two public benchmark datasets,\nnamely, the Penn Treebank dataset (Marcus and\nMarcinkiewicz, 1993) and the end-to-end (E2E)\ntext generation dataset (Novikova et al., 2017).\nExperimental results show that our HR-VAE\nmodel not only can effectively mitigate the latent\nvariable collapse issue with a stable training pro-\ncess, but also can give better predictive perfor-\nmance than the baselines, as evidenced by both\nquantitative (e.g., negative log likelihood and per-\nplexity) and qualitative evaluation. The code for\nour model is available online2.\n2\nMethodology\n2.1\nBackground of VAE\nA variational autoencoder (VAE) is a deep genera-\ntive model, which combines variational inference\nwith deep learning. The VAE modi\ufb01es the conven-\ntional autoencoder architecture by replacing the\ndeterministic latent representation z of an input x\nwith a posterior distribution P(z|x), and imposing\na prior distribution on the posterior, such that the\nmodel allows sampling from any point of the latent\nspace and yet able to generate novel and plausible\noutput. The prior is typically chosen to be stan-\ndard normal distributions, i.e., P(z) = N(0, 1),\nsuch that the KL divergence between posterior and\nprior can be computed in closed form (Kingma\nand Welling, 2013).\nTo train a VAE, we need to optimise the\nmarginal likelihood P\u03b8(x) =\nR\nP(z)P\u03b8(x|z)dz,\n2https://github.com/ruizheliUOA/HR-VAE\nwhere the log likelihood can take following form:\nlog P\u03b8(x) = L(\u03b8, \u03c6; x) + KL (Q\u03c6(z|x)\u2225P\u03b8(z|x))\n(1)\nL(\u03b8, \u03c6; x) = EQ\u03c6(z|x)[log P\u03b8(x|z)]\n\u2212KL (Q\u03c6(z|x)\u2225P(z))\n(2)\nHere Q\u03c6(z|x) is the variational approximation for\nthe true posterior P\u03b8(z|x). Speci\ufb01cally, Q\u03c6(z|x)\ncan be regarded as an encoder (a.k.a. the recogni-\ntion model) and P\u03b8(x|z) the decoder (a.k.a. the\ngenerative model).\nBoth encoder and decoder\nare implemented via neural networks. As proved\nin (Kingma and Welling, 2013), optimising the\nmarginal log likelihood is essentially equivalent\nto maximising L(\u03b8, \u03c6; x), i.e., the evidence lower\nbound (ELBO), which consists of two terms. The\n\ufb01rst term is the expected reconstruction error in-\ndicating how well the model can reconstruct data\ngiven a latent variable. The the second term is the\nKL divergence of the approximate posterior from\nprior, i.e., a regularisation pushing the learned pos-\nterior to be as close to the prior as possible.\n2.2\nVariational Autoendoder with Holistic\nRegularisation\nIn this section, we discuss the technical details\nof the proposed holistic regularisation VAE (HR-\nVAE) model, a general architecture which can ef-\nfectively mitigate the KL vanishing phenomenon.\nOur model design is motivated by one notice-\nable defect shared by the VAE-RNN based mod-\nels in previous works (Bowman et al., 2016; Yang\net al., 2017; Xu and Durrett, 2018; Dieng et al.,\n2019). That is, all these models, as shown in Fig-\nure 1a, only impose a standard normal distribution\nprior on the last hidden state of the RNN encoder,\nwhich potentially leads to learning a suboptimal\nrepresentation of the latent variable and results in\nmodel vulnerable to KL loss vanishing. Our hy-\npothesis is that to learn a good representation of\n", "Marco Fraccaro, S\u00f8ren Kaae S\u00f8nderby, Ulrich Paquet,\nand Ole Winther. 2016. Sequential neural models\nwith stochastic layers. In Advances in neural infor-\nmation processing systems, pages 2199\u20132207.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDiederik P Kingma and Max Welling. 2013.\nAuto-\nencoding\nvariational\nbayes.\narXiv\npreprint\narXiv:1312.6114.\nMitchell P Marcus and Mary Ann Marcinkiewicz.\n1993.\nBuilding a large annotated corpus of en-\nglish: The penn treebank. Computational Linguis-\ntics, 19(2).\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111\u20133119.\nJekaterina Novikova, Ond\u02c7rej Du\u02c7sek, and Verena Rieser.\n2017. The e2e dataset: New challenges for end-to-\nend generation. In Proceedings of the 18th Annual\nSIGdial Meeting on Discourse and Dialogue, pages\n201\u2013206.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2017. A hybrid convolutional variational au-\ntoencoder for text generation. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 627\u2013637.\nCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e,\nS\u00f8ren Kaae S\u00f8nderby, and Ole Winther. 2016. How\nto train deep variational autoencoders and proba-\nbilistic ladder networks. In 33rd International Con-\nference on Machine Learning (ICML 2016).\nShang-Yu Su, Kai-Ling Lo, Yi Ting Yeh, and Yun-\nNung Chen. 2018. Natural language generation by\nhierarchical decoding with linguistic patterns.\nIn\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 61\u201366.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2018. Learning neural templates for text generation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3174\u20133187.\nJiacheng Xu and Greg Durrett. 2018. Spherical latent\nspaces for stable variational autoencoders. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4503\u2013\n4513.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017.\nImproved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning-Volume 70,\npages 3881\u20133890. JMLR. org.\n", "A Stable Variational Autoencoder for Text Modelling\nRuizhe Li\u2660, Xiao Li\u2660, Chenghua Lin\u2665, Matthew Collinson\u2660and Rui Mao\u2660\n\u2660Department of Computing Science, University of Aberdeen, UK\n{r02rl17, x.li, matthew.collinson, r03rm16}@abdn.ac.uk\n\u2665Department of Computer Science, University of Shef\ufb01eld, UK\nc.lin@sheffield.ac.uk\nAbstract\nVariational Autoencoder (VAE) is a powerful\nmethod for learning representations of high-\ndimensional data. However, VAEs can suffer\nfrom an issue known as latent variable collapse\n(or KL loss vanishing), where the posterior\ncollapses to the prior and the model will ig-\nnore the latent codes in generative tasks. Such\nan issue is particularly prevalent when em-\nploying VAE-RNN architectures for text mod-\nelling (Bowman et al., 2016). In this paper,\nwe present a simple architecture called holistic\nregularisation VAE (HR-VAE), which can ef-\nfectively avoid latent variable collapse. Com-\npared to existing VAE-RNN architectures, we\nshow that our model can achieve much more\nstable training process and can generate text\nwith signi\ufb01cantly better quality.\n1\nIntroduction\nVariational Autoencoder (VAE) (Kingma and\nWelling, 2013) is a powerful method for learning\nrepresentations of high-dimensional data. How-\never, recent attempts of applying VAEs to text\nmodelling are still far less successful compared\nto its application to image and speech (Bach-\nman, 2016; Fraccaro et al., 2016; Semeniuta et al.,\n2017). When applying VAEs for text modelling,\nrecurrent neural networks (RNNs)1 are commonly\nused as the architecture for both encoder and de-\ncoder (Bowman et al., 2016; Xu and Durrett, 2018;\nDieng et al., 2019).\nWhile such a VAE-RNN\nbased architecture allows encoding and generating\nsentences (in the decoding phase) with variable-\nlength effectively, it is also vulnerable to an issue\nknown as latent variable collapse (or KL loss van-\nishing), where the posterior collapses to the prior\nand the model will ignore the latent codes in gen-\nerative tasks.\n1NB: here we refer RNN to any type of recurrent neural\narchitectures including LSTM and GRU.\nVarious efforts have been made to alleviate the\nlatent variable collapse issue.\nBowman et al.\n(2016) uses KL annealing, where a variable weight\nis added to the KL term in the cost function at\ntraining time. Yang et al. (2017) discovered that\nthere is a trade-off between the contextual capacity\nof the decoder and effective use of encoding infor-\nmation, and developed a dilated CNN as decoder\nwhich can vary the amount of conditioning con-\ntext. They also introduced a loss clipping strategy\nin order to make the model more robust. Xu and\nDurrett (2018) addressed the problem by replacing\nthe standard normal distribution for the prior with\nthe von Mises-Fisher (vMF) distribution.\nWith\nvMF, the KL loss only depends on the concentra-\ntion parameter which is \ufb01xed during training and\ntesting, and hence results in a constant KL loss. In\na more recent work, Dieng et al. (2019) avoided\nlatent variable collapse by including skip connec-\ntions in the generative model, where the skip con-\nnections enforce strong links between the latent\nvariables and the likelihood function.\nAlthough the aforementioned works show ef-\nfectiveness in addressing the latent variable col-\nlapse issue to some extent, they either require\ncarefully engineering to balance the weight be-\ntween the reconstruction loss and KL loss (Bow-\nman et al., 2016; S\u00f8nderby et al., 2016), or re-\nsort to designing more sophisticated model struc-\ntures (Yang et al., 2017; Xu and Durrett, 2018; Di-\neng et al., 2019).\nIn this paper, we present a simple architec-\nture called holistic regularisation VAE (HR-VAE),\nwhich can effectively avoid latent variable col-\nlapse. In contrast to existing VAE-RNN models\nfor text modelling which merely impose a standard\nnormal distribution prior on the last hidden state\nof the RNN encoder, our HR-VAE model imposes\nregularisation for all hidden states of the RNN en-\ncoder. Another advantage of our model is that it\narXiv:1911.05343v1  [cs.CL]  13 Nov 2019\n", "data and a good generative model, it is crucial to\nimpose the standard normal prior on all the hidden\nstates of the RNN-based encoder (see Figure 1b),\nwhich allows a better regularisation of the model\nlearning process.\nWe implement the HR-VAE model using a two-\nlayer LSTM for both the encoder and decoder.\nHowever, one should note that our architecture can\nbe readily applied to other types of RNN such\nas GRU. For each time stamp t (see Figure 1b),\nwe concatenate the hidden state ht and the cell\nstate ct of the encoder. The concatenation (i.e.,\n[ht; ct]) is then fed into two linear transformation\nlayers for estimating \u00b5t and \u03c32\nt , which are pa-\nrameters of a normal distribution corresponding to\nthe concatenation of ht and ct. Let Q\u03c6t(zt|x) =\nN(zt|\u00b5t, \u03c32\nt ), we wish Q\u03c6t(zt|x) to be close to a\nprior P(zt), which is a standard Gaussian. Finally,\nthe KL divergence between these two multivariate\nGaussian distributions (i.e., Q\u03c6t and P(zt)) will\ncontribute to the overall KL loss of the ELBO.\nBy taking the average of the KL loss at each time\nstamp t, the resulting ELBO takes the following\nform\nL(\u03b8, \u03c6; x) = EQ\u03c6(zN|x)[log P\u03b8(x|zN)]\n\u22121\nN\nN\nX\nt=0\nKL(Q\u03c6t(zt|x)\u2225P(zt)).\n(3)\nAs can be seen in Eq. 3, our solution to the KL\ncollapse issue does not require any engineering for\nbalancing the weight between the reconstruction\nterm and KL loss as commonly the case in exist-\ning works (Bowman et al., 2016; S\u00f8nderby et al.,\n2016). The weight between these two terms of our\nmodel is simply 1 : 1.\n3\nExperimental Setup\n3.1\nDatasets\nWe evaluate our model on two public datasets,\nnamely,\nPenn Treebank (PTB) (Marcus and\nMarcinkiewicz, 1993) and the end-to-end (E2E)\ntext generation corpus (Novikova et al., 2017),\nwhich have been used in a number of previous\nworks for text generation (Bowman et al., 2016;\nXu and Durrett, 2018; Wiseman et al., 2018;\nSu et al., 2018).\nPTB consists of more than\n40,000 sentences from Wall Street Journal articles\nwhereas the E2E dataset contains over 50,000 sen-\ntences of restaurant reviews. The statistics of these\ntwo datasets are summarised in Table 1.\n3.2\nImplementation Details\nFor the PTB dataset, we used the train-test split\nfollowing (Bowman et al., 2016; Xu and Dur-\nrett, 2018).\nFor the E2E dataset, we used the\ntrain-test split from the original dataset (Novikova\net al., 2017) and indexed the words with a fre-\nquency higher than 3. We represent input data with\n512-dimensional word2vec embeddings (Mikolov\net al., 2013). We set the dimension of the hidden\nlayers of both encoder and decoder to 256. The\nAdam optimiser (Kingma and Ba, 2014) was used\nfor training with an initial learning rate of 0.0001.\nEach utterance in a mini-batch was padded to the\nmaximum length for that batch, and the maximum\nbatch-size allowed is 128.\n3.3\nBaselines\nWe compare our HR-VAE model with three strong\nbaselines using VAE for text modelling:\nVAE-LSTM-base3:\nA variational autoencoder\nmodel which uses LSTM for both encoder and de-\ncoder. KL annealing is used to tackled the latent\nvariable collapse issue (Bowman et al., 2016);\nVAE-CNN4: A variational autoencoder model\nwith a LSTM encoder and a dilated CNN de-\ncoder (Yang et al., 2017);\nvMF-VAE5: A variational autoencoder model us-\ning LSTM for both encoder and decoder where\nthe prior distribution is the von Mises-Fisher\n(vMF) distribution rather than a Gaussian distri-\nbution (Xu and Durrett, 2018).\n4\nExperimental Results\nWe evaluate our HR-VAE model in two experi-\nmental settings, following the setup of (Bowman\net al., 2016; Xu and Durrett, 2018). In the standard\nsetting, the input to the decoder at each time stamp\nis the concatenation of latent variable z and the\nground truth word of the previous time stamp. Un-\nder this setting, the decoder will be more power-\nful because it uses the ground truth word as input,\nresulting in little information of the training data\ncaptured by latent variable z. The inputless set-\nting, in contrast, does not use the previous ground\ntruth word as input for the decoder. In other words,\n3https://github.com/timbmg/Sentence-VAE\n4https://github.com/ke\ufb01rski/contiguous-succotash\n5https://github.com/jiacheng-xu/vmf vae nlp\n"], "summary": "The cited paper, \"A Stable Variational Autoencoder for Text Modelling,\" proposes HR-VAE to address the latent variable collapse issue in VAE-RNN models for text generation, a problem also tackled by your work. While your paper focuses on discrete variational attention and an autoregressive prior, the cited paper introduces a holistic regularization approach by imposing a standard normal prior on all hidden states of the RNN encoder, rather than just the last one. This directly relates to your mention of \"information under-representation\" in VAEs, as both papers aim to improve the latent space's ability to summarize data and prevent posterior collapse. The cited work provides a comparative baseline for VAE-based text generation models, which is relevant to your claim of superior performance against state-of-the-art counterparts.", "citation": "Heng Wang, Zengchang Qin, Tao Wan (2017). Text Generation Based on Generative Adversarial Nets with Latent Variable. arXiv:1712.00170. https://arxiv.org/abs/1712.00170"}, {"paper_id": 55, "text": ["learnable distribution given by\np\u03b4(z) = 1\nK\nPK\nk=1q\u03c6(z|sk),\n(12)\nwhere q\u03c6 is the learned approximate posterior, and\nwe call the parameter \u03b4 := {sk}K\nk=1 pseudo in-\nputs. Intuitively, p\u03b4(z) seeks to match the aggre-\ngated posterior (Makhzani et al., 2015): q(z) =\n1\nN\nPN\ni=1 q\u03c6(z|xi) in a cost-ef\ufb01cient manner via\nparameterizing the pseudo inputs. By replacing the\nprior distribution p(z) in (10) with p\u03b4(z), we com-\nplete the \ufb01nal objective of the proposed APo-VAE.\nThe detailed training procedure is summarized in\nAlgorithm 1.\n4\nRelated Work\nVAE for Text Generation.\nMany VAE models\nhave been proposed for text generation, most of\nwhich focus on solving the issue of posterior col-\nlapse. The most popular strategy is to alter the\ntraining dynamics, keeping the encoder away from\nbad local optima. For example, variants of KL an-\nnealing (Bowman et al., 2016; Zhao et al., 2018; Fu\net al., 2019) dynamically adjust the weight on the\nKL penalty term as training progresses. Lagging\nVAE (He et al., 2019) aggressively optimizes the\nencoder before each decoder update, to overcome\nthe imbalanced training issue between the encoder\nand decoder. Alternative strategies have also been\nproposed based on competing theories or heuristics.\n\u03b4-VAE (Razavi et al., 2019) tackles this issue by\nenforcing a minimum KL divergence between the\nposterior and the prior. Yang et al. (2017) blames\nmode-collapse on the auto-regressive design of the\ndecoder and advocates alternative architectures. A\nsemi-amortized inference network is considered\nby Kim et al. (2018) to bridge the amortization gap\nbetween log-likelihood and the ELBO.\nRecent work has also shown that posterior col-\nlapse can be ameliorated by using more expressive\npriors and variational posteriors other than Gaus-\nsian. Flow-based VAE is considered in Ziegler\nand Rush (2019) to enhance the \ufb02exibility of prior\ndistributions. A topic-guided prior is proposed\nin Wang et al. (2019a) to achieve more controllable\ntext generation. Fang et al. (2019) explores implicit\nsample-based representations, without requiring an\nexplicit density form for the approximate poste-\nrior. Xu and Durrett (2018) considers replacing\nthe Gaussian with the spherical von Mises-Fisher\n(vMF) distribution. Compared to these prior arts,\nour model features structured representation in hy-\nperbolic space, which not only captures latent hier-\narchies but also combats posterior collapse.\nHyperbolic Space Representation Learning.\nThere has been a recent surge of interest in rep-\nresentation learning in hyperbolic space, largely\ndue to its exceptional effectiveness modeling data\nwith underlying graphical structure (Chamberlain\net al., 2017), such as relation nets (Nickel and\nKiela, 2017). In the context of NLP, hyperbolic\ngeometry has been considered for word embed-\ndings (Tifrea et al., 2018).\nA popular vehicle\nfor hyperbolic representation learning is the auto-\nencoder (AE) framework (Grattarola et al., 2019;\nOvinnikov, 2019), where the decoders are built to\nef\ufb01ciently exploit the hyperbolic geometry (Ganea\net al., 2018). Closest to our APo-VAE are the works\nof hyperbolic VAEs (Mathieu et al., 2019; Nagano\net al., 2019), where wrapped normal distributions\nhave been used. Drawing power from the dual form\nof the KL, the proposed APo-VAE highlights an\nimplicit posterior and data-driven prior, showing\nimproved training stability.\n5\nExperiments\nWe evaluate the proposed model on three tasks: (i)\nlanguage modeling, (ii) unaligned style transfer,\nand (iii) dialog-response generation, with quan-\ntitative results, human evaluation and qualitative\nanalysis.\n5.1\nExperimental Setup\nDatasets.\nWe use three datasets for language\nmodeling: Penn Treebank (PTB) (Marcus et al.,\n1993), Yahoo and Yelp corpora (Yang et al., 2017).\nPTB contains one million words of 1989 Wall\nStreet Journal material annotated in Treebank II\nstyle, with 42k sentences of varying lengths. Yahoo\nand Yelp are much larger datasets, each containing\n100k sentences with greater average length.\nFor unaligned style transfer, we use the Yelp\nrestaurant reviews dataset (Shen et al., 2017),\nwhich is obtained by pre-processing the Yelp\ndataset, i.e., sentences are shortened for more feasi-\nble sentence level sentiment analysis. Overall, the\ndataset includes 350k positive and 250k negative\nreviews (based on user rating).\nFollowing Gu et al. (2019), we use the Switch-\nboard (Godfrey and Holliman, 1997) dataset for\ndialogue-response generation. The former contains\n2.4k two-sided telephone conversations, manually\n", "Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael\nNoseworthy, Laurent Charlin, and Joelle Pineau.\n2016. How not to evaluate your dialogue system:\nAn empirical study of unsupervised evaluation met-\nrics for dialogue response generation. arXiv preprint\narXiv:1603.08023.\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly,\nIan Goodfellow, and Brendan Frey. 2015. Adversar-\nial autoencoders. arXiv preprint arXiv:1511.05644.\nMitchell Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993.\nBuilding a large annotated\ncorpus of english: The penn treebank.\nComputa-\ntional Linguistics.\nEmile Mathieu, Charline Le Lan, Chris J Maddison,\nRyota Tomioka, and Yee Whye Teh. 2019. Continu-\nous hierarchical representations with poincar\u00e9 varia-\ntional auto-encoders. In NeurIPS.\nLars Mescheder, Sebastian Nowozin, and Andreas\nGeiger. 2017. Adversarial variational bayes: Uni-\nfying variational autoencoders and generative adver-\nsarial networks. In ICML.\nYoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fu-\njita, and Masanori Koyama. 2019. A wrapped nor-\nmal distribution on hyperbolic space for gradient-\nbased learning. In ICML.\nMaximillian Nickel and Douwe Kiela. 2017. Poincar\u00e9\nembeddings for learning hierarchical representa-\ntions. In NeurIPS.\nIvan Ovinnikov. 2019.\nPoincar\u00e9 wasserstein autoen-\ncoder. arXiv preprint arXiv:1901.01427.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP.\nYuchen Pu, Weiyao Wang, Ricardo Henao, Liqun\nChen, Zhe Gan, Chunyuan Li, and Lawrence Carin.\n2017.\nAdversarial symmetric variational autoen-\ncoder. In NIPS.\nAli Razavi, A\u00e4ron van den Oord, Ben Poole, and Oriol\nVinyals. 2019.\nPreventing posterior collapse with\ndelta-vaes. In ICLR.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan\nWierstra. 2014. Stochastic backpropagation and ap-\nproximate inference in deep generative models. In\nICML.\nR Tyrrell Rockafellar et al. 1966.\nExtension of\nfenchel\u2019duality theorem for convex functions. Duke\nmathematical journal.\nRik Sarkar. 2011. Low distortion delaunay embedding\nof trees in hyperbolic plane. In International Sympo-\nsium on Graph Drawing, pages 355\u2013366. Springer.\nHuajie Shao, Shuochao Yao, Dachun Sun, Aston\nZhang, Shengzhong Liu, Dongxin Liu, Jun Wang,\nand Tarek Abdelzaher. 2020. Controlvae: Control-\nlable variational autoencoder. In ICML.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In NeurIPS.\nWenxian Shi, Hao Zhou, Ning Miao, Shenjian Zhao,\nand Lei Li. 2019. Fixing gaussian mixture vaes for\ninterpretable text generation. ICML.\nChenyang Tao, Liqun Chen, Shuyang Dai, Junya Chen,\nKe Bai, Dong Wang, Jianfeng Feng, Wenlian Lu,\nGeorgiy V Bobashev, and Lawrence Carin. 2019.\nOn fenchel mini-max learning. In NeurIPS.\nAlexandru Tifrea, Gary B\u00e9cigneul, and Octavian-\nEugen Ganea. 2018.\nPoincar\u00e9 glove: Hyperbolic\nword embeddings. ICLR.\nJakub M Tomczak and Max Welling. 2018. Vae with a\nvampprior. In AISTATS.\nAbraham Albert Ungar. 2008. A gyrovector space ap-\nproach to hyperbolic geometry. Synthesis Lectures\non Mathematics and Statistics, 1(1):1\u2013194.\nWenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang,\nGuoyin Wang, Dinghan Shen, Changyou Chen, and\nLawrence Carin. 2019a.\nTopic-guided variational\nautoencoders for text generation. NAACL.\nWenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang,\nLiqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian\nYang, Ricardo Henao, and Lawrence Carin. 2019b.\nImproving textual network learning with variational\nhomophilic embeddings. NeurIPS.\nJiacheng Xu and Greg Durrett. 2018.\nSpherical la-\ntent spaces for stable variational autoencoders. In\nEMNLP.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017.\nImproved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In ICML.\nJake Zhao, Yoon Kim, Kelly Zhang, Alexander M\nRush, and Yann LeCun. 2017a. Adversarially reg-\nularized autoencoders. ICML.\nShengjia Zhao, Jiaming Song, and Stefano Ermon.\n2018. Infovae: Information maximizing variational\nautoencoders. In AAAI.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017b. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In ACL.\nZachary M Ziegler and Alexander M Rush. 2019. La-\ntent normalizing \ufb02ows for discrete sequences.\nIn\nICML.\n", "transcribed and aligned. We split the data into train-\ning, validation and test sets following the protocol\ndescribed in Zhao et al. (2017b).\nEvaluation Metrics.\nTo benchmark language\nmodeling performance, we report the ELBO and\nPerplexity (PPL) of APo-VAE and baselines. In\norder to verify our proposed Apo-VAE is more re-\nsistant to posterior collapse, we also report the KL-\ndivergence DKL(q\u03c6(z|x)\u2225p(z)) and mutual infor-\nmation (MI) between z and x (He et al., 2019). The\nnumber of active units (AU) of the latent code is\nalso reported, where the activity of a latent dimen-\nsion z is measured as Az = CovxEz\u223cq\u03c6(z|x)[z],\nand de\ufb01ned as active if Az > 0.01.\nTo evaluate our model on unaligned style trans-\nfer, we consider the transfer accuracy from one sen-\ntiment to another, the BLEU scores between orig-\ninal and transferred sentences, the reconstruction\nperplexity of original sentences, and the reverse per-\nplexity (RPPL) based on a language model from\nthe transferred sentences.\nFor dialogue-response generation, we adopt the\nevaluation metrics used in previous studies (Zhao\net al., 2017b; Gu et al., 2019), including BLEU (Pa-\npineni et al., 2002), BOW (Liu et al., 2016), and\nintra/inter-dist values (Gu et al., 2019). The \ufb01rst\ntwo metrics are used to assess the relevance of the\ngenerated response, and the third is for diversity\nevaluation.\nModel Implementation.\nFor language model-\ning, we adopt the LSTM (Hochreiter and Schmid-\nhuber, 1997) for both the encoder and decoder,\nwith dimension of the latent code set to 32. Fol-\nlowing Mathieu et al. (2019), the hyper-parameter\nc is set to 0.7. For unaligned style transfer, we\nextend our model in the same fashion as Fang\net al. (2019). For dialogue-response generation, we\nmodify APo-VAE following the conditional VAE\nframework (Zhao et al., 2017b). Speci\ufb01cally, an\nextra input of context embedding s is supplied to\nthe model (i.e., p\u03b8(x, z|s), q\u03c6(z|x, s)). The prior\np(z|s) is a wrapped normal conditioned on context\nembedding, learned together with the posterior.\n5.2\nExperimental Results\nLanguage Modeling.\nTable 1 shows results on\nlanguage modeling. We compare APo-VAE with\nother VAE-based solutions, including \u03b2-VAE (Hig-\ngins et al., 2017), SA-VAE (Kim et al., 2018),\nlagging VAE (LAG-VAE) (He et al., 2019), vMF-\nVAE (Xu and Durrett, 2018), Poincar\u00e9 VAE (P-\nModel\n-ELBO\nPPL\nKL\nMI\nAU\nPTB\nVAE\n102.6\n108.26\n1.1\n0.8\n2\n\u03b2-VAE\n104.5\n117.92\n7.5\n3.1\n5\nSA-VAE\n102.6\n107.71\n1.2\n0.7\n2\nvMF-VAE\n95.8\n93.70\n2.9\n3.2\n21\nP-VAE\n91.4\n76.13\n4.5\n2.9\n23\niVAE\n87.2\n53.44\n12.5\n12.2\n32\nAPo-VAE\n87.2\n53.32\n8.4\n4.8\n32\nAPo-VAE+VP\n87.0\n53.02\n8.9\n4.5\n32\nYahoo\nVAE\n328.6\n61.21\n0.0\n0.0\n0\n\u03b2-VAE\n328.7\n61.29\n6.3\n2.8\n8\nSA-VAE\n327.2\n60.15\n5.2\n2.9\n10\nLAG-VAE\n326.7\n59.77\n5.7\n2.9\n15\nvMF-VAE\n318.5\n53.92\n6.3\n3.7\n23\nP-VAE\n313.4\n50.57\n7.2\n3.3\n27\niVAE\n309.1\n47.93\n11.4\n10.7\n32\nAPo-VAE\n286.2\n47.00\n6.9\n4.1\n32\nAPo-VAE+VP\n285.6\n46.61\n8.1\n4.9\n32\nYelp\nVAE\n357.9\n40.56\n0.0\n0.0\n0\n\u03b2-VAE\n358.2\n40.69\n4.2\n2.0\n4\nSA-VAE\n357.8\n40.51\n2.8\n1.7\n8\nLAG-VAE\n355.9\n39.73\n3.8\n2.4\n11\nvMF-VAE\n356.2\n51.03\n4.1\n3.9\n13\nP-VAE\n355.4\n50.64\n4.3\n4.8\n19\niVAE\n348.7\n36.88\n11.6\n11.0\n32\nAPo-VAE\n319.7\n34.10\n12.1\n7.5\n32\nAPo-VAE+VP\n316.4\n32.91\n12.7\n6.2\n32\nTable 1: Results on PTB, Yahoo, and Yelp datasets. A\nbetter language model achieves lower negative ELBO\nand PPL. Higher KL and MI indicate a better utilization\nof the latent space.\nVAE) (Mathieu et al., 2019) and iVAE3 (Fang et al.,\n2019). On all three datasets, the proposed model\nachieves lower negative ELBO and PPL than other\nmodels, demonstrating its strong ability to better\nmodel sequential text data. Meanwhile, the larger\nKL term and higher mutual information (between\nz and x) of APo-VAE model indicate its robust-\nness in handling posterior collapse. In addition,\nthe introduction of a data-driven prior (denoted\nas APo-VAE+VP) further boosts the performance,\nespecially on negative ELBO and PPL.\nVisualization.\nTo verify our hypothesis that the\nproposed model is capable of learning latent\ntree structure in text data, we visualize the two-\ndimensional projection of the learned latent code\nin Figure 3. For visualization, we randomly draw\n5k samples from PTB-test, and encode them to\nthe latent space using the APo-VAE encoder. We\ncolor-code each sentence based on its length (i.e.,\nblue for long sentences and red for short sentences).\nNote that only a small portion of data have a length\nlonger than 32 (< 10%), and human inspection\n3We report iVAEMI results in all our experiments.\n", "APo-VAE: Text Generation in Hyperbolic Space\nShuyang Dai1\u2217Zhe Gan2 Yu Cheng2 Chenyang Tao1 Lawrence Carin1 Jingjing Liu2\n1Duke University\n2Microsoft Corporation\n{shuyang.dai, chenyang.tao, lcarin}@duke.edu\n{zhe.gan, yu.cheng, jingjl}@microsoft.com\nAbstract\nNatural language often exhibits inherent hier-\narchical structure ingrained with complex syn-\ntax and semantics.\nHowever, most state-of-\nthe-art deep generative models learn embed-\ndings only in Euclidean vector space, with-\nout accounting for this structural property of\nlanguage.\nWe investigate text generation in\na hyperbolic latent space to learn continuous\nhierarchical representations. An Adversarial\nPoincar\u00e9 Variational Autoencoder (APo-VAE)\nis presented, where both the prior and varia-\ntional posterior of latent variables are de\ufb01ned\nover a Poincar\u00e9 ball via wrapped normal dis-\ntributions.\nBy adopting the primal-dual for-\nmulation of Kullback-Leibler divergence, an\nadversarial learning procedure is introduced\nto empower robust model training. Extensive\nexperiments in language modeling, unaligned\nstyle transfer, and dialog-response generation\ndemonstrate the effectiveness of the proposed\nAPo-VAE model over VAEs in Euclidean la-\ntent space, thanks to its superb capabilities in\ncapturing latent language hierarchies in hyper-\nbolic space.\n1\nIntroduction\nThe Variational Autoencoder (VAE) (Kingma and\nWelling, 2013; Rezende et al., 2014) is a generative\nmodel widely applied to language-generation tasks,\nwhich propagates latent codes drawn from a simple\nprior to manifest data samples through a decoder.\nThe generative model is augmented by an infer-\nence network, which feeds observed data samples\nthrough an encoder to yield a distribution on the\ncorresponding latent codes. Since natural language\noften manifests a latent hierarchical structure, it is\ndesirable for the latent code in a VAE to re\ufb02ect such\ninherent language structure, so that the generated\ntext can be more natural and expressive. An exam-\nple of language structure is illustrated in Figure 1,\nwhere sentences are organized into a tree structure.\n\u2217Work was done when the author interned at Microsoft.\nFigure 1: Illustration of the latent hierarchy in natural\nlanguage. Each tree node is a latent code of its corre-\nsponding sentence.\nThe root node corresponds to simple sentences (e.g.,\n\u201cYes\u201d), while nodes on outer leaves represent sen-\ntences with more complex syntactic structure and\nricher, more speci\ufb01c semantic meaning (e.g., \u201cThe\nfood in the restaurant is awesome\u201d)1.\nIn existing VAE-based generative models, such\nstructures are not explicitly considered. The la-\ntent code often employs a simple Gaussian prior,\nand the posterior is approximated as a Gaussian\nwith diagonal covariance matrix. Such embeddings\nassume Euclidean structure, which is inadequate\nin capturing geometric structure illustrated in Fig-\nure 1. While some variants have been proposed to\nenrich the prior distributions (Xu and Durrett, 2018;\nWang et al., 2019a,b; Shi et al., 2019), there is no\nevidence that structural information in language\ncan be recovered effectively by the model.\nHyperbolic geometry has recently emerged as an\neffective method for representation learning from\ndata with hierarchical structure (Mathieu et al.,\n1Another possible way to organize sentences is a hierarchy\nof topics, e.g., a parent node can be a sentence on \u201csports\u201d,\nwhile its children are sentences on \u201cbasketball\u201d and \u201cskiing\u201d.\narXiv:2005.00054v3  [cs.LG]  14 Jul 2021\n"], "summary": "The cited paper, \"APo-VAE: Text Generation in Hyperbolic Space,\" addresses the issue of posterior collapse in VAEs, a problem also tackled by my work. While my research proposes a discrete variational attention model with a categorical distribution to prevent posterior collapse, APo-VAE explores the use of hyperbolic latent space and wrapped normal distributions to achieve similar robustness. Both papers aim to improve language generation by enhancing the latent space, with APo-VAE focusing on capturing hierarchical structures in text through hyperbolic geometry, a different approach to my discrete attention mechanism.", "citation": "Luca Celotti, Simon Brodeur, Jean Rouat (2020). AriEL: volume coding for sentence generation. arXiv:2003.13600. https://arxiv.org/abs/2003.13600"}, {"paper_id": 45, "text": ["where KL(\u00b7||\u00b7) is the Kullback-Leibler (KL) diver-\ngence, p(z) = N(0, 1) is the prior distribution.\nThe \ufb01rst term ensures that VAE can distill com-\npact variable z in latent space for reconstruction.\nThe second term pushes posterior distribution to be\nclose to the prior distribution, securing the mutual\ninformation between original data and the latent\nspace (Dupont, 2018).\nConditional Text Generation with VAE. Condi-\ntional text generation has drawn much attention\nrecently. By controlling the properties of generated\ncontents, we can apply the generative models to\nmany real-world scenarios. We follow the problem\nsetting in (Hu et al., 2017). Given a set of k condi-\ntions C = {c1, c2, ..., ck}, an unlabeled corpus X,\nand conditional text samples Y = Y1 \u222aY2 \u222a...\u222aYk\nwhere each Yi is a set of text samples that carries\nthe condition ci. The goal of a VAE model is to\nlearn a decoder p\u03b8(\u02c6y|z, ci) that takes the latent vari-\nable z and the condition ci to calculate the distri-\nbution over the text samples Yi. Thus, when the\ncondition ci and a randomly sampled latent vari-\nable z \u223cp(z) speci\ufb01ed, the model could generate\nrealistic text samples matching the given condition.\n4\nPre-train and Plug-in Variational\nAuto-Encoder\nAs a basis for semi-supervised learning, a large\nunlabeled corpus should include diverse text which\ncovers a vast spectrum of conditions. Thus, text un-\nder each condition forms a conditional latent space,\nwhich could be mapped from a larger global latent\nspace. Based on this, we propose a PRETRAIN-\nVAE and a PLUGINVAE to derive the global and\nconditional latent space, respectively.\n4.1\nFramework\nPRETRAINVAE is composed of a pre-trained\nglobal encoder for text representation and a pre-\ntrained global decoder for text generation.\nPRETRAINVAE. The encoder and decoder of\nPRETRAINVAE are used to encode and generate\ntext, respectively. As discussed above, PRETRAIN-\nVAE is trained on a large amount of unlabeled text\nto derive the global latent space Zg for the latent\nvariable zg, where Zg \u2208Rdg and dg is the space\ndimension. Previous studies usually use a common\nVAE for text representation and generation. How-\never, as pointed out in (Bowman et al., 2016), VAE\nsuffers the notorious \u201cposterior collapse\u201d problem.\nTo address this, we utilize Wasserstein Autoen-\ncoder (WAE) (Tolstikhin et al., 2018) for PRE-\nTRAINVAE. Different from the original VAE, WAE\nencourages aggregated posterior distribution to be\nclose to the prior, which is effective in alleviat-\ning the reconstruction problem of VAE (Tolstikhin\net al., 2018). Speci\ufb01cally, we adopt WAE-GAN, a\nvariant of WAE, which incorporates the merits of\nadversarial learning. During training, the encoder\nencg(x) = qg(zg|x) encodes the text to the latent\nspace and the decoder decg(zg) = pg(x|zg) recon-\nstruct the text with the latent variable zg. Thus, the\nloss function of PRETRAINVAE is formulated as:\nLPRETRAINVAE(x) = \u2212Eqg(zg|x)[log pg(x|zg)]\n+ \u03bbD(Q(zg), p(zg))\n(2)\nwhere Q(zg) =\nR\nqg(zg|x)p(x) dx is the aggre-\ngated posterior distribution; p(zg) is the prior nor-\nmal distribution; D is the adversarial discriminator;\n\u03bb is the coef\ufb01cient hyper-parameter (\u03bb > 0).\nPLUGINVAE. For each condition, we use a\ncondition-speci\ufb01c PLUGINVAE to derive the con-\nditional space. That is, PLUGINVAE is proposed\nto learn the transformation between the condi-\ntional and global latent space for each condition.\nSpeci\ufb01cally, for each condition ci, we use a lim-\nited number of conditional samples yi and utilize\nthe global encoder encg to encode them into vyi.\nNote that normally, the encoded text samples un-\nder a single condition are not likely to densely\nclustered in the global text space Zg, since the\nlearning process of Zg is condition-independent\nand the unlabeled corpus contains diverse text sam-\nples. PLUGINVAE for condition ci consists of an\nencoder encci(vyi) = qci(zci|vyi) and a decoder\ndecci(zci) = pci(vyi|zci). The learned condition-\ndependent latent space is Zci \u2208Rdc, where dc is\nthe space dimension. Thus, PLUGINVAE is ca-\npable of mapping the samples in the global latent\nspace to and from a denser conditional latent space\n(i.e., dc < dg). During training, the loss function\nof PLUGINVAE for a single condition is de\ufb01ned\nas:\nLsingle(vyi) = \u2212Eq(zci|vyi)[log pci(vyi|zci)]\n+ | (KL(qci(zci|vyi)\u2225p(zci)) \u2212\u03b2 |\n(3)\nwhere p(zci) is the prior normal distribution of the\nconditional latent space; zci is the latent variable;\nvyi = encg(yi) is encoded text samples from Yi.\nTo enhance the diversity of generated text, we intro-\nduce an extra constant term \u03b2 to control the amount\n", "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAli Razavi, A\u00a8aron van den Oord, and Oriol Vinyals.\n2019. Generating diverse high-\ufb01delity images with\nVQ-VAE-2. In NeurIPS.\nAbigail See, Stephen Roller, Douwe Kiela, and Jason\nWeston. 2019.\nWhat makes a good conversation?\nhow controllable attributes affect human judgments.\nIn NAACL-HLT.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S.\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In NeurIPS.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.\nLearning structured output representation using\ndeep conditional generative models. In NeurIPS.\nIlya O. Tolstikhin, Olivier Bousquet, Sylvain Gelly,\nand Bernhard Sch\u00a8olkopf. 2018.\nWasserstein auto-\nencoders. In ICLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nKe Wang and Xiaojun Wan. 2018. Sentigan: Gener-\nating sentimental texts via mixture adversarial net-\nworks. In IJCAI.\nLiwei Wang, Alexander G. Schwing, and Svetlana\nLazebnik. 2017.\nDiverse and accurate image de-\nscription using a variational auto-encoder with an ad-\nditive gaussian encoding space. In NeurIPS.\nJiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh\nAcharya, and Luc Van Gool. 2018. Wasserstein di-\nvergence for gans. In ECCV.\nXiaopeng Yang, Xiaowen Lin, Shunda Suo, and Ming\nLi. 2018. Generating thematic chinese poetry using\nconditional variational autoencoders with hybrid de-\ncoders. In IJCAI.\nXiaoyuan Yi, Maosong Sun, Ruoyu Li, and Zonghan\nYang. 2018. Chinese poetry generation with a work-\ning memory model. In IJCAI.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI.\nTiancheng Zhao, Ran Zhao, and Maxine Esk\u00b4enazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In ACL.\nWangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and\nMing Zhou. 2020.\nSelf-adversarial learning with\ncomparative discrimination for text generation. In\nICLR.\n", "Different from the existing end-to-end neural\nmodels (Mirza and Osindero, 2014; Sohn et al.,\n2015; Kingma et al., 2014), PPVAE focuses on the\nlearning of pure transformation between the con-\ntinuous latent spaces, instead of the tricky discrete\ntext generation. Once trained, PRETRAINVAE is\n\ufb01xed for text representation and generation under\nall conditions. Our proposed framework decouples\nthe conditional space learning from the text genera-\ntion, endowing PPVAE with more \ufb02exibility when\nhandling emerging conditions. Also, training only\na small conditional network for latent space trans-\nformation is much more ef\ufb01cient than co-training\nwith the text generation. Additionally, we can eas-\nily increase the capability of generation using a\nlarger corpus or deeper neural networks for text\nencoding and decoding. Our main contributions\ncan be summarized as follows: (1) We propose\na novel framework, PPVAE, for conditional text\ngeneration, which allows a separate training for\na new condition without retraining the whole net-\nwork. (2) We conduct extensive experiments and\nanalysis to verify the effectiveness of our proposed\nPPVAE. Our framework achieves state-of-the-art\nperformance on conditionality in both automatic\nand human evaluations.\n2\nRelated work\nBoosted by the recent success of deep learning tech-\nnology, Natural Language Generation (NLG) has\nrecently become popular in the NLP community.\nMany great works have attempted to solve various\nsubtasks like dialogue generation (Li et al., 2016),\npoetry generation (Yi et al., 2018) and story gen-\neration (Fan et al., 2018) and new techniques keep\nemerging (Bowman et al., 2016; Yu et al., 2017;\nZhou et al., 2020). However, due to the black-\nbox nature of neural networks, the recent proposed\ngeneric models suffer the problem of lacking inter-\npretability and controllability.\nTo handle this problem and support generating\nplausible text with a speci\ufb01ed condition, condi-\ntional text generation (Kikuchi et al., 2016; Ficler\nand Goldberg, 2017; Hu et al., 2017) has recently at-\ntracted extensive attention. Current research in this\ndirection mainly falls into two fashions: the super-\nvised methods and semi-supervised methods. For\nsupervised methods, Mirza and Osindero (2014);\nSohn et al. (2015) \ufb01rst converted the condition in-\nformation to one-hot vectors, then integrated them\ninto a generator and a discriminator. To enhance\nthe correlation between structured conditional code\nand generated samples, Chen et al. (2016) adopted\nan extra adversarial classi\ufb01er to infer the struc-\ntured code from generated samples. Wang and\nWan (2018) used multiple generators for multiple\nconditions and a multi-class classi\ufb01er to provide\ntraining signals for the learning of generators.\nHowever, given only a limited number of condi-\ntional samples, semi-supervised methods are com-\npulsory. To utilize the implicit conditional distribu-\ntion behind the unlabeled text, Kingma et al. (2014)\nintroduced a classi\ufb01er into the VAE architecture.\nHu et al. (2017) further involved two additional\nindependent regularization terms in enhancing the\ndisentanglement between structured code and un-\nstructured code. Very recently, Keskar et al. (2019)\nused human-de\ufb01ned \u201ccontrol code\u201d to pre-trained\nLanguage Model in an unsupervised manner.\nOur work falls in the category of semi-\nsupervised learning yet differs from the existing\nworks in the following ways: (1) Our model decou-\nples the text generation module from the condition\nrepresentation module which two are tightly fused\nas a single one in previous studies, enabling pos-\nsible exploitation for pre-trained Language Mod-\nels (e.g., GPT-2 (Radford et al., 2019)). (2) Our\nmodel allows single-condition generation, which\ncould inspire new applications like polite speech\ngenerator (Niu and Bansal, 2018) and data augmen-\ntation (Guo et al., 2018). (3) Our model can handle\nemerging conditions while achieving state-of-the-\nart performance with fewer parameters and less\ntraining time.\n3\nPreliminaries\nVariational Auto-Encoder (VAE). VAE (Kingma\nand Ba, 2015) is widely used in continuous genera-\ntion (e.g., image generation). Bowman et al. (2016)\nintroduced VAE to NLG to solve the \u201cone-to-many\u201d\ngeneration problem (i.e., generating multiple feasi-\nble samples for the same input). Given a latent vari-\nable z randomly sampled from a prior distribution,\nVAE comprises an encoder enc(x) = q\u03c6(z|x) and\na decoder dec(z) = p\u03b8(x|z). The encoder aims to\nencode input data x into latent space Z \u2208Rd. The\ndecoder is used to reconstruct the original input x,\ngiven the corresponding z. Thus, the loss function\nof VAE is formulated as:\nLVAE(x) = \u2212Eq\u03c6(z|x)[log p\u03b8(x|z)]\n+ KL(q\u03c6(z|x)\u2225p(z))\n(1)\n", "Pre-train and Plug-in: Flexible Conditional Text Generation with\nVariational Auto-Encoders\nYu Duan1\u2217, Canwen Xu2\u2217, Jiaxin Pei3\u2217, Jialong Han4\u2020, Chenliang Li2\u2021\n1 Alibaba Group, China 2 Wuhan University, China\n3 University of Michigan, United States 4 Amazon, United States\n1 derrick.dy@alibaba-inc.com, 2 {xucanwen,cllee}@whu.edu.cn\n3 pedropei@umich.edu, 4 jialonghan@gmail.com\nAbstract\nConditional Text Generation has drawn much\nattention as a topic of Natural Language Gener-\nation (NLG) which provides the possibility for\nhumans to control the properties of generated\ncontents. Current conditional generation mod-\nels cannot handle emerging conditions due to\ntheir joint end-to-end learning fashion. When\na new condition added, these techniques re-\nquire full retraining. In this paper, we present\na new framework named Pre-train and Plug-in\nVariational Auto-Encoder (PPVAE) towards\n\ufb02exible conditional text generation. PPVAE\ndecouples the text generation module from\nthe condition representation module to allow\n\u201cone-to-many\u201d conditional generation. When\na fresh condition emerges, only a lightweight\nnetwork needs to be trained and works as a\nplug-in for PPVAE, which is ef\ufb01cient and\ndesirable for real-world applications. Exten-\nsive experiments demonstrate the superiority\nof PPVAE against the existing alternatives\nwith better conditionality and diversity but less\ntraining effort.1\n1\nIntroduction\nCurrently, neural generation techniques have pow-\nered many inspiring applications, e.g., poem gener-\nation (Yang et al., 2018), neural machine translation\n(NMT) (Bahdanau et al., 2015) and chatbot (Zhao\net al., 2017). Conditional (also known as control-\nlable) text generation is an important task of text\ngeneration, aiming to generate realistic text that\ncarries a speci\ufb01c attribute (e.g., positive or negative\nsentiment). A common solution is to encode the\ncondition into a vector representation and then in-\ntegrate it with the text generation process (Kingma\n\u2217The \ufb01rst three authors contribute equally to this paper.\n\u2020 Work done when Jialong Han was with Tencent AI Lab.\n\u2021 Chenliang Li is the corresponding author.\n1The code is available at https://github.com/\nWHUIR/PPVAE.\net al., 2014; Hu et al., 2017; Mirza and Osindero,\n2014). These existing neural models have achieved\nencouraging results. However, when a new con-\ndition is added (e.g., a new topic for categorical\ngeneration), they require a full retraining or \ufb01ne-\ntuning. This process is both time-consuming and\ncomputationally inef\ufb01cient (Houlsby et al., 2019).\nBoth \ufb01ne-tuning and retraining are not desirable\nin real-world applications since the delivery (e.g.,\ntransmitting updated weights through the Internet)\nand client-side re-deployment (e.g., distribute up-\ndated weights to users) of large-scale weights are\noften dif\ufb01cult.\nInspired by the recent success of Variational\nAuto-Encoder (VAE) (Kingma and Welling, 2014)\nbased post-hoc conditional image generation strat-\negy (Engel et al., 2018), we provide a new perspec-\ntive for \ufb02exible conditional text generation. We\npropose Pre-train and Plug-in Variational Auto-\nEncoder (PPVAE), which decouples the text gen-\neration module from the condition representation\nmodule. PPVAE is a hierarchical framework com-\nposed of two VAEs: (1) PRETRAINVAE, which\nderives the global latent space of text with its en-\ncoder (pre-trained global encoder) and learns to\ngenerate text based on an easily-accessible large un-\nlabeled dataset with its decoder (pre-trained global\ndecoder); (2) PLUGINVAE, which is a lightweight\nneural network that learns to transform vectors\nfrom the conditional latent space to the global la-\ntent space, and vice versa. This mapping function\ncan be easily learned with only a few conditional\ntraining samples. In this sense, once we transform\na latent variable (also known as latent code) ran-\ndomly sampled from the conditional space distri-\nbution to the global space, the pre-trained global\ndecoder is directly adopted for generation. In other\nwords, whenever a new condition emerges, we only\nneed to train a PLUGINVAE and directly plug it\ninto the framework.\narXiv:1911.03882v4  [cs.CL]  8 May 2020\n"], "summary": "The cited paper, \"Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders,\" addresses the posterior collapse problem in VAEs for text generation, a challenge also tackled in your work. While your research proposes a discrete variational attention model to mitigate posterior collapse and enhance latent space, the cited paper uses a \"Pre-train and Plug-in\" framework with Wasserstein Autoencoders (WAE) to achieve similar goals. Specifically, they utilize WAE-GAN for their PRETRAINVAE to alleviate reconstruction issues and posterior collapse, offering an alternative approach to improving VAE performance in text generation. Their work also focuses on flexible conditional text generation by decoupling the generation and condition representation modules, which is a different emphasis from your discrete attention mechanism.", "citation": "Teng Long, Yanshuai Cao, Jackie Chi Kit Cheung (2019). On Posterior Collapse and Encoder Feature Dispersion in Sequence VAEs. arXiv:1911.03976. https://arxiv.org/abs/1911.03976"}, {"paper_id": 27, "text": ["Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation\nWenxian Shi 1 Hao Zhou 1 Ning Miao 1 Lei Li 1\nAbstract\nDeep generative models are commonly used for\ngenerating images and text. Interpretability of\nthese models is one important pursuit, other than\nthe generation quality. Variational auto-encoder\n(VAE) with Gaussian distribution as prior has\nbeen successfully applied in text generation, but\nit is hard to interpret the meaning of the latent\nvariable. To enhance the controllability and in-\nterpretability, one can replace the Gaussian prior\nwith a mixture of Gaussian distributions (GM-\nVAE), whose mixture components could be re-\nlated to hidden semantic aspects of data. In this\npaper, we generalize the practice and introduce\nDEM-VAE, a class of models for text genera-\ntion using VAEs with a mixture distribution of\nexponential family. Unfortunately, a standard\nvariational training algorithm fails due to the\nmode-collapse problem. We theoretically iden-\ntify the root cause of the problem and propose\nan effective algorithm to train DEM-VAE. Our\nmethod penalizes the training with an extra dis-\npersion term to induce a well-structured latent\nspace. Experimental results show that our ap-\nproach does obtain a meaningful space, and it\noutperforms strong baselines in text generation\nbenchmarks. The code is available at https:\n//github.com/wenxianxian/demvae.\n1. Introduction\nText generation is one of the most challenging problems\nin Natural Language Processing (NLP), which is widely\napplied to tasks such as machine translation (Brown et al.,\n1993) and dialog system (Young et al., 2013). Though\ngeneration quality is the main concern of most work, the\ninterpretability for generation process is also of great im-\n1ByteDance\nAI\nlab.\nCorrespondence\nto:\nWenx-\nian\nShi\n<shiwenxian@bytedance.com>,\nHao\nZhou\n<zhouhao.nlp@bytedance.com>,\nNing\nMiao\n<miaon-\ning@bytedance.com>, Lei Li <lileilab@bytedance.com>.\nProceedings of the 37 th International Conference on Machine\nLearning, Online, PMLR 119, 2020. Copyright 2020 by the au-\nthor(s).\nportance. Interpretable generation models could explore\nthe latent data structures, such as topics (Wang et al., 2019)\nand dialog actions (Zhao et al., 2018b), and uses them to\nguide text classi\ufb01cation and further generation. Among\ndeep generative models, variational auto-encoder (Kingma\n& Welling, 2013; Rezende et al., 2014, VAE) is especially\nsuitable for interpretable text generation because it maps\nsentences to a regularized latent variable z, which can be\nused to derive interpretable structures (Bowman et al., 2016;\nMiao et al., 2016; Semeniuta et al., 2017; Xu & Durrett,\n2018).\nHowever, the continuous latent variable z of vanilla VAE\nmakes it dif\ufb01cult to interpret discrete attributes, such as\ntopics and dialog actions. Recently, Zhao et al. (2018b)\npropose to replace the continuous latent variable with a\ndiscrete one for better interpretation in generating dialog,\nwhere the discrete latent variable represents the dialog ac-\ntions in their system, producing promising results even in\nan unsupervised setting. Unfortunately, the expressiveness\nof VAE with only a discrete latent variable c is very limited.\nIt only contains log(#c) bits of information, where #c is\nthe number of all possible values of c. So it is impossible\nfor discrete VAE to express the complicated sentence space.\nTo solve the above concern, Gaussian mixture VAE (GM-\nVAE) offers a natural choice, which enjoys the bene\ufb01ts\nof both discrete and continuous latent space. Each com-\nponent c represents a discrete attribute, while continuous\nlatent variable z in each component represents different\nsentences with the same attribute. However, GM-VAE is\nprone to mode-collapse, which makes it dif\ufb01cult to train. In\nother words, different components tend to have very close\nmeans and variances after training, which makes GM-VAE\ndegenerate to vanilla VAE with only one Gaussian com-\nponent (Fig. 1). As a result, GM-VAE fails to capture the\nmulti-modal data structure, and cannot effectively utilize\nthe discrete latent variables. For example, as illustrated in\nFig. 1a, utterances ask about the weather and requesting\nappointments are mapped into the same mode. In this paper,\nwe theoretically demonstrate that mode-collapse does not\nonly occur in GM-VAE, which is a general problem for\nVAEs with exponential family mixture priors (EM-VAE).\nWe \ufb01nd that the problem is intrinsically caused by a \u201cdis-\npersion\u201d term in the evidence lower bound (ELBO), which\nmakes it extremely easy to fall into the solution of mode\narXiv:1906.06719v4  [cs.LG]  21 Aug 2020\n", "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation\nMiao, Y., Yu, L., and Blunsom, P. Neural variational in-\nference for text processing. International Conference on\nMachine Learning, pp. 1727\u20131736, 2016.\nMikolov, T., Kara\ufb01at, M., Burget, L., Cernock, J., and Khu-\ndanpur, S. Recurrent neural network based language\nmodel. Interspeech, pp. 1045\u20131048, 2010.\nMitchell, J. and Lapata, M. Vector-based models of semantic\ncomposition. proceedings of ACL-08: HLT, pp. 236\u2013244,\n2008.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. Bleu: a\nmethod for automatic evaluation of machine translation.\npp. 311\u2013318, 2002.\nPennington, J., Socher, R., and Manning, C. Glove: Global\nvectors for word representation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pp. 1532\u20131543, 2014.\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic\nbackpropagation and approximate inference in deep gen-\nerative models. arXiv preprint arXiv:1401.4082, 2014.\nRus, V. and Lintean, M. A comparison of greedy and opti-\nmal assessment of natural language student input using\nword-to-word similarity metrics. In Proceedings of the\nSeventh Workshop on Building Educational Applications\nUsing NLP, pp. 157\u2013162. Association for Computational\nLinguistics, 2012.\nSemeniuta, S., Severyn, A., and Barth, E. A hybrid convo-\nlutional variational autoencoder for text generation. In\nProceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 627\u2013637, 2017.\nSerban, I. V., Sordoni, A., Bengio, Y., Courville, A., and\nPineau, J. Building end-to-end dialogue systems using\ngenerative hierarchical neural network models. In Thirti-\neth AAAI Conference on Arti\ufb01cial Intelligence, 2016.\nSerban, I. V., Sordoni, A., Lowe, R., Charlin, L., Pineau,\nJ., Courville, A., and Bengio, Y. A hierarchical latent\nvariable encoder-decoder model for generating dialogues.\nIn Thirty-First AAAI Conference on Arti\ufb01cial Intelligence,\n2017.\nSordoni, A., Bengio, Y., Vahabi, H., Lioma, C., Grue Simon-\nsen, J., and Nie, J.-Y. A hierarchical recurrent encoder-\ndecoder for generative context-aware query suggestion.\nIn Proceedings of the 24th ACM International on Con-\nference on Information and Knowledge Management, pp.\n553\u2013562. ACM, 2015.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Man-\nzagol, P. A. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local de-\nnoising criterion. Journal of Machine Learning Research,\n11(12):3371\u20133408, 2010.\nWainwright, M. J., Jordan, M. I., et al. Graphical models,\nexponential families, and variational inference. Founda-\ntions and Trends R\u20ddin Machine Learning, 1(1\u20132):1\u2013305,\n2008.\nWang, W., Gan, Z., Xu, H., Zhang, R., Wang, G.,\nShen, D., Chen, C., and Carin, L. Topic-guided vari-\national autoencoders for text generation. arXiv preprint\narXiv:1903.07137, 2019.\nWen, T., Miao, Y., Blunsom, P., and Young, S. J. Latent\nintention dialogue models. International Conference on\nMachine Learning, pp. 3732\u20133741, 2017.\nXing, E. P., Jordan, M. I., and Russell, S. A generalized\nmean \ufb01eld algorithm for variational inference in exponen-\ntial families. In Proceedings of the Nineteenth conference\non Uncertainty in Arti\ufb01cial Intelligence, pp. 583\u2013591.\nMorgan Kaufmann Publishers Inc., 2003.\nXu, J. and Durrett, G. Spherical latent spaces for stable\nvariational autoencoders. Empirical Methods in Natural\nLanguage Processing, 2018.\nYoung, S., Ga\u02c7si\u00b4c, M., Thomson, B., and Williams, J. D.\nPomdp-based statistical spoken dialog systems: A review.\nProceedings of the IEEE, 101(5):1160\u20131179, 2013.\nZhang, B., Xiong, D., Duan, H., Zhang, M., et al. Variational\nneural machine translation. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language\nProcessing, pp. 521\u2013530, 2016.\nZhao, J. J., Kim, Y., Zhang, K., Rush, A. M., and Lecun,\nY. Adversarially regularized autoencoders. International\nConference on Machine Learning, pp. 5897\u20135906, 2018a.\nZhao, S., Song, J., and Ermon, S. Infovae: Information\nmaximizing variational autoencoders. arXiv: Learning,\n2017a.\nZhao, T., Zhao, R., and Eskenazi, M. Learning discourse-\nlevel diversity for neural dialog models using conditional\nvariational autoencoders.\nIn Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1, pp. 654\u2013\n664, 2017b.\nZhao, T., Lee, K., and Eskenazi, M. Unsupervised discrete\nsentence representation learning for interpretable neural\ndialog generation. In ACL, 2018b.\nZhou, C. and Neubig, G. Multi-space variational encoder-\ndecoders for semi-supervised labeled sequence transduc-\ntion. Meeting of the Association for Computational Lin-\nguistics, 1:310\u2013320, 2017.\n", "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation\nWill it be humid in New York today?\nRemind me about my meeting.\n(a) GM-VAE\nRemind me about the football game.\nWill it be overcast tomorrow?\n(b) DGM-VAE\nFigure 1. Latent space learned by GM-VAE and our proposed\nDEM-VAE with a speci\ufb01c Gaussian mixture prior (named DGM-\nVAE) for dialog intention recognition. Notice that our proposed\nDGM-VAE avoids mode-collapse of GM-VAE. As a result, re-\nquests with different intentions are clearly separated into two clus-\nters.\ncollapse.\nTo address the mode-collapse problem, we propose Dis-\npersed EM-VAE (DEM-VAE), which introduces an extra\ndispersion term in the training objective (Fig. 1b) for over-\ncoming the contraction tendency introduced by ELBO. Ex-\nperimental results show that our proposed DEM-VAE alle-\nviates the mode-collapse problem effectively. Furthermore,\nDEM-VAE outperforms strong baselines in interpretable\ntext generation benchmarks. Although DEM-VAE mod-\nerately decreases the sentence likehoood because of the\ninclusion of external training term, it outputs sentences with\nsigni\ufb01cantly better quality in aspects of other evaluation\nmetrics such as rPPL and BLEU scores.\n2. Related Work\nVAEs for Language Generation.\nVariational auto-\nencoders are proposed by Kingma & Welling (2013, VAEs)\nand (Rezende et al., 2014), and applied by Bowman et al.\n(2016) for natural language generation. VAEs are extended\nby many following works in various speci\ufb01c language gen-\neration tasks, such as dialog generation (Serban et al., 2017;\nWen et al., 2017; Zhao et al., 2017b; 2018b), summariza-\ntion (Li et al., 2017a) and other natural language generation\ntasks (Miao et al., 2016; Zhang et al., 2016; Semeniuta et al.,\n2017; Gupta et al., 2018; Xu & Durrett, 2018; ?; ?).\nAdditionally, Wen et al. (2017) and Zhao et al. (2018b) pro-\npose to replace the continuous latent variable with a discrete\none for interpretable sentence generation. Kingma et al.\n(2014) propose the semi-VAE for semi-supervised learning.\nThis model is then adopted by Hu et al. (2017); Zhou &\nNeubig (2017) for style-transfer and labeled sequence trans-\nduction, respectively. Different from GM-VAE, continuous\nand discrete latent variables in semi-VAE are independent.\nGaussian Mixture VAEs. Using Gaussian mixture models\nas priors in VAEs is not new. Gaussian mixture variational\nauto-encoder has been used in the unsupervised cluster-\ning (Dilokthanakul et al., 2016; Jiang et al., 2017), obtaining\npromising results. Wang et al. (2019) used GMM as priors\nfor topic-guided text generation. In this work, we apply\nGM-VAE for interpretable text generation and propose the\nDGM-VAE to address the mode-collapse problem according\nto our theoretical analysis.\nKL Collapse vs. Mode Collapse. The vanilla VAE models\nusually suffer from the KL collapse problem (Bowman et al.,\n2016) in language generation, in which the KL regulariza-\ntion term will quickly collapse to 0. A line of following\nwork (Bowman et al., 2016; Zhao et al., 2017b; 2018b; Hig-\ngins et al., 2017; He et al., 2018; Li et al., 2019) is proposed\nto avoid the KL collapse problem.\nThese methods are different from ours in two-folds. First,\nthe problem to be solved is different. Our method aims to\naddress the mode-collapse in the mixture of exponential\nfamily VAEs, while these methods try to \ufb01x the posterior\ncollapse in general VAEs. Mode collapse and posterior\ncollapse are not the same. Posterior collapse means that\nthe posterior distributions of all latent variables collapse\nto their priors, while mode-collapse means the multiple\nmodes of prior collapses to one mode. Mode-collapse might\nalso occur when the posterior does not collapse. Second,\nalthough our method and previous methods of solving KL\ncollapse have similar solutions that \ufb01nd the problem-causing\nterm in the objective and weakens it, a heuristic dispersion\nterm instead of the whole KL term is introduced according\nto our theoretical analysis. The dispersion term is explicitly\nrelated to the difference between prior components and is\nspeci\ufb01cally effective for the mode collapse problem.\n3. Approach\nWe \ufb01rst describe the VAEs with mixture of exponential\nfamily priors for text generation, and investigate the mode-\ncollapse issue in them. Based on the results of the investi-\ngation, we propose dispersed exponential family mixture\nVAEs to \ufb01x this issue, among which dispersed Gaussian\nmixture VAE (DGM-VAE) is particularly exempli\ufb01ed.\n3.1. Mixture of Exponential Family VAEs\nMixture of Exponential Family VAEs are variational auto-\nencoders that adopts the mixture of exponential family dis-\ntributions as its prior. GM-VAE is the most popular expo-\nnential family mixture VAE, whose prior is a mixture of\nGaussian (Bishop, 2006). GM-VAE employs a discrete la-\ntent variable c to represent the mixture components, and a\ncontinuous latent variable z dependent on c. In this model,\n", "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation\n(a) GM-VAE #2000\n(b) GM-VAE #10000\n(c) DGM-VAE #2000\n(d) DGM-VAE #10000\n(e) GM-VAE + Lmi #2000\n(f) GM-VAE + Lmi #10000\n(g) DGM-VAE + Lmi #2000\n(h) DGM-VAE + Lmi #10000\nFigure 4. Visualization of the mode collapse problem in DD dataset for GM-VAE, GM-VAE adding Lmi term, DGM-VAE and DGM-VAE\nadding Lmi. Gaussian mixture priors are represented by grey points (mean) and circles (variance). The mean of posteriors are marked as\ncolored points (colors are associated with discrete latent variables).\ning is applied in the whole KL term. All hyper-parameters\nincluding \u03b2 are chosen according to the reverse perplexity\n(language generation task) or BLEU scores (dialog genera-\ntion task) in the validation set. Details of hyper-parameters\nare included in the supplementary.\n4.2. Effects of DGM-VAE on Mode-Collapse\nWe illustrate the effectiveness of DGM-VAE to alleviate the\nmode-collapse problem. We train GM-VAE and DGM-VAE\nin utterances on the DD dataset, and randomly sample 300\npoints from test data at 2,000 and 10,000 training steps,\nrespectively. The dimension of latent space is set to 2 for\nvisualization. As in Fig. 4, the mean and variance of each\nGaussian component are indicated by grey points and circles,\nrespectively. The means of posteriors are marked as colored\npoints (points with different discrete latent variables are\nassociated with different colors).\nIt can be seen that, after 10,000 training steps, the vanilla\nGM-VAE degenerates into uni-Gaussian VAE, with the\nsame mean values of all Gaussian components (Fig. 4a and\n4b). DGM-VAE gives promising results as shown in Fig.\n4d and 4h, in which different components of the GMM are\ndispersed and cluster data points into multiple modes well.\nAs shown in Fig. 4f, adding mutual information to GM-VAE\nindeed helps to alleviate the mode-collapse. However, the\nposterior points are quite concentrated to the priors, which\nmakes the latent space degenerates into a discrete one so\nthat the model cannot enjoy the high capacity and diversity\nof continuous variables.\n4.3. Language Generation Performance\nWe evaluate the performance of language generation on PTB\nin Tab. 1, comparing DGM-VAE (\u03b2 = 0.8) with baselines\ndescribed in Sec. 4.1. The test set of PTB is also included\nfor comparison of text \ufb02uency.\nWe include four metrics to evaluate the generation perfor-\nmances: reverse perplexity (rPPL), BLEU (Papineni et al.,\n2002), word-level KL divergence (wKL) and negative log-\nlikelihood (NLL). Reverse perplexity is the perplexity of\nan LSTM language model (Merity et al., 2017) trained on\nthe synthetic data sampled from priors of VAE variants, and\nevaluated on the test set (Zhao et al., 2018a). Lower rPPL\nindicates that the sampled sentences are more diverse and\n\ufb02uent. The BLEU score between input and reconstructed\nsentences re\ufb02ects the ability of reconstruction. Word-level\nKL divergence between word frequencies calculated in gen-\nerated data and training data shows the quality of generation.\nNegative log-likelihood2 re\ufb02ects the generation ability of\nmodels. These metrics are evaluated on the test set of PTB,\nexcept rPPL and wKL, which are calculated on sentences\ngenerated by sampling from these models\u2019 prior distribu-\ntion3 (sampling a random vector for AE).\nBesides, the values of the regularization terms are also\nincluded in order to give some indications of the mode-\ncollapse and KL collapse. We list the KL divergence of\ncontinuous latent variables (KL(z)) and discrete latent vari-\nables (KL(c)). The weighted variance of parameters (Var4)\nand mutual information (MI5) terms are shown as well.\nWe \ufb01rst present the ablation study to show the contribution\nof dispersion term and mutual information term. Results\nare shown in Tab. 1. First of all, because extra bias is\nintroduced in training objectives (higher mutual information\nor higher dispersion), the optimization object is no longer\nthe lower bound of log-likelihood. As a result, the NLL\nwill be worse with extra mutual information or dispersion\n2Evaluate by importance sampling (Burda et al., 2015). The\nnumber of sampling is 500.\n3Sample size here is 40,000, the same as PTB training set.\n4Var is calculated according to Eq. 2 and taking the average\nover all samples. Variance of multiple parameters are summed.\n5MI is averaged over multivariate.\n"], "summary": "The cited paper, \"Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation,\" addresses the mode-collapse problem in VAEs with mixture priors, which is distinct from the posterior collapse problem your work tackles. While your research focuses on discrete variational attention models to prevent posterior collapse and enhance latent space for language generation, this paper introduces DEM-VAE to achieve a well-structured and interpretable latent space by preventing mode-collapse in mixture VAEs. Specifically, it explores the use of discrete latent variables within a mixture model to improve interpretability, a concept that aligns with your interest in discrete representations for language. The paper's discussion of discrete latent variables and their limitations in expressiveness, as well as the benefits of combining discrete and continuous latent spaces, provides a relevant comparison point for your discrete variational attention model.", "citation": "Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, Andr\u00e9 Freitas (2023). LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces. arXiv:2312.13208. https://arxiv.org/abs/2312.13208"}, {"paper_id": 22, "text": ["Deep Residual Output Layers for Neural Language Generation\nwe propose a deep residual nonlinear output mapping from\nword embeddings to the joint input-output space, which bet-\nter captures the output structure while it avoids over\ufb01tting\nwith two different dropout strategies between layers, and\npreserves useful information with residual connections to\nthe word embeddings and, optionally, to the outputs of pre-\nvious layers.1 For the rest of the model, we keep the same\ninput encoder architecture and still use the dot product and\nsoftmax function for output label prediction.\nWe demonstrate on language modeling and machine transla-\ntion that we can match or improve state-of-the-art recurrent\nand self-attention architectures by simply increasing the\npower of the output mapping, while using a single softmax\noperation and without changing the dimensionality or rank\nof the classi\ufb01er. The results suggest that the classi\ufb01er does\nnot necessarily need to be high rank to better model lan-\nguage if it better captures the output space structure. Further\nanalysis reveals the signi\ufb01cance of different model compo-\nnents and improvements on predicting low frequency words.\n2. Background: Neural Language Generation\nThe output layer of neural models for language genera-\ntion tasks such as language modeling (Bengio et al., 2003;\nMikolov & Zweig, 2012; Merity et al., 2017), machine trans-\nlation (Bahdanau et al., 2015; Luong et al., 2015; Johnson\net al., 2017) and summarization (Rush et al., 2015; Paulus\net al., 2018), typically consists of a linear unit with a weight\nmatrix W \u2208IRdh\u00d7|V| and a bias vector b \u2208IR|V| followed\nby a softmax activation function, where V is the vocabu-\nlary. Thus, at a given time t, the output probability distri-\nbution for the current output yt conditioned on the inputs\ni.e. the previous outputs, yt\u22121\n1\n= (y1, y2, \u00b7 \u00b7 \u00b7 , yt\u22121) with\nyi \u2208{0, 1}|V| : P|V|\nj\nyij = 1 \u2200i \u2208N, is de\ufb01ned as:\np(yt|yt\u22121\n1\n) \u221dexp(WT ht + b),\n(1)\nwhere ht is the input encoder\u2019s hidden representation at time\nt with dh dimensions. The parameterisation in Eq. 1 makes\nit dif\ufb01cult to learn the structure of the output space or to\ntransfer this information from one label to another because\nthe parameters for output label i, WT\ni , are independent from\nthe parameters for any other output label j, WT\nj .\n2.1. Weight Tying\nLearning the structure of the output space can be helped by\nlearning it jointly with the structure of input word embed-\ndings, but this still does not support the transfer of learned\ninformation across output labels. In particular, since the\noutput labels are words and thus the output parameters WT\nhave one row per word, it is common to tie these parameters\n1Our code and settings are available at http://github.\ncom/idiap/drill.\nwith those of the input word embeddings E \u2208IR|V|\u00d7d, by\nsetting W = ET (Inan et al., 2016; Press & Wolf, 2017).\nMaking this substitution in Eq 1, we obtain:\np(yt|yt\u22121\n1\n) \u221dexp(Eht + b)\n(2)\nAlthough there is no explicit transfer across outputs, this\nparameterisation can implicitly learn the output structure,\nas can be seen if we assume an implicit factorization of the\ninput embeddings, E \u2248ElWl as in (Mikolov et al., 2013).\n2.2. Bilinear Mapping\nThe above bilinear form, excluding the bias, is similar to the\nform of joint input-output space learning models (Yazdani\n& Henderson, 2015; Nam et al., 2016) which have been\nproposed in the context of zero-shot text classi\ufb01cation. This\nmotivates the learning of explicit relationships across out-\nputs and inputs through parameter sharing via Wl as above.\nBy substituting this factorization in Eq 2, we obtain:\np(yt|yt\u22121\n1\n) \u221dexp(ElWlht + b)\n(3)\nwhere Wl \u2208IRd\u00d7dh is the bilinear mapping and E, ht are\nthe output embeddings and the encoded input respectively,\nas above. This parametrization has been previously also\nproposed by Gulordava et al. (2018) for language modeling\nalbeit with a different motivation, namely to decouple the\nhidden state from the word embedding prediction.\n2.3. Dual Nonlinear Mapping\nAnother existing output layer parameterisation which ex-\nplicitly learns the structure of the output is from (Pappas\net al., 2018). Speci\ufb01cally, two nonlinear functions, gout(\u00b7)\nand gin(\u00b7), are introduced which aim to capture the output\nand context structure respectively:\np(yt|yt\u22121\n1\n) \u221dexp\n\u0000gout(E)gin(ht) + b\n\u0001\n,\n(4)\n\u221dexp\n\u0000\u03c3(EU + bu)\u03c3(Vht + bv) + b\n\u0001\n(5)\nwhere \u03c3(\u00b7) is a nonlinear activation function such as ReLU\nor Tanh, the matrix U \u2208IRd\u00d7dj and bias bu \u2208IRdj are\nthe linear projection of the encoded outputs, and the matrix\nV \u2208IRdj\u00d7dh and bias bv \u2208IRdj are the linear projection\nof the context, and b \u2208IRV captures the biases of the target\noutputs in the vocabulary.\nThe parameterisation of Eq. 5 enables learning a more rich\noutput structure than the bilinear mapping of Eq. 3 because\nit learns nonlinear relationships. Both, however, allow for\ncontrolling the capacity of the output layer independently\nof the dimensionality of the context ht and the word em-\nbedding E, by increasing the breadth of the joint projection,\ne.g. the dimensionality of the U and V matrices in Eq. 5\nabove. This increased capacity can be seen in the inequali-\nties below for the number of parameters of the output layers\n", "Deep Residual Output Layers for Neural Language Generation\nModel\n#Param\nValidation\nTest\nMikolov & Zweig (2012) \u2013 RNN-LDA + KN-5 + cache\n9M\u2021\n-\n92.0\nZaremba et al. (2014) \u2013 LSTM\n20M\n86.2\n82.7\nGal & Ghahramani (2016a) \u2013 Variational LSTM (MC)\n20M\n-\n78.6\nKim et al. (2016) \u2013 CharCNN\n19M\n-\n78.9\nMerity et al. (2016) \u2013 Pointer Sentinel-LSTM\n21M\n72.4\n70.9\nGrave et al. (2016) \u2013 LSTM + continuous cache pointer\u2020\n-\n-\n72.1\nInan et al. (2016) \u2013 Tied Variational LSTM + augmented loss\n24M\n75.7\n73.2\nZilly et al. (2017) \u2013 Variational RHN\n23M\n67.9\n65.4\nZoph & Le (2016) \u2013 NAS Cell\n25M\n-\n64.0\nMelis et al. (2017) \u2013 2-layer skip connection LSTM\n24M\n60.9\n58.3\nMerity et al. (2017) \u2013 AWD-LSTM w/o \ufb01netune\n24M\n60.7\n58.8\nMerity et al. (2017) \u2013 AWD-LSTM\n24M\n60.0\n57.3\nOurs \u2013 AWD-LSTM-DRILL w/o \ufb01netune\n24M\n59.6\n57.0\nOurs \u2013 AWD-LSTM-DRILL\n24M\n58.2\n55.7\nMerity et al. (2017) \u2013 AWD-LSTM + continuous cache pointer\u2020\n24M\n53.9\n52.8\nKrause et al. (2018) \u2013 AWD-LSTM + dynamic evaluation\u2020\n24M\n51.6\n51.1\nOurs \u2013 AWD-LSTM-DRILL + dynamic evaluation\u2020\n24M\n49.5\n49.4\nYang et al. (2018) \u2013 AWD-LSTM-MoS\n22M\n56.54\n54.44\nYang et al. (2018) \u2013 AWD-LSTM-MoS + dynamic evaluation\u2020\n22M\n48.33\n47.69\nTable 1. Model perplexity with a single softmax (upper part) and multiple softmaxes (lower part) on validation and test sets on Penn\nTreebank. Baseline results are obtained from Merity et al. (2017) and Krause et al. (2018). \u2020 indicates the use of dynamic evaluation.\n4. Experiments\nWe evaluate on three language generation tasks. The \ufb01rst\ntwo tasks are standard language modeling tasks, i.e. predict-\ning the next word given the sequence of previous words. The\nthird task is a conditional language modeling task, namely\nneural machine translation, i.e. predicting the next word\nin the target language given the source sentence and the\nprevious words in the translation. To demonstrate the gen-\nerality of the proposed output mapping we incorporate it\nin three different neural architectures which are considered\nstate-of-the-art for their corresponding tasks.\n4.1. Language Modeling\nDatasets and Metrics. Following previous work in lan-\nguage modeling (Yang et al., 2018; Krause et al., 2018;\nMerity et al., 2017; Melis et al., 2017), we evaluate the pro-\nposed model in terms of perplexity on two widely used lan-\nguage modeling datasets, namely Penn Treebank (Mikolov\net al., 2010) and WikiText-2 (Merity et al., 2017) which\nhave vocabularies of 10,000 and 33,278 words, respectively.\nFor fair comparison, we use the same regularization and\noptimization techniques with Merity et al. (2017).\nModel Con\ufb01guration. To compare with the state-of-the-\nart we use the proposed output layer within the best ar-\nchitecture by Merity et al. (2017), which is a highly regu-\nlarized 3-layer LSTM with 400-dimensional embeddings\nand 1150-dimensional hidden states, noted as AWD-LSTM.\nOur hyper-parameters were optimized based on validation\nperplexity, as follows: 4-layer label encoder depth, 400-\ndimensional label embeddings, 0.6 dropout rate, residual\nconnection to E, uniform weight initialization in the interval\n[\u22120.1, 0.1], for both datasets, and, furthermore, sigmoid ac-\ntivation and variational dropout for PennTreebank, as well\nas relu activation and standard dropout for Wikitext-2. The\nrest of the hyper-parameters were set to the optimal ones\nfound for each dataset by Merity et al. (2017).\nFor the implementation of the AWD-LSTM we used the\nlanguage modeling toolkit in Pytorch provided by Merity\net al. (2017),3 and for the dynamic evaluation the code in\nPytorch provided by Krause et al. (2018).4\n4.1.1. RESULTS\nThe results in terms of perplexity for our models, denoted\nby DRILL, and several competitive baselines, are displayed\nin Table 1 for PennTreebank and Table 2 for Wikitext-2. For\nthe single-softmax models (above the double lines), for both\ndatasets, our models improve over the state-of-the-art by\n+1.6 perplexity on PennTreebank and by +3.9 perplexity on\nWikitext-2. Moreover, when our model is combined with\nthe dynamic evaluation approach proposed by Krause et al.\n(2018), it improves even more over these models by +1.7 on\nPennTreebank and by +2.3 on Wikitext-2.\nIn contrast to other more complicated previous models, our\nmodel uses a standard LSTM architecture, following the\nwork of Merity et al. (2017); Melis et al. (2017). For in-\nstance, Zilly et al. (2017) uses of a recurrent highway net-\nwork which is an extension of an LSTM to allow multiple\n3http://github.com/salesforce/awd-lstm-lm\n4http://github.com/benkrause/dynamic-\nevaluation\n", "Deep Residual Output Layers for Neural Language Generation\nNikolaos Pappas 1 James Henderson 1\nAbstract\nMany tasks, including language generation, bene-\n\ufb01t from learning the structure of the output space,\nparticularly when the space of output labels is\nlarge and the data is sparse. State-of-the-art neu-\nral language models indirectly capture the output\nspace structure in their classi\ufb01er weights since\nthey lack parameter sharing across output labels.\nLearning shared output label mappings helps, but\nexisting methods have limited expressivity and are\nprone to over\ufb01tting. In this paper, we investigate\nthe usefulness of more powerful shared mappings\nfor output labels, and propose a deep residual out-\nput mapping with dropout between layers to better\ncapture the structure of the output space and avoid\nover\ufb01tting. Evaluations on three language gener-\nation tasks show that our output label mapping\ncan match or improve state-of-the-art recurrent\nand self-attention architectures, and suggest that\nthe classi\ufb01er does not necessarily need to be high-\nrank to better model natural language if it is better\nat capturing the structure of the output space.\n1. Introduction\nLearning the structure of the output space bene\ufb01ts a wide\nvariety of tasks, such as object recognition and novelty de-\ntection in images (Weston et al., 2011; Socher et al., 2013;\nFrome et al., 2013; Zhang et al., 2016; Chen et al., 2018a),\nzero-shot prediction in texts (Dauphin et al., 2014; Yazdani\n& Henderson, 2015; Nam et al., 2016; Rios & Kavuluru,\n2018), and structured prediction in either images or text\n(Srikumar & Manning, 2014a; Dyer et al., 2015; Belanger\n& McCallum, 2016; Graber et al., 2018). When the space of\noutput labels is large or their data is sparse, treating labels\nas independent classes makes learning dif\ufb01cult, because\nidentifying one label is not helped by data for other labels.\nThis problem can be addressed by learning output label em-\n1Idiap Research Institute, Martigny, Switzerland. Correspon-\ndence to: Nikolaos Pappas <nikolaos.pappas@idiap.ch>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nbeddings to capture the similarity structure of the output\nlabel space, so that data for similar labels can help classi-\n\ufb01cation, even to the extent of enabling few-shot or even\nzero-shot classi\ufb01cation. This approach has been particularly\nsuccessful in natural language generation tasks, where word\nembeddings give a useful similarity structure for next-word-\nprediction in tasks such as machine translation (Vaswani\net al., 2017) and language modeling (Merity et al., 2017).\nExisting neural language models typically use a log-linear\nclassi\ufb01er to predict words (Vaswani et al., 2017; Chen et al.,\n2018b). We can view the output label weights as a word\nembedding, and the input encoder as mapping the context to\na vector in the same embedding space. Then the similarity\nbetween these two embeddings in this joint input-label space\nis measured with a dot product followed by the softmax\nfunction. We will refer to this part as the classi\ufb01er, distinct\nfrom the input encoder which only depends on the input\nand the label encoder which only depends on the label. To\nimprove performance and reduce model size, sometimes the\noutput label weights are tied to the input word embedding\nvectors (Inan et al., 2016; Press & Wolf, 2017), but there is\nno parameter sharing taking place across different words,\nwhich limits the effective transfer between them.\nRecent work has shown improvements over speci\ufb01c vanilla\nrecurrent architectures by sharing parameters across outputs\nthrough a bilinear mapping on neural language modeling\n(Gulordava et al., 2018) or a dual nonlinear mapping on\nneural machine translation (Pappas et al., 2018), which can\nmake the classi\ufb01er more powerful. However, the shallow\nmodeling constraints and the lack of regularization capabil-\nities limit their applicability on arbitrary tasks and model\narchitectures. Orthogonal to these studies, Yang et al. (2018)\nachieved state-of-the-art improvements on language model-\ning by increasing the power of the classi\ufb01er using a mixture\nof softmax functions, albeit at the expense of computational\nef\ufb01ciency. A natural question arises of whether one can\nmake the classi\ufb01er more powerful by simply increasing the\npower of the label mapping while using a single softmax\nfunction without modifying its dimensionality or rank.\nIn this paper, we attempt to answer this question by in-\nvestigating alternative neural architectures for learning the\nembedding of an output label in the joint input-label space\nwhich address the aforementioned limitations. In particular,\narXiv:1905.05513v2  [cs.CL]  22 May 2019\n", "Deep Residual Output Layers for Neural Language Generation\nModel\n#Param\nValidation\nTest\nInan et al. (2016) \u2013 Variational LSTM + augmented loss\n28M\n91.5\n87.0\nGrave et al. (2016) \u2013 LSTM + continuous cache pointer\u2020\n-\n-\n68.9\nMelis et al. (2017) \u2013 2-layer skip connection LSTM\n24M\n69.1\n65.9\nMerity et al. (2017) \u2013 AWD-LSTM w/o \ufb01netune\n33M\n69.1\n66.0\nMerity et al. (2017) \u2013 AWD-LSTM\n33M\n68.6\n65.8\nOurs \u2013 AWD-LSTM-DRILL w/o \ufb01netune\n34M\n65.7\n62.8\nOurs \u2013 AWD-LSTM-DRILL\n34M\n64.9\n61.9\nMerity et al. (2017) \u2013 AWD-LSTM + continuous cache pointer \u2020\n33M\n53.8\n52.0\nKrause et al. (2018) \u2013 AWD-LSTM + dynamic evaluation\u2020\n33M\n46.4\n44.3\nOurs \u2013 AWD-LSTM-DRILL + dynamic evaluation\u2020\n34M\n43.9\n42.0\nYang et al. (2018) \u2013 AWD-LSTM-MoS\n35M\n63.88\n61.45\nYang et al. (2018) \u2013 AWD-LSTM-MoS + dynamical evaluation\u2020\n35M\n42.41\n40.68\nTable 2. Model perplexity with a single softmax (upper part) and multiple softmaxes (lower part) on validation and test sets on WikiText-2.\nBaseline results are obtained from Merity et al. (2017) and Krause et al. (2018). \u2020 indicates the use of dynamic evaluation.\nhidden state updates per time step, Zoph & Le (2016) uses\nreinforcement learning to generate an RNN cell which is\neven more complicated than an LSTM cell, and Merity et al.\n(2016) makes use of a probabilistic mixture model which\ncombines a typical language model with a pointer network\nwhich reproduces words from the recent context.\nInterestingly, our model also signi\ufb01cantly reduces the perfor-\nmance gap against multiple softmax models. In particular,\nwhen our \ufb01netuned model is compared to the corresponding\nmixture-of-softmaxes (MoS) model, which makes use of 15\nsoftmaxes in the classi\ufb01er, it reduces the difference against\nAWD-LSTM from 2.8 to 1.2 points on PennTreebank and\nfrom 4.3 to 0.4 points on WikiText-2. When our model is\ncompared to MoS with dynamic evaluation, the difference\nis reduced from 3.4 points to 1.7 points on PennTreebank\nand from 3.6 to 1.3 on WikiText-2. Note that the rank of the\nlog-probability matrix for MoS on PennTreebank is 9,981,\nwhile for AWD-LSTM and our model the rank is only 400.\nThis observation questions the high-rank hypothesis of MoS,\nwhich states that the log-probability matrix has to be high\nrank to better capture language. Our results suggest that the\nlog-probability matrix does not need to be high rank if the\nclassi\ufb01er is better at capturing the output space structure.\nFurthermore, as shown in Table 3, the MoS model is far\nslower than AWD-LSTM, even for these small datasets\nand reduced dimensionality settings,5 whereas adding our\nlabel encoder to AWD-LSTM results in only a small speed\ndifference. In particular, on PennTreebank the MoS model\ntakes about 139 seconds per epoch while AWD-LSTM about\n47 seconds per epoch, which makes it slower by a factor\nof 3.0\u00d7, whereas our model is only 1.1\u00d7 slower than this\n5 Note that even though the MoS models have a comparable\nnumber of parameters to the other models, they use smaller values\nfor several crucial hyper-parameters, such as word embedding size,\nhidden state size and batch size, likely to make the training speed\nmore manageable and avoid over\ufb01tting.\nModel\nPennTreebank Wikitext-2\nAWD-LSTM\n47 sec (1.0\u00d7)\n89 sec (1.0\u00d7)\nAWD-LSTM-DRILL 53 sec (1.1\u00d7)\n106 sec (1.2\u00d7)\nAWD-LSTM-MoS\n139 sec (3.0\u00d7)\n862 sec (9.7\u00d7)\nTable 3. Average time taken per epoch on the two datasets: Pen-\nnTreebank (|V| \u224820K) and Wikitext-2 (|V| \u224833K).\nbaseline. On Wikitext-2, the differences are even more\npronounced due to the larger size of the vocabulary. The\nMoS model takes about 862 seconds per epoch while AWD-\nLSTM takes about 89 seconds per epoch, which makes it\nslower by a factor of 9.7\u00d7, whereas our model with 4-layers\nis only 1.2\u00d7 slower than the baseline. We attempted to\ncombine our label encoder with the MoS model, but its\ntraining speed exceeded our computation budget.\nOverall, these results demonstrate that the proposed deep\nresidual output mapping improves signi\ufb01cantly the state-\nof-the-art single-softmax neural architecture for language\nmodeling, namely AWD-LSTM, without hurting its ef\ufb01-\nciency. Hence, it could be a useful and practical addition to\nother existing architectures. In addition, our model remains\ncompetitive against models based on multiple softmaxes and\ncould be combined with them in the future, since our work\nis orthogonal to using multiple softmaxes. To demonstrate\nthat our model is also applicable to larger datasets as well, in\nSection 4.2 below we apply our method to neural machine\ntranslation. But before moving to that experiment, we \ufb01rst\nperform an ablation analysis of these results.\n4.1.2. ABLATION ANALYSIS\nTo give further insights into the source of the improvement\nfrom our output layer parameterisation, in Table 4 we com-\npare its ablated variants with previous output layer parame-\nterisations. Each alternative is combined with the state-of-\nthe-art encoder network AWD-LSTM (Merity et al., 2017).\nWe observe that full softmax produces the highest perplexity\n"], "summary": "The cited paper, \"Deep Residual Output Layers for Neural Language Generation,\" focuses on improving the output layer of neural language models, particularly for tasks like language modeling and machine translation. While your work addresses issues in variational autoencoders (VAEs) for language generation, such as information under-representation and posterior collapse, the cited paper offers a different perspective on enhancing language generation by better capturing the output space structure. It proposes a deep residual output mapping to improve the classifier's ability to model language, which could potentially complement your approach by providing a more robust output mechanism for your discrete variational attention model. Specifically, the cited paper's exploration of single softmax functions and their efficiency compared to multiple softmaxes could be relevant if you consider different output layer designs for your VAE.", "citation": "Haoqin Tu, Yitong Li (2022). An Overview on Controllable Text Generation via Variational Auto-Encoders. arXiv:2211.07954. https://arxiv.org/abs/2211.07954"}, {"paper_id": 53, "text": ["On Variational Learning of Controllable Representations for Text without Supervision\npropose to constrain the posterior mean to a learned prob-\nability simplex and only perform manipulation within the\nprobability simplex, which is referred as CP-VAE (Con-\nstrained Posterior VAE). Two regularizers are added to the\noriginal objective of VAE. The \ufb01rst enforces an orthogonal\nstructure of the learned probability simplex; the other en-\ncourages this simplex to be \ufb01lled without holes. Besides\ncon\ufb01rming that latent vacancy is indeed a cause of fail-\nure in previous sequence VAEs\u2019, CP-VAE is also the \ufb01rst\nsuccessful attempt towards unsupervised learning of control-\nlable representations for text to the best of our knowledge.\nExperimental results on text style transfer show that our\nmethod outperforms unsupervised baselines and strong su-\npervised approaches, whose decoding network are trained\nfrom scratch. Without supervision and the help of pre-\ntraining for generation, our method achieves comparable\nresults with state-of-the-art supervised approaches lever-\naging large-scale pre-trained models for generation, with\nrespect to the automatic evaluation metrics used in text style\ntransfer. Our proposed framework also enables \ufb01ner-grained\nand more \ufb02exible control over text generation. In particular,\nwe can switch the topic in the middle of sentence generation,\nand the model will often still \ufb01nd a way to complete the\nsentence in a natural way, which has never been attempted\nby previous methods.1\n2. Background: Variational Autoencoders\nThe variational autoencoder (VAE) (Kingma & Welling,\n2013) is a generative model de\ufb01ned by a prior p(z) and\na conditional distribution p\u03b8(x|z). The VAE is trained to\noptimize a tractable variational lower bound of log p\u03b8(x):\nLVAE(x;\u03b8,\u03c6) =Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)]\n\u2212DKL(q\u03c6(z|x)||p(z)),\n(1)\nwhere q\u03c6(z|x) is a variational distribution parameterized\nby an encoding network with parameters \u03c6, and p\u03b8(x|z)\ndenotes the decoding network with parameters \u03b8. This ob-\njective tries to minimize the reconstruction error to generate\nthe data, and at the same time regularizes q\u03c6(z|x) towards\nthe prior p(z). For text modelling, the input x is some ob-\nserved text. Both the encoding and decoding network are\nusually recurrent neural networks, and the model is called a\nsequence VAE.\nNote that during learning, the decoding network p\u03b8(x|z)\nonly learns to decode z that are sampled from q\u03c6(z|x). In\nother words, the decoding network is never trained to de-\ncode the entire latent space. Instead, it only learns to pro-\ncess z sampled from the aggregated posterior distribution\nq\u03c6(z) = Ex\u223cpd(x)q\u03c6(z|x), where pd(x) is the training data\ndistribution. If q\u03c6(z) has regions of low density, there is\n1The code to reproduce our results can be found in https:\n//github.com/BorealisAI/CP-VAE\nno guarantee that p\u03b8 would generalize well to such places.\nThis is an important intuition that will become central to our\nanalysis in Sec. 3.\n3. Latent Vacancy Hypothesis\nWe hypothesize that when trained on text data, the aggre-\ngated posterior of sequence-VAEs tend to have vacant re-\ngions of low density, where the decoder may fail to general-\nize to. The decoder could generalize to the vacant regions\nwithout ever seeing training examples, but there is no guar-\nantee it can perform well in this case especially if the such\nvacancy is large. Fig. 1 is an illustration of the intuition.\nIn this section, we conduct exploratory study on unsuper-\nvised sentiment manipulation and provide evidence from\ntwo different aspects to verify the above-mentioned hypoth-\nesis. First, we measure how the negative log-likelihood of\nlatent codes under the aggregated posterior changes before\nand after manipulation. Second, since topology is the tech-\nnical language to describe the notion of vacant regions or\nholes, we employ topological data analysis to con\ufb01rm the\nexacerbation of latent vacancy problem on text as compared\nto images. In addition, we give a preview of our proposed\nmethod (later formally introduced in Section 4) and demon-\nstrate that it avoids the latent vacancy problem using the\nsame analyses.\nFigure 1. Illustration of why latent vacancy prevents effective ma-\nnipulation in VAEs. The aggregated posterior shown has multiple\ndisconnected areas and direct manipulations of the relevant factor\nmay fall into vacant regions of low density.\n3.1. Unsupervised Sentiment Manipulation\nHere we describe the setup used to discover a sentiment\nlatent dimension and subsequent exploration of manipulat-\ning the sentiment attribute. Note that discovering sentiment\n", "On Variational Learning of Controllable Representations for Text without Supervision\nmaximize\nK\nX\ni=1\np2\ni ,\nsubject to\nK\nX\ni=1\npi = 1.\nBy introducing a Lagrange multiplier \u03bb, the Lagrange function is de\ufb01ned as\nL(p1, p2, . . . , pK, \u03bb) =\nK\nX\ni=1\np2\ni \u2212\u03bb(\nK\nX\ni=1\npi \u22121).\nIn order to \ufb01nd the optimal point, we require that\n\u2202\n\u2202pi\n K\nX\ni=1\np2\ni \u2212\u03bb(\nK\nX\ni=1\npi \u22121)\n!\n= 2pi \u2212\u03bb = 0,\ni = 1, 2, . . . , K,\nwhich shows that all pi are equal. By using the constraint P\ni pi = 1, we \ufb01nd pi = 1\nK , i = 1, 2, . . . , K. By plugging into\nthe results, \u00b5\u22a4\u00b5 = \u03b1 P\ni p2\ni reaches its minimum \u03b1\nK .\nE. Comparisons with Baselines on Topic Modelling\nExperimental setup:\nWe use the AG news dataset for this task constructed by (Zhang et al., 2015). It contains four topic\ncategories which are World, Sports, Business and Sci/Tech, with the title and description \ufb01elds. For each category, there\nare 30, 000 training samples and 1, 900 test samples. In this paper, we drop the title and just use the description \ufb01eld. We\ncompare our approach to two standard baselines for unsupervised topic modelling: (1) LDA (Blei et al., 2003), a standard\nimplementation of LDA is used for this baseline5; (2) k-means. To show the power of our approach beyond the pre-trained\nsentence representations, we perform k-means clustering directly on the sentence representations. Following (Manning et al.,\n2010), we assign each inferred topic to one of the gold-standard topics with the optimal mapping and report the precision\n(a.k.a. purity), recall (a.k.a. collocation) and F1 score. The number of topics is chosen to be 10. The results reported for the\nbaselines and our model are the average over 10 runs.\nQuantitative results:\nThe results are shown in Table 7. We can see that our approach achieves comparable results to\nLDA while signi\ufb01cantly outperforming k-means in all four categories, indicating that our approach can go beyond just\nclustering on pre-trained sentence representations.\nTable 7. Results for topic identi\ufb01cation.\nTopic\nModel\nPrecision\nRecall\nF1\nWorld\nLDA\n69.73\n75.32\n72.14\nk-means\n67.64\n47.63\n55.90\nOurs\n80.83\n70.55\n74.59\nSports\nLDA\n79.17\n82.50\n80.22\nk-means\n47.66\n89.50\n62.04\nOurs\n81.14\n78.88\n79.49\nBusiness\nLDA\n72.10\n66.45\n68.46\nk-means\n53.06\n53.16\n53.11\nOurs\n64.04\n64.53\n63.97\nSci/Tech\nLDA\n66.55\n59.77\n61.60\nk-means\n81.32\n31.59\n44.67\nOurs\n65.20\n71.74\n66.77\n5https://radimrehurek.com/gensim/\n", "On Variational Learning of Controllable\nRepresentations for Text without Supervision\nPeng Xu 1 Jackie Chi Kit Cheung 1 2 3 Yanshuai Cao 1\nAbstract\nThe variational autoencoder (VAE) can learn the\nmanifold of natural images on certain datasets, as\nevidenced by meaningful interpolation or extrap-\nolation in the continuous latent space. However,\non discrete data such as text, it is unclear if un-\nsupervised learning can discover a similar latent\nspace that allows controllable manipulation. In\nthis work, we \ufb01nd that sequence VAEs trained on\ntext fail to properly decode when the latent codes\nare manipulated, because the modi\ufb01ed codes of-\nten land in holes or vacant regions in the aggre-\ngated posterior latent space, where the decoding\nnetwork fails to generalize. Both as a validation\nof the explanation and as a \ufb01x to the problem,\nwe propose to constrain the posterior mean to a\nlearned probability simplex, and perform manipu-\nlation within this simplex. Our proposed method\nmitigates the latent vacancy problem and achieves\nthe \ufb01rst success in unsupervised learning of con-\ntrollable representations for text. Empirically, our\nmethod outperforms unsupervised baselines and\nstrong supervised approaches on text style trans-\nfer, and is capable of performing more \ufb02exible\n\ufb01ne-grained control over text generation than ex-\nisting methods.\n1. Introduction\nHigh-dimensional data, such as images and text, are often\ngenerated through the interaction of many complex factors,\nsuch as lighting and pose in images or style and content in\ntexts. Recently, VAEs and other unsupervised generative\nmodels have found successes in modelling the manifold of\nnatural images (Higgins et al., 2017; Kumar et al., 2017;\nChen et al., 2016). These models often discover controllable\nlatent factors that allow manipulation of the images through\n1Borealis AI 2McGill University 3Canada CIFAR Chair, Mila.\nCorrespondence to: Peng Xu <pxu4@ualberta.ca>.\nProceedings of the 37 th International Conference on Machine\nLearning, Online, PMLR 119, 2020. Copyright 2020 by the au-\nthor(s).\nconditional generation from interpolated or extrapolated la-\ntent codes, often with impressive quality. On the other hand,\nwhile various attributes of text such as sentiment and topic\ncan be discovered in an unsupervised way, manipulating the\ntext by changing these learned factors has not been possi-\nble with unsupervised generative models, to the best of our\nknowledge. C\u00b4\u0131fka et al. (2018); Zhao et al. (2018) observed\nthat text manipulation is generally more challenging com-\npared to images, and the successes of these models cannot\nbe directly transferred to texts.\nControllable text generation aims at generating realistic text\nwith control over various attributes including sentiment,\ntopic and other high-level properties. The possibility of\nunsupervised controllable text generation could help in a\nwide range of applications such as dialogues systems (Wen\net al., 2016). Existing approaches (Shen et al., 2017; Fu\net al., 2018; Li et al., 2018; Sudhakar et al., 2019) all rely\non supervised learning from annotated attributes to generate\nthe text in a controllable fashion. The high cost of labelling\nlarge training corpora with attributes of interest limits the\nusage of these models, as pre-existing annotations often do\nnot align with desired downstream goals. Even if cheap\nlabels are available, for example, review scores as a proxy\nfor sentiment, the control is limited to the variation de\ufb01ned\nby the attributes.\nIn this work, we examine the obstacles that prevent sequence\nVAEs (Bowman et al., 2015) from performing well in un-\nsupervised controllable text generation. We empirically\ndiscover that manipulating the latent factors for typical se-\nmantic variations often leads to latent codes that reside in\nsome low-density region of the aggregated posterior dis-\ntribution. In other words, there are vacant regions in the\nlatent code space (Makhzani et al., 2015; Rezende & Viola,\n2018) not being considered by the decoding network, at\nleast not at convergence. As a result, the decoding network\nis unable to process such manipulated latent codes, yielding\nunpredictable generation results of low quality. Although\nthis issue has been raised in prior works, we provide direct\nevidence using topological data analysis to show that this\nvacancy problem is more severe for VAEs trained on text\nthan image.\nIn order to mitigate the latent vacancy problem on text, we\narXiv:1905.11975v4  [cs.CL]  7 Aug 2020\n", "On Variational Learning of Controllable Representations for Text without Supervision\nSudhakar et al., 2019; Logeswaran et al., 2018; Lample et al.,\n2018). The requirement of labelled data largely restricts the\ncapabilities and the applications of these models. Instead,\nall our proposed framework needs is raw text without any\nannotated attribute.\n7. Conclusion\nIn this work, we investigate latent vacancy as an important\nproblem in unsupervised learning of controllable representa-\ntions when modelling text with VAEs. To mitigate this, we\npropose to constrain the posterior within a learned probabil-\nity simplex and encourage this space to be \ufb01lled, achieving\nthe \ufb01rst success towards controlled text generation without\nsupervision. However, the constrained posterior also means\nthat the aggregated posterior can never match the isotropic\nGaussian prior which points to a potential future direction\nto resolve this mismatch by selecting or learning a better\nprior as in (Tomczak & Welling, 2017).\nAcknowledgements\nThanks to Ivan Kobyzev for the useful discussion and feed-\nback, and to all the anonymous reviewers for their valuable\ninputs.\nReferences\nBao, Y., Zhou, H., Huang, S., Li, L., Mou, L., Vechtomova,\nO., Dai, X., and Chen, J. Generating sentences from dis-\nentangled syntactic and semantic spaces. arXiv preprint\narXiv:1907.05789, 2019.\nBengio, Y., Courville, A., and Vincent, P. Representation\nlearning: A review and new perspectives. IEEE transac-\ntions on pattern analysis and machine intelligence, 35(8):\n1798\u20131828, 2013.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\nallocation. Journal of machine Learning research, 3(Jan):\n993\u20131022, 2003.\nBowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Joze-\nfowicz, R., and Bengio, S. Generating sentences from\na continuous space. arXiv preprint arXiv:1511.06349,\n2015.\nChen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever,\nI., and Abbeel, P. Infogan: Interpretable representation\nlearning by information maximizing generative adversar-\nial nets. In Advances in neural information processing\nsystems, pp. 2172\u20132180, 2016.\nC\u00b4\u0131fka, O., Severyn, A., Alfonseca, E., and Filippova, K. Eval\nall, trust a few, do wrong to none: Comparing sentence\ngeneration models. arXiv preprint arXiv:1804.07972,\n2018.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nFu, Z., Tan, X., Peng, N., Zhao, D., and Yan, R. Style\ntransfer in text: Exploration and evaluation. In Thirty-\nSecond AAAI Conference on Arti\ufb01cial Intelligence, 2018.\nHe, J., Spokoyny, D., Neubig, G., and Berg-Kirkpatrick, T.\nLagging inference networks and posterior collapse in vari-\national autoencoders. arXiv preprint arXiv:1901.05534,\n2019.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nvae: Learning basic visual concepts with a constrained\nvariational framework. In International Conference on\nLearning Representations, volume 3, 2017.\nHu, Z., Yang, Z., Liang, X., Salakhutdinov, R., and Xing,\nE. P. Toward controlled generation of text. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1587\u20131596. JMLR. org, 2017.\nHyndman, R. J. Computing and graphing highest density\nregions. The American Statistician, 50(2):120\u2013126, 1996.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKumar, A., Sattigeri, P., and Balakrishnan, A. Variational\ninference of disentangled latent concepts from unlabeled\nobservations. arXiv preprint arXiv:1711.00848, 2017.\nLample, G., Subramanian, S., Smith, E., Denoyer, L., Ran-\nzato, M., and Boureau, Y.-L.\nMultiple-attribute text\nrewriting. 2018.\nLi, J., Jia, R., He, H., and Liang, P. Delete, retrieve, generate:\nA simple approach to sentiment and style transfer. arXiv\npreprint arXiv:1804.06437, 2018.\nLogeswaran, L., Lee, H., and Bengio, S. Content preserving\ntext generation with attribute controls. In Advances in\nNeural Information Processing Systems, pp. 5103\u20135113,\n2018.\nMakhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and\nFrey, B.\nAdversarial autoencoders.\narXiv preprint\narXiv:1511.05644, 2015.\nManning, C., Raghavan, P., and Sch\u00a8utze, H. Introduction to\ninformation retrieval. Natural Language Engineering, 16\n(1):100\u2013103, 2010.\n"], "summary": "The cited paper, \"On Variational Learning of Controllable Representations for Text without Supervision,\" addresses the \"latent vacancy\" problem in VAEs for text generation, where manipulated latent codes fall into low-density regions, leading to poor decoding. This is relevant to my work's focus on information under-representation and posterior collapse in VAEs for language generation. While my work proposes discrete variational attention and an auto-regressive prior to enhance the latent space and avoid posterior collapse, the cited paper tackles a related issue of latent space utility for controllable generation by constraining the posterior mean to a learned probability simplex. Both papers aim to improve the effectiveness of VAEs for language generation, with the cited paper focusing on unsupervised controllable text generation and my work on general language generation with discrete latent spaces.", "citation": "Lei Shu, Alexandros Papangelis, Yi-Chia Wang, Gokhan Tur, Hu Xu, Zhaleh Feizollahi, Bing Liu, Piero Molino (2020). Controllable Text Generation with Focused Variation. arXiv:2009.12046. https://arxiv.org/abs/2009.12046"}, {"paper_id": 50, "text": ["and model the prior of the attention vector by a Gaussian distribution, for which we further propose two\nplausible priors, whose mean is either a zero vector or an average of source hidden states.\nWe evaluate our approach on two experiments: question generation and dialog systems. Experiments\nshow that the proposed variational attention yields a higher diversity than variational Seq2Seq with de-\nterministic attention, while retaining high quality of generated sentences. In this way, we make VED\nwork properly with the powerful attention mechanism.\nIn summary, the main contributions of this paper are two-fold: (1) We discover a \u201cbypassing\u201d phe-\nnomenon in VED, which could make the learning of variational space ineffective. (2) We propose a\nvariational attention mechanism that models the attention vector as random variables to alleviate the\nabove problem. To the best of our knowledge, we are the \ufb01rst to address the attention mechanism in\nvariational encoder-decoder neural networks. Our model is a general framework, which can be applied\nfor various text generation tasks.\n2\nBackground and Motivation\nIn this section, we introduce the variational autoencoder and the attention mechanism. We also present a\npilot experiment motivating our variational attention model.\n2.1\nVariational Autoencoder (VAE)\nA VAE encodes data Y (e.g., a sentence) as hidden random variables Z, based on which the decoder\nreconstructs Y . Consider a generative model, parameterized by \u03b8, as\np\u03b8(Z, Y ) = p\u03b8(Z)p\u03b8(Y |Z)\n(1)\nGiven a dataset D = {y(n)}N\nn=1, the likelihood of a data point is\nlog p\u03b8(y(n)) \u2265Ez\u223cq\u03c6(z|y(n))\n\"\nlog\n(\np\u03b8(y(n), z)\nq\u03c6(z|y(n))\n)#\n= Ez\u223cq\u03c6(z|y(n))\nh\nlog p\u03b8(y(n)|z)\ni\n\u2212KL\n\u0010\nq\u03c6(z|y(n))\u2225p(z)\n\u0011 \u2206= L(n)(\u03b8, \u03c6)\n(2)\nVAE models both q\u03c6(z|y) and p\u03b8(y|z) with neural networks, parametrized by \u03c6 and \u03b8, respectively.\nFigure 1a shows the graphical model of this process. The training objective is to maximize the lower\nbound of the likelihood L(\u03b8, \u03c6), which can be rewritten as minimizing\nJ(n) = Jrec(\u03b8, \u03c6, y(n)) + KL\n\u0010\nq\u03c6(z|y(n))\u2225p(z)\n\u0011\n(3)\nThe \ufb01rst term, called reconstruction loss, is the (expected) negative log-likelihood of data, similar to\ntraditional deterministic autoencoders. The expectation is obtained by Monte Carlo sampling. The sec-\nond term is the KL-divergence between z\u2019s posterior and prior distributions. Typically the prior is set to\nstandard normal N(0, I).\n2.2\nVariational Encoder-Decoder (VED)\nIn some applications, we would like to transform source information to target information, e.g., machine\ntranslation, dialogue systems, and text summarization. In these tasks, \u201cauto\u201d-encoding is not suf\ufb01cient,\nand an encoding-decoding framework is required. Different efforts have been made to extend VAE to\nvariational encoder-decoder (VED) frameworks, which transform an input X to output Y . One possible\nextension is to condition all probabilistic distributions further on X (Zhang et al., 2016; Cao and Clark,\n2017; Serban et al., 2017). In this case, the posterior of z is given by q\u03c6(z|X, Y ). This, however,\nintroduces a discrepancy between training and prediction, since Y is not available during the prediction\nstage.\n", "Figure 4: BLEU-2, BLEU-4, Entropy, and Dist-1 calculated on the validation set as training progresses.\nFigure 5: BLEU-2, BLEU-4, Entropy, and Dist-1 with different \u03b3a values.\nSource when the british forces evacuated at the close of the war in 1783 ,\nthey transported 3,000 freedmen for resettlement in nova scotia .\nReference in what year did the american revolutionary war end ?\nVED+DAttn\nhow many people evacuated in newfoundland ?\nhow many people evacuated in newfoundland ?\nwhat did the british forces seize in the war ?\nVED+Vattn-\u00afh\nhow many people lived in nova scotia ?\nwhere did the british forces retreat ?\nwhen did the british forces leave the war ?\nSource downstream , more than 200,000 people were evacuated from\nmianyang by june 1 in anticipation of the dam bursting .\nReference how many people were evacuated downstream ?\nVED+DAttn\nhow many people evacuated from the mianyang basin ?\nhow many people evacuated from the mianyang basin ?\nhow many people evacuated from the mianyang basin ?\nVED+VAttn-\u00afh\nhow many people evacuated from the tunnel ?\nhow many people evacuated from the dam ?\nhow many people were evacuated from fort in the dam ?\nTable 3: Case study of question generation.\nHuman Evaluation.\nIn order to assess the quality of the generated text in terms of language \ufb02uency,\na human evaluation study was carried out. For each of the two models under comparison (VED+DAttn\nand VED+VAttn-\u00afh), a randomly shuf\ufb02ed subset of 100 generated questions were selected. Six human\nevaluators were asked to rate the \ufb02uency of these 200 questions on a 5-point scale: 5-Flawless, 4-Good,\n3-Adequate, 2-Poor, 1-Incomprehensible, following Stent et al. (2005). The average rating obtained for\nVED+DAttn was 3.99 and for VED+VAttn-\u00afh was 4.01, the difference between which is not statistically\nsigni\ufb01cant. The human annotations achieved 0.61 average Spearman correlation coef\ufb01cient (measuring\norder correlation) between any two annotators. According to Swinscow (1976), this indicates moderate\nto strong correlation among different annotators. Hence, we conclude variational attention does not\nnegatively affect the \ufb02uency of sentences.\nLearning curves.\nFigure 4 shows the trends of sentence quality (BLEU-2 and BLEU-4) and diversity\n(entropy and Dist-1) of all models on the validation set, as training progresses.2 We see that BLEU\nand diversity are con\ufb02icting objectives: a high BLEU score indicates resemblance to the groundtruth,\nresulting in low diversity. However, the variational attention mechanisms (red and green lines in Figure 4)\nremain high in both aspects, showing the effectiveness of our model.\nStrength of Attention\u2019s KL Loss.\nWe tuned the KL term\u2019s strength in variational attention, i.e., \u03b3a\nin Eq. (8), and plot the BLEU and diversity metrics in Figure 5.\nIn this experiment, we used the\nVED+DAttn-\u00afh variant. As shown, a decrease in \u03b3a increases the quality of generated sentences at the\ncost of diversity. This is expected because a lower \u03b3a gives the model less incentive to optimize the\nattention\u2019s KL term, which then causes the model to behave more \u201cdeterministic.\u201d Based on this experi-\n2Other metrics are omitted because the trend is the same.\n", "Variational Attention for Sequence-to-Sequence Models\nHareesh Bahuleyan\u2217\u2020 Lili Mou\u2217\u2021\nOlga Vechtomova\u2020\nPascal Poupart\u2020\n\u2020University of Waterloo, Canada\n{hpallika, ovechtomova, ppoupart}@uwaterloo.ca\n\u2021AdeptMind Research, Toronto, Canada\ndoublepower.mou@gmail.com\nAbstract\nThe variational encoder-decoder (VED) encodes source information as a set of random variables\nusing a neural network, which in turn is decoded into target data using another neural network. In\nnatural language processing, sequence-to-sequence (Seq2Seq) models typically serve as encoder-\ndecoder networks. When combined with a traditional (deterministic) attention mechanism, the\nvariational latent space may be bypassed by the attention model, and thus becomes ineffective.\nIn this paper, we propose a variational attention mechanism for VED, where the attention vector\nis also modeled as Gaussian distributed random variables. Results on two experiments show that,\nwithout loss of quality, our proposed method alleviates the bypassing phenomenon as it increases\nthe diversity of generated sentences.1\n1\nIntroduction\nThe variational autoencoder (VAE), proposed by Kingma and Welling (2014), encodes data to latent (ran-\ndom) variables, and then decodes the latent variables to reconstruct the input data. Theoretically, it opti-\nmizes a variational lower bound of the log-likelihood of the data. Compared with traditional variational\nmethods such as mean-\ufb01eld approximation (Wainwright et al., 2008), VAE leverages modern neural net-\nworks and hence is a more powerful density estimator. Compared with traditional autoencoders (Hinton\nand Salakhutdinov, 2006), which are deterministic, VAE populates hidden representations to a region (in-\nstead of a single point), making it possible to generate diversi\ufb01ed data from the vector space (Bowman\net al., 2016) or even control the generated samples (Hu et al., 2017).\nIn natural language processing (NLP), recurrent neural networks (RNNs) are typically used as both the\nencoder and decoder, known as a sequence-to-sequence (Seq2Seq) model. Although variational Seq2Seq\nmodels are much trickier to train in comparison to the image domain, Bowman et al. (2016) succeed in\ntraining a sequence-to-sequence VAE and generating sentences from a continuous latent space. Such an\narchitecture can further be extended to a variational encoder-decoder (VED) to transform one sequence\ninto another with the \u201cvariational\u201d property (Serban et al., 2017; Zhou and Neubig, 2017).\nWhen applying attention mechanisms (Bahdanau et al., 2015) to variational Seq2Seq models, however,\nwe \ufb01nd the generated sentences are of less variety, implying that the variational latent space is ineffec-\ntive. The attention mechanism summarizes source information as an attention vector by weighted sum,\nwhere the weights are a learned probabilistic distribution; then the attention vector is fed to the decoder.\nEvidence shows that attention signi\ufb01cantly improves Seq2Seq performance in translation (Bahdanau et\nal., 2015), summarization (Rush et al., 2015), etc. In variational Seq2Seq, however, the attention mecha-\nnism unfortunately serves as a \u201cbypassing\u201d mechanism. In other words, the variational latent space does\nnot need to learn much, as long as the attention mechanism itself is powerful enough to capture source\ninformation.\nIn this paper, we propose a variational attention mechanism to address this problem. We model the\nattention vector as random variables by imposing a probabilistic distribution. We follow traditional VAE\n\u2217The \ufb01rst two authors contributed equally.\n1Code is available at https://github.com/HareeshBahuleyan/tf-var-attention\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nLicense details: http:\n//creativecommons.org/licenses/by/4.0/\nIn Proceedings of COLING 2018. Also accepted by TADGM Workshop@ICML 2018 for presentation.\narXiv:1712.08207v3  [cs.CL]  21 Jun 2018\n", "Figure 2: (a) Variational Seq2Seq model. (b) Variational Seq2Seq with deterministic attention. (c)\nVariational Seq2Seq with hidden state initialization. (d) Variational Seq2Seq with variational attention.\nWe observe that, if the decoder has a direct, deterministic access to the source, the latent variables Z\nmight not capture much information so that the VAE or VED does not play a role in the process. We call\nthis a bypassing phenomenon.\nTheoretically, if p\u03b8(Y |\u00b7) is aware of X by itself, i.e., p\u03b8(Y |\u00b7) becomes p\u03b8(Y |X, Z), it could be\nlearned as p\u03b8(Y |X) without hurting the reconstruction loss Jrec, but the KL term in Eq. (3) can be min-\nimized by \ufb01tting the posterior to its prior. This degrades a variational Seq2Seq model to a deterministic\none.\nThe phenomenon can be best shown with a bypassing connection between the encoder and decoder\nfor hidden state initialization. Some previous studies using VEDs set the decoder\u2019s initial state to be\nthe encoder\u2019s \ufb01nal state (Cao and Clark, 2017), shown in Figure 2c. We conducted a pilot study with a\nSeq2Seq VAE with a subset (\u223c80k samples) of the massive dataset provided by Bowman et al. (2015),\nand show generated sentences and entropy in Table 1. We see that the variational Seq2Seq can only\ngenerate very similar sentences with such bypassing connections (Table 1b), as opposed to generating\ndiversi\ufb01ed samples from the latent space (Table 1a). We also computed the entropy for 10 randomly\nsampled outputs for a given input sentence. Quantitatively, the entropy decreases by 0.5 on average for\n1k unseen input sentences. This shows a signi\ufb01cant difference because entropy is a logarithmic metric.\nOur analysis sheds light on the design philosophy of neural architectures in VAE or VED.\nSince attention largely improves model performance for deterministic Seq2Seq models, it is tempting\nto include attention in the variational Seq2Seq as well. However, our pilot experiment raises the doubt\nif a traditional attention mechanism, which is deterministic, may bypass the latent space in VED, as\nillustrated by a graphical model in Figure 1c. Also, evidence in Zheng et al. (2018) shows the attention\nmechanism is so powerful that removing other connections between the encoder and decoder has little\neffect on BLEU scores in machine translation. Therefore, a VED with deterministic attention might learn\nreconstruction mostly from attention, whereas the posterior of the latent space can \ufb01t to its prior in order\nto minimize the KL term.\nTo alleviate this problem, we propose a variational attention mechanism for variational Seq2Seq mod-\nels, as is described in detail in the next section.\n3\nThe Proposed Variational Attention\nLet us consider the decoding process of an RNN. At each timestep j, it adjusts its hidden state h(tar)\nj\nwith\nan input of a word embedding yj\u22121 (typically the groundtruth during training and the prediction from\nthe previous step during testing). This is given by h(tar)\nj\n= RNN\u03b8(h(tar)\nj\u22121, yj\u22121). In our experiments, we\nuse long short-term memory units (Hochreiter and Schmidhuber, 1997) as RNN\u2019s transition. Enhanced\n"], "summary": "The cited paper by Bahuleyan et al. (2018) addresses the \"bypassing phenomenon\" in Variational Encoder-Decoders (VEDs) where a deterministic attention mechanism can render the variational latent space ineffective, leading to less diverse generated sentences. While your work focuses on discrete latent variables and attention for language generation to combat information under-representation and posterior collapse, Bahuleyan et al. propose a variational attention mechanism where the attention vector itself is modeled as random variables (Gaussian distributed). This is relevant to your work as it explores a different approach to enhancing the latent space and improving diversity in VAE-based language models, specifically by making the attention mechanism variational rather than the primary latent space. Their findings suggest that a variational attention can alleviate the bypassing problem and increase diversity, which aligns with your goal of improving language generation quality and diversity.", "citation": "Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Guodong Zhou, Shuming Shi (2019). A Discrete CVAE for Response Generation on Short-Text Conversation. arXiv:1911.09845. https://arxiv.org/abs/1911.09845"}, {"paper_id": 17, "text": ["prior distribution based on the piecewise constant\ndistribution.\nWe derive an analytical, tractable\nform that is applicable to the variational autoen-\ncoder framework and propose a differentiable\nparametrization for it. We then evaluate the ef-\nfectiveness of the distribution when utilized both\nas a prior and as approximate posterior across\nvariational architectures in two natural language\nprocessing tasks: document modeling and natu-\nral language generation for dialogue.\nWe show\nthat the piecewise constant distribution is able to\ncapture elements of a target distribution that can-\nnot be captured by simpler priors \u2014 such as the\nuni-modal Gaussian.\nWe demonstrate state-of-\nthe-art results on three document modeling tasks,\nand show improvements on a dialogue natural lan-\nguage generation. Finally, we illustrate qualita-\ntively how the piecewise constant distribution rep-\nresents multi-modal latent structure in the data.\n2\nRelated Work\nThe idea of using an arti\ufb01cial neural network to\napproximate an inference model dates back to the\nearly work of Hinton and colleagues (Hinton and\nZemel, 1994; Hinton et al., 1995; Dayan and Hin-\nton, 1996).\nResearchers later proposed Markov\nchain Monte Carlo methods (MCMC) (Neal,\n1992), which do not scale well and mix slowly,\nas well as variational approaches which require\na tractable, factored distribution to approximate\nthe true posterior distribution (Jordan et al., 1999).\nOthers have since proposed using feed-forward in-\nference models to initialize the mean-\ufb01eld infer-\nence algorithm for training Boltzmann architec-\ntures (Salakhutdinov and Larochelle, 2010; Oror-\nbia II et al., 2015).\nRecently, the variational\nautoencoder framework (VAE) was proposed by\nKingma and Welling (2014) and Rezende et al.\n(2014), closely related to the method proposed by\nMnih and Gregor (2014). This framework allows\nthe joint training of an inference network and a di-\nrected generative model, maximizing a variational\nlower-bound on the data log-likelihood and facil-\nitating exact sampling of the variational posterior.\nOur work extends this framework.\nWith respect to document modeling, neural ar-\nchitectures have been shown to outperform well-\nestablished topic models such as Latent Dirich-\nlet Allocation (LDA) (Hofmann, 1999; Blei et al.,\n2003).\nResearchers have successfully proposed\nseveral models involving discrete latent vari-\nables (Salakhutdinov and Hinton, 2009; Hinton\nand Salakhutdinov, 2009; Srivastava et al., 2013;\nLarochelle and Lauly, 2012; Uria et al., 2014;\nLauly et al., 2016; Bornschein and Bengio, 2015;\nMnih and Gregor, 2014). The success of such dis-\ncrete latent variable models \u2014 which are able to\npartition probability mass into separate regions \u2014\nserves as one of our main motivations for investi-\ngating models with more \ufb02exible continuous latent\nvariables for document modeling. More recently,\nMiao et al. (2016) proposed to use continuous la-\ntent variables for document modeling.\nResearchers have also investigated latent vari-\nable models for dialogue modeling and dialogue\nnatural language generation (Bangalore et al.,\n2008; Crook et al., 2009; Zhai and Williams,\n2014).\nThe success of discrete latent variable\nmodels in this task also motivates our investi-\ngation of more \ufb02exible continuous latent vari-\nables.\nClosely related to our proposed ap-\nproach is the Variational Hierarchical Recur-\nrent Encoder-Decoder (VHRED, described below)\n(Serban et al., 2017b), a neural architecture with\nlatent multivariate Gaussian variables. In parallel\nwith our work, Zhao et al. (2017) has also pro-\nposed a latent variable model for dialogue mod-\neling with the speci\ufb01c goal of generating diverse\nnatural language responses.\nResearchers have explored more \ufb02exible dis-\ntributions for the latent variables in VAEs, such\nas autoregressive distributions, hierarchical prob-\nabilistic models and approximations based on\nMCMC sampling (Rezende et al., 2014; Rezende\nand Mohamed, 2015; Kingma et al., 2016; Ran-\nganath et al., 2016; Maal\u00f8e et al., 2016; Salimans\net al., 2015; Burda et al., 2016; Chen et al., 2017;\nRuiz et al., 2016). These are all complimentary\nto our approach; it is possible to combine them\nwith the piecewise constant latent variables.\nIn\nparallel to our work, multiple research groups have\nalso proposed VAEs with discrete latent variables\n(Maddison et al., 2017; Jang et al., 2017; Rolfe,\n2017; Johnson et al., 2016). This is a promising\nline of research, however these approaches often\nrequire approximations which may be inaccurate\nwhen applied to larger scale tasks, such as docu-\nment modeling or natural language generation. Fi-\nnally, discrete latent variables may be inappropri-\nate for certain natural language processing tasks.\n", "tablished models on this task, such as the LSTM\nRNN language model (Serban et al., 2017a). The\nHRED model\u2019s encoder RNN uses a bidirectional\nGRU RNN encoder, where the forward and back-\nward RNNs each have 1000 hidden units.\nThe\ncontext RNN is a GRU encoder with 1000 hidden\nunits, and the decoder RNN is an LSTM decoder\nwith 2000 hidden units.2 The encoder and con-\ntext RNNs both use layer normalization (Ba et al.,\n2016).3\nWe also experiment with an additional\nrecti\ufb01ed linear layer applied on the inputs to the\ndecoder RNN. As with other hyper-parameters,\nwe choose whether to include this additional layer\nbased on the validation set performance. HRED,\nas well as all other models, use a word embedding\ndimensionality of size 400.\nG-HRED: We compare to G-VHRED, which\nis VHRED with Gaussian latent variables (Serban\net al., 2017b). G-VHRED uses the same hyper-\nparameters for the encoder, context and decoder\nRNNs as the HRED model. The model has 100\nGaussian latent variables per utterance.\nP-HRED:\nThe \ufb01rst model we propose is P-\nVHRED, which is VHRED model with piecewise\nconstant latent variables. We use n = 3 number\nof pieces for each latent variable. P-VHRED also\nuses the same hyper parameters for the encoder,\ncontext and decoder RNNs as the HRED model.\nSimilar to G-VHRED, P-VHRED has 100 piece-\nwise constant latent variables per utterance.\nH-HRED: The second model we propose is H-\nVHRED, which has 100 piecewise constant (with\nn = 3 pieces per variable) and 100 Gaussian la-\ntent variables per utterance. H-VHRED also uses\nthe same hyper-parameters for the encoder, con-\ntext and decoder RNNs as HRED.\nResults:\nThe results are given in Table 3.\nAll latent variable models outperform HRED w.r.t.\nboth activities and entities. This strongly suggests\nthat the high-level concepts represented by the\nlatent variables help generate meaningful, goal-\ndirected responses.\nFurthermore, each type of\nlatent variable appears to help with a different\naspects of the generation task.\nG-VHRED per-\nforms best w.r.t. activities (e.g. download, install\nand so on), which occur frequently in the dataset.\n2Since training lasted between 1-3 weeks for each model,\nwe had to \ufb01x the number of hidden units during preliminary\nexperiments on the training and validation datasets.\n3We did not apply layer normalization to the decoder\nRNN, because several of our colleagues have found that this\nmay hurt the performance of generative language models.\nThis suggests that the Gaussian latent variables\nlearn useful latent representations for frequent ac-\ntions.\nOn the other hand, H-VHRED performs\nbest w.r.t. entities (e.g. Firefox, GNOME), which\nare often much rarer and mutually exclusive in\nthe dataset. This suggests that the combination of\nGaussian and piecewise latent variables help learn\nuseful representations for entities, which could\nnot be learned by Gaussian latent variables alone.\nWe further conducted a qualitative analysis of the\nmodel responses, which supports these conclu-\nsions. See Appendix G.4\n7\nConclusions\nIn this paper, we have sought to learn rich and\n\ufb02exible multi-modal representations of latent vari-\nables for complex natural language processing\ntasks. We have proposed the piecewise constant\ndistribution for the variational autoencoder frame-\nwork. We have derived closed-form expressions\nfor the necessary quantities required for in the au-\ntoencoder framework, and proposed an ef\ufb01cient,\ndifferentiable implementation of it. We have in-\ncorporated the proposed piecewise constant dis-\ntribution into two model classes \u2014 NVDM and\nVHRED \u2014 and evaluated the proposed models on\ndocument modeling and dialogue modeling tasks.\nWe have achieved state-of-the-art results on three\ndocument modeling tasks, and have demonstrated\nsubstantial improvements on a dialogue modeling\ntask.\nOverall, the results highlight the bene\ufb01ts\nof incorporating the \ufb02exible, multi-modal piece-\nwise constant distribution into variational autoen-\ncoders. Future work should explore other natural\nlanguage processing tasks, where the data is likely\nto arise from complex, multi-modal latent factors.\nAcknowledgments\nThe authors acknowledge NSERC, Canada Re-\nsearch Chairs, CIFAR, IBM Research, Nuance\nFoundation and Microsoft Maluuba for fund-\ning.\nAlexander G. Ororbia II was funded\nby a NACME-Sloan scholarship.\nThe authors\nthank Hugo Larochelle for sharing the News-\nGroup 20 dataset.\nThe authors thank Lau-\nrent Charlin, Sungjin Ahn, and Ryan Lowe for\nconstructive feedback.\nThis research was en-\nabled in part by support provided by Calcul\nQubec (www.calculquebec.ca) and Com-\npute Canada (www.computecanada.ca).\n4Results on a Twitter dataset are given in the appendix.\n", "Piecewise Latent Variables for Neural Variational Text Processing\nIulian V. Serban1\u2217and Alexander G. Ororbia II2\u2217and Joelle Pineau3 and Aaron Courville1\n1 Department of Computer Science and Operations Research, Universite de Montreal\n2College of Information Sciences & Technology, Penn State University\n3School of Computer Science, McGill University\niulian [DOT] vlad [DOT] serban [AT] umontreal [DOT] ca\nago109 [AT] psu [DOT] edu\njpineau [AT] cs [DOT] mcgill [DOT] ca\naaron [DOT] courville [AT] umontreal [DOT] ca\nAbstract\nAdvances in neural variational inference\nhave facilitated the learning of power-\nful directed graphical models with con-\ntinuous latent variables, such as varia-\ntional autoencoders.\nThe hope is that\nsuch models will learn to represent rich,\nmulti-modal latent factors in real-world\ndata, such as natural language text. How-\never, current models often assume simplis-\ntic priors on the latent variables \u2014 such\nas the uni-modal Gaussian distribution \u2014\nwhich are incapable of representing com-\nplex latent factors ef\ufb01ciently.\nTo over-\ncome this restriction, we propose the sim-\nple, but highly \ufb02exible, piecewise constant\ndistribution. This distribution has the ca-\npacity to represent an exponential num-\nber of modes of a latent target distribution,\nwhile remaining mathematically tractable.\nOur results demonstrate that incorporating\nthis new latent distribution into different\nmodels yields substantial improvements in\nnatural language processing tasks such as\ndocument modeling and natural language\ngeneration for dialogue.\n1\nIntroduction\nThe development of the variational autoencoder\nframework (Kingma and Welling, 2014; Rezende\net al., 2014) has paved the way for learning large-\nscale, directed latent variable models. This has led\nto signi\ufb01cant progress in a diverse set of machine\nlearning applications, ranging from computer vi-\nsion (Gregor et al., 2015; Larsen et al., 2016) to\nnatural language processing tasks (Mnih and Gre-\ngor, 2014; Miao et al., 2016; Bowman et al., 2015;\n\u2217The \ufb01rst two authors contributed equally.\nSerban et al., 2017b). It is hoped that this frame-\nwork will enable the learning of generative pro-\ncesses of real-world data \u2014 including text, audio\nand images \u2014 by disentangling and representing\nthe underlying latent factors in the data.\nHow-\never, latent factors in real-world data are often\nhighly complex. For example, topics in newswire\ntext and responses in conversational dialogue of-\nten posses latent factors that follow non-linear\n(non-smooth), multi-modal distributions (i.e. dis-\ntributions with multiple local maxima).\nNevertheless, the majority of current models as-\nsume a simple prior in the form of a multivariate\nGaussian distribution in order to maintain mathe-\nmatical and computational tractability. This is of-\nten a highly restrictive and unrealistic assumption\nto impose on the structure of the latent variables.\nFirst, it imposes a strong uni-modal structure on\nthe latent variable space; latent variable samples\nfrom the generating model (prior distribution) all\ncluster around a single mean. Second, it forces\nthe latent variables to follow a perfectly symmet-\nric distribution with constant kurtosis; this makes\nit dif\ufb01cult to represent asymmetric or rarely occur-\nring factors. Such constraints on the latent vari-\nables increase pressure on the down-stream gen-\nerative model, which in turn is forced to carefully\npartition the probability mass for each latent factor\nthroughout its intermediate layers. For complex,\nmulti-modal distributions \u2014 such as the distribu-\ntion over topics in a text corpus, or natural lan-\nguage responses in a dialogue system \u2014 the uni-\nmodal Gaussian prior inhibits the model\u2019s ability\nto extract and represent important latent structure\nin the data. In order to learn more expressive latent\nvariable models, we therefore need more \ufb02exible,\nyet tractable, priors.\nIn this paper, we introduce a simple, \ufb02exible\narXiv:1612.00377v4  [cs.CL]  23 Sep 2017\n", "3\nNeural Variational Models\nWe start by introducing the neural variational\nlearning framework. We focus on modeling dis-\ncrete output variables (e.g. words) in the context\nof natural language processing applications. How-\never, the framework can easily be adapted to han-\ndle continuous output variables.\n3.1\nNeural Variational Learning\nLet w1, . . . , wN be a sequence of N tokens\n(words) conditioned on a continuous latent vari-\nable z. Further, let c be an additional observed\nvariable which conditions both z and w1, . . . , wN.\nThen, the distribution over words is:\nP\u03b8(w1, . . . , wN|c) =\nZ\nN\nY\nn=1\nP\u03b8(wn|w<n, z, c)P\u03b8(z|c)dz,\nwhere \u03b8 are the model parameters. The model \ufb01rst\ngenerates the higher-level, continuous latent vari-\nable z conditioned on c. Given z and c, it then gen-\nerates the word sequence w1, . . . , wN. For unsu-\npervised modeling of documents, the c is excluded\nand the words are assumed to be independent of\neach other, when conditioned on z:\nP\u03b8(w1, . . . , wN) =\nZ\nN\nY\nn=1\nP\u03b8(wn|z)P\u03b8(z)dz.\nModel parameters can be learned using the varia-\ntional lower-bound (Kingma and Welling, 2014):\nlog P\u03b8(w1, . . . , wN|c)\n\u2265\nEz\u223cQ\u03c8(z|w1,...,wN,c)[log P\u03b8(wn|w<n, z, c)]\n\u2212KL [Q\u03c8(z|w1, . . . , wN, c)||P\u03b8(z|c)] ,\n(1)\nwhere we note that Q\u03c8(z|w1, . . . , wN, c) is the\napproximation to the intractable, true posterior\nP\u03b8(z|w1, . . . , wN, c).\nQ is called the encoder,\nor sometimes the recognition model or inference\nmodel, and it is parametrized by \u03c8. The distri-\nbution P\u03b8(z|c) is the prior model for z, where\nthe only available information is c.\nThe VAE\nframework further employs the re-parametrization\ntrick, which allows one to move the derivative of\nthe lower-bound inside the expectation.\nTo ac-\ncomplish this, z is parametrized as a transforma-\ntion of a \ufb01xed, parameter-free random distribu-\ntion z = f\u03b8(\u03f5), where \u03f5 is drawn from a ran-\ndom distribution. Here, f is a transformation of\n\u03f5, parametrized by \u03b8, such that f\u03b8(\u03f5) \u223cP\u03b8(z|c).\nFor example, \u03f5 might be drawn from a standard\nGaussian distribution and f might be de\ufb01ned as\nf\u03b8(\u03f5) = \u00b5 + \u03c3\u03f5, where \u00b5 and \u03c3 are in the param-\neter set \u03b8. In this case, z is able to represent any\nGaussian with mean \u00b5 and variance \u03c32.\nModel parameters are learned by maximizing\nthe variational lower-bound in eq. (1) using gra-\ndient descent, where the expectation is computed\nusing samples from the approximate posterior.\nThe majority of work on VAEs propose to\nparametrize z as multivariate Gaussian distrib-\ntions. However, this unrealistic assumption may\ncritically hurt the expressiveness of the latent vari-\nable model. See Appendix A for a detailed dis-\ncussion. This motivates the proposed piecewise\nconstant latent variable distribution.\n3.2\nPiecewise Constant Distribution\nWe propose to learn latent variables by parametriz-\ning z using a piecewise constant probability den-\nsity function (PDF). This should allow z to rep-\nresent complex aspects of the data distribution in\nlatent variable space, such as non-smooth regions\nof probability mass and multiple modes.\nLet n \u2208N be the number of piecewise constant\ncomponents. We assume z is drawn from PDF:\nP(z) = 1\nK\nn\nX\ni=1\n1 i \u22121\nn\n\u2264z\u2264i\nn\n!ai,\n(2)\nwhere 1(x) is the indicator function, which is one\nwhen x is true and otherwise zero. The distribu-\ntion parameters are ai > 0, for i = 1, . . . , n. The\nnormalization constant is:\nK =\nn\nX\ni=1\nKi, where K0 = 0, Ki = ai\nn , for i = 1, . . . , n.\nIt is straightforward to show that a piecewise con-\nstant distribution with more than n > 2 pieces\nis capable of representing a bi-modal distribution.\nWhen n > 2, a vector z of piecewise constant\nvariables can represent a probability density with\n2|z| modes. Figure 1 illustrates how these variables\nhelp model complex, multi-modal distributions.\nIn order to compute the variational bound, we\nneed to draw samples from the piecewise constant\ndistribution using its inverse cumulative distribu-\ntion function (CDF). Further, we need to compute\nthe KL divergence between the prior and posterior.\nThe inverse CDF and KL divergence quantities are\nboth derived in Appendix B. During training we\nmust compute derivatives of the variational bound\nin eq. (1). These expressions involve derivatives\n"], "summary": "The cited paper, \"Piecewise Latent Variables for Neural Variational Text Processing,\" by Serban et al. (2017), is highly relevant to your work on discrete variational attention models. While your paper focuses on discrete latent spaces for attention mechanisms to address posterior collapse and information under-representation, Serban et al. investigate the use of flexible, multi-modal piecewise constant distributions for continuous latent variables in VAEs, specifically to overcome the limitations of simplistic Gaussian priors and better capture complex, multi-modal latent factors in natural language. Their work, particularly their exploration of discrete latent variables as a promising line of research, directly informs your choice of a discrete nature in languages and discrete latent space, offering a contrasting yet complementary perspective on enhancing VAEs for language generation. They also highlight the challenges of applying discrete latent variable approaches to larger-scale tasks, which your work aims to address.", "citation": "Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie (2022). Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation. arXiv:2210.12409. https://arxiv.org/abs/2210.12409"}, {"paper_id": 25, "text": ["ICML 2024 Next Generation of Sequence Modeling Architectures Workshop\nParallelizing Autoregressive Generation with\nVariational State Space Models\nGaspard Lambrechts*\nGASPARD.LAMBRECHTS@ULIEGE.BE\nYann Claes*\nY.CLAES@ULIEGE.BE\nPierre Geurts\nP.GEURTS@ULIEGE.BE\nDamien Ernst\nDERNST@ULIEGE.BE\nMontefiore Institute, University of Li`ege\nAbstract\nAttention-based models such as Transformers and recurrent models like state space models (SSMs)\nhave emerged as successful methods for autoregressive sequence modeling. Although both enable\nparallel training, none enable parallel generation due to their autoregressiveness. We propose the\nvariational SSM (VSSM), a variational autoencoder (VAE) where both the encoder and decoder are\nSSMs. Since sampling the latent variables and decoding them with the SSM can be parallelized,\nboth training and generation can be conducted in parallel. Moreover, the decoder recurrence allows\ngeneration to be resumed without reprocessing the whole sequence. Finally, we propose the autore-\ngressive VSSM that can be conditioned on a partial realization of the sequence, as is common in\nlanguage generation tasks. Interestingly, the autoregressive VSSM still enables parallel generation.\nWe highlight on toy problems (MNIST, CIFAR) the empirical gains in speed-up and show that it\ncompetes with traditional models in terms of generation quality (Transformer, Mamba SSM).\nKeywords: Parallel, Autoregressive, Generation, VAE, SSM, VSSM\n1. Introduction\nSequence modeling tasks, namely time-series forecasting and text generation, have gained in popu-\nlarity and various types of architectures were designed to tackle such problems. Transformers were\nproven effective [17, 19], yet they nonetheless reprocess the complete sequence at each timestep,\nmaking generation less efficient. Recurrent neural networks (RNNs) [3, 8] update a hidden state\nbased on new inputs at each timestep, enabling efficient generation. SSMs [9\u201311, 18], a recently in-\ntroduced class of RNNs, enable parallel training thanks to their linear recurrence. Alternatively, sev-\neral works adapt VAEs for sequential modeling. Some architectures integrate Transformers [13, 14]\nand enable parallel training, although little work [5] proposes models that can be conditioned on\npartial realizations (e.g., prompts). Conversely, variational RNNs (VRNNs) [4] loose parallelizabil-\nity by making the model both autoregressive and recurrent, allowing it to be conditioned on partial\nrealizations and to resume generation. However, all introduced autoregressive models perform gen-\neration sequentially, as they are explicitly conditioned on previously generated data.\nTherefore, we propose the VSSM, a VAE whose encoder and decoder are SSMs. Thanks to key\narchitectural choices, both training and inference can be performed in parallel and linear time with\n*Equal contributions.\n\u00a9 G. Lambrechts, Y. Claes, P. Geurts & D. Ernst.\narXiv:2407.08415v1  [cs.LG]  11 Jul 2024\n", "PARALLELIZING AUTOREGRESSIVE GENERATION WITH VARIATIONAL STATE SPACE MODELS\nAcknowledgments\nGaspard Lambrechts gratefully acknowledges the financial support of the Wallonia-Brussels Feder-\nation for his FRIA grant. Yann Claes gratefully acknowledges the financial support of the Walloon\nRegion under Grant No. 2010235 (ARIAC by Digital Wallonia 4.AI). Computational resources\nhave been provided by the Consortium des \u00b4Equipements de Calcul Intensif (C\u00b4ECI), funded by\nthe National Fund for Scientific Research (F.R.S.-FNRS) under Grant No. 2502011 and by the Wal-\nloon Region, including the Tier-1 supercomputer of the Wallonia-Brussels Federation, infrastructure\nfunded by the Walloon Region under Grant No. 1117545.\nReferences\n[1] Guy E Blelloch. Prefix Sums and Their Applications. School of Computer Science, Carnegie\nMellon University Pittsburgh, PA, USA, 1990.\n[2] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders.\narXiv preprint arXiv:1509.00519, 2015.\n[3] Kyunghyun Cho, Bart van Merri\u00a8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder\u2013\ndecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empiri-\ncal Methods in Natural Language Processing, 2014.\n[4] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua\nBengio. A Recurrent Latent Variable Model for Sequential Data. In Advances in Neural\nInformation Processing Systems, 2015.\n[5] Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, and Changyou Chen. Transformer-\nBased Conditional Variational Autoencoder for Controllable Story Generation. arXiv preprint\narXiv:2101.00828, 2021.\n[6] Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, and Xavier\nAlameda-Pineda. Dynamical Variational Autoencoders: A Comprehensive Review. Foun-\ndations and Trends in Machine Learning, 2021.\n[7] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. DiffuSeq: Se-\nquence to Sequence Text Generation with Diffusion Models. In International Conference on\nLearning Representations, 2023.\n[8] Alex Graves.\nGenerating Sequences with Recurrent Neural Networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[9] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\narXiv preprint arXiv:2312.00752, 2023.\n[10] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00b4e. On the Parameterization and Ini-\ntialization of Diagonal State Space Models. In Advances in Neural Information Processing\nSystems, 2022.\n5\n", "PARALLELIZING AUTOREGRESSIVE GENERATION WITH VARIATIONAL STATE SPACE MODELS\nrespect to the sequence length, while still allowing generation to be to resumed without reprocessing\nthe entire sequence. In contrast, a VAE with Transformer encoder and decoder, which we call Trans-\nformer VAE (TVAE), would preserve parallel training and generation, but would not be resumable.\nWe then propose the autoregressive VSSM, that can be conditioned on partial realizations of the\nsequence and still generates in parallel. The VSSM combines all advantages of previous models,\nas observed in Figure 1a, while producing results comparable to Transformers and SSMs on simple\ntasks (MNIST, CIFAR). We highlight a recent work [20] that proposes a similar architecture, yet\ntheir prior and generative models are explicitly autoregressive and do not exploit the parallelizabil-\nity of SSMs. Moreover, they only consider generation from sampled latents, while we also propose\nan approach to condition the model on partial realizations. We do not consider diffusion models for\nsequences (e.g., [7]), but note that they would not allow recurrent (i.e., resuming) generation.\nModel\nTraining\n//\nSampling\n//\nPrompt Resume\nTransformer\nO(T 2)\n\u2713\nO(T 2)\n\u2717\n\u2713\n\u2717\nRNN\nO(T)\n\u2717\nO(T)\n\u2717\n\u2713\n\u2713\nSSM\nO(T)\n\u2713\nO(T)\n\u2717\n\u2713\n\u2713\nTVAE\nO(T 2)\n\u2713\nO(T 2)\n\u2713\n\u2717/\u2713\n\u2717\nVRNN\nO(T)\n\u2717\nO(T)\n\u2717\n\u2713\n\u2713\nVSSM\nO(T)\n\u2713\nO(T)\n\u2713\n\u2713\n\u2713\n(a) Time complexities and parallelizability at training\nand sampling, and generation properties.\nx1:C\n\u2205C+1:W1\n\u2205W1+1:W2\nPartial\nhe\nC\nPartial\nhe\nW1\nPartial\nz1:C\nzC+1:W1\nzW1+1:W2\nDecoder\nhd\nC\nDecoder\nhd\nW1\nDecoder\n\u02c6x1:C\n\u02c6xC+1:W1\n\u02c6xW1+1:W2\n(b) Parallel and recurrent sampling algorithm,\ngiven a contextual prompt x1:C.\nFigure 1: Sequence models properties and VSSM sampling algorithm.\n2. Background\n2.1. Variational Autoencoders for Time Series\nWe consider dynamical VAEs [6], that model sequential data x1:T of length T through T latent\nvariables z1:T . Given a target space X, they define the joint distribution p\u03d5(x1:T , z1:T ) with,\n\u2022 A latent space Z,\n\u2022 A prior distribution p\u03d5(z1:T ) = QT\nt=1 p\u03d5(zt|z1:t\u22121),\n\u2022 A generative distribution p\u03d5(x1:T |z1:T ) = QT\nt=1 p\u03d5(xt|x1:t\u22121, z1:T ),\nwhere \u03d5 denotes the parameters of these probability distributions. Unfortunately, the likelihood\nof the data p\u03d5(x1:T ) = Ep\u03d5(z1:T )p\u03d5(x1:T |z1:T ) under this model cannot be evaluated in practice.\nNevertheless, we can show that the log-likelihood is lower bounded by the evidence lower bound\n(ELBO), for any conditional probability distribution q(z1:T |x1:T ),\nlog p\u03d5(x1:T ) \u2265\nE\nq(z1:T |x1:T )\nlog p\u03d5(x1:T |z1:T ) \u2212KL(q(z1:T |x1:T ) \u2225p\u03d5(z1:T )) = ELBO\u03d5(x1:T ) (1)\nMoreover, the ELBO becomes tight when q(z1:T |x1:T ) corresponds to the true posterior distribution\np\u03d5(z1:T |x1:T ). Thus, the generative model p\u03d5 is usually jointly optimized with,\n\u2022 A posterior distribution q\u03c8(z1:T |x1:T ) = QT\nt=1 q\u03c8(zt|z1:t\u22121, x1:T ),\nwhere \u03c8 denotes the parameters of this distribution. These four components compose the dynamical\nVAE. More details are provided in Section A.\n2\n", "PARALLELIZING AUTOREGRESSIVE GENERATION WITH VARIATIONAL STATE SPACE MODELS\n\u2022 A partial posterior distribution D(zt|\u00afvt), where probabilities \u00afvt = fpar\n\u03c9 (\u00afx1:T ) are the output of\na stacked SSM, such that q\u03c9(z1:T |x1:C) = QT\nt=1 q\u03c9(zt|x1:min(C,t)) = QT\nt=1 D(zt|fpar\n\u03c9 (\u00afx1:T )).\nNote that the partial posterior distribution q\u03c9 should ideally correspond to the prior when \u00afx1:T =\n(\u2205, . . . , \u2205), and it will be used in practice for unconditional generation.\nThe autoregressive VSSM generates in parallel, possibly conditioned on a partial realization, and\ncan resume generation, as illustrated on Figure 1 (see detailed algorithms comparison in Section B).\n4. Experiments\nIn the following, we compare Transformer, SSM and VSSM on two toy sequence modeling tasks:\nMNIST, for which we consider 28-dimensional sequences of length 28, and CIFAR, for which\nwe consider (32 \u00d7 3)-dimensional sequences of length 32. Transformer and SSM both output the\nmean of a Gaussian distribution of fixed variance. For more details about model architectures,\nsee Section C.1. We report samples, generation times and likelihoods in Figure 2, estimated by\nimportance-sampling for the VSSM, see Section C.2. We report additional results in Section C.3.\n0\n50\n100\n150\n200\nEpoch\n10\n20\n30\nLog Likelihood\nSSM\nTransformer\nVSSM\nPartial\n(a) MNIST valid log-likelihood.\nFull\nPartial\nLikelihood\n0\n10\n20\n30\nLog Likelihood\nFull\nPartial\nTime\n0\n25\n50\n75\n100\n125\nTime per token [\u00b5s]\n(b) MNIST test statistics.\nTransformer\nSSM\nVSSM\nTransformer\nSSM\nVSSM\nPrompt\n(c) MNIST samples.\n0\n50\n100\n150\n200\nEpoch\n60\n70\n80\n90\n100\n110\nLog Likelihood\nSSM\nTransformer\nVSSM\nPartial\n(d) CIFAR valid log-likelihood.\nFull\nPartial\nLikelihood\n0\n20\n40\n60\n80\n100\nLog Likelihood\nFull\nPartial\nTime\n0\n50\n100\n150\nTime per token [\u00b5s]\n(e) CIFAR test statistics.\nTransformer\nSSM\nVSSM\nTransformer\nSSM\nVSSM\nPrompt\n(f) CIFAR samples.\nFigure 2: We report results over 5 runs of each model. Confidence intervals correspond to the minimum\nand maximum values observed. In 2a, 2d, we plot the median full and partial log-likelihood\nlog p\u03d5(x1:T ) and log p\u03d5(xC+1:T | x1:C) on the validation set throughout training. In 2b, 2e, we\nreport the average full and partial log-likelihood on the test set, along with mean execution times\nat generation, in both cases. In 2c, 2f, we report random qualitative examples for all models, for\nunconditioned sampling (first three rows) and conditioned on partial realizations (last three rows).\n5. Conclusion\nWe introduce the VSSM, a dynamical VAE using SSMs as encoder and decoder. Compared to other\narchitectures, our model is the first one that can generate in parallel while being recurrent, which\nallows generation to be resumed. Although tested on simple tasks, we show that it produces decent\nresults in only a fraction of the time. The advantages of this architecture motivate further work to\nscale and improve performance on more challenging tasks such as language generation.\n4\n"], "summary": "The cited paper introduces Variational State Space Models (VSSMs) for parallel autoregressive generation, addressing the sequential nature of generation in existing models. While your work focuses on discrete variational attention models to tackle posterior collapse and information under-representation in language generation, the cited paper's VSSM also uses a VAE framework and aims for efficient generation, albeit through a different architectural approach (SSMs) and with a primary focus on parallelization rather than posterior collapse. However, the cited paper does mention that the advantages of their architecture motivate further work to scale and improve performance on challenging tasks such as language generation, suggesting a potential future intersection with your research area.", "citation": "Arya D. McCarthy, Xian Li, Jiatao Gu, Ning Dong (2019). Improved Variational Neural Machine Translation by Promoting Mutual Information. arXiv:1909.09237. https://arxiv.org/abs/1909.09237"}, {"paper_id": 49, "text": ["5\nConclusions\nIn this paper, we introduced AutoGen: an novel modelling approach to improving the descriptiveness\nof latent variables in VAEs by adding the log likelihood of m high-\ufb01delity reconstructions to the\nobjective function. This approach is theoretically principled in that it retains a bound on a meaningful\nobjective, and computationally amounts to a simple factor of (1 + m) in front of the reconstruction\nterm in the standard ELBO. We \ufb01nd that the most natural version of AutoGen (with m = 1) provides\nsigni\ufb01cantly better reconstructions than the Standard VAE approach to language modelling, and only\nminimally deprecates generation from the prior.\n6\nAcknowledgments\nThis work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and by\nAWS Cloud Credits for Research.\nReferences\n[1] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., and Bengio, S. Generating\nSentences from a Continuous Space. In Conference on Computational Natural Language\nLearning, 2016.\n[2] Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I.,\nand Abbeel, P. Variational Lossy Autoencoder. In International Conference on Learning\nRepresentations, 2017.\n[3] Dieng, A. B., Wang, C., Gao, J., and Paisley, J. TopicRNN: A Recurrent Neural Network with\nLong-Range Semantic Dependency. In International Conference on Learning Representations,\n2017.\n[4] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., and Courville,\nA. PixelVAE: A Latent Variable Model for Natural Images. In International Conference on\nLearning Representations, 2017.\n[5] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and\nLerchner, A.\nbeta-VAE: Learning Basic Visual Concepts with a Constrained Variational\nFramework. In International Conference on Learning Representations, 2017.\n[6] Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In International Conference\non Learning Representations, 2014.\n[7] Kiros, R., Salakhutdinov, R., and Zemel, R. Multimodal Neural Language Models. In Interna-\ntional Conference on Machine Learning, 2014.\n[8] Mansimov, E., Parisotto, E., Ba, J. L., and Salakhutdinov, R. Generating Images from Captions\nwith Attention. In International Conference on Learning Representations, 2016.\n[9] Pu, Y., Gan, Z., Henao, R., Yuan, X., Li, C., Stevens, A., and Carin, L. Variational Autoencoder\nfor Deep Learning of Images, Labels and Captions.\nIn Advances in Neural Information\nProcessing Systems, 2016.\n[10] Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic Backpropagation and Approximate\nInference in Deep Generative Models. In International Conference on Machine Learning, 2014.\n[11] Semeniuta, S., Severyn, A., and Barth, E. A Hybrid Convolutional Variational Autoencoder for\nText Generation. In Conference on Empirical Methods in Natural Language Processing, 2017.\n[12] Shah, H., Zheng, B., and Barber, D. Generating Sentences Using a Dynamic Canvas. In\nAssociation for the Advancement of Arti\ufb01cial Intelligence, 2017.\n[13] Yang, Z., Hu, Z., Salakhutdinov, R., and Berg-Kirkpatrick, T. Improved Variational Autoen-\ncoders for Text Modeling using Dilated Convolutions. In International Conference on Machine\nLearning, 2017.\n[14] Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S.\nAligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and\nReading Books. In International Conference on Computer Vision, 2015.\n9\n", "z\nx\n(a)\nx\nz\nx\u2032\n(b)\nz\nx\nx\u2032\n(c)\nFigure 1: (a) Standard generative model. (b) Stochastic autoencoder with tied observations. (c)\nEquivalent tied stochastic autoencoder with AutoGen parameterisation.\nIn language modelling, typically both the generative model (\u201cdecoder\u201d) p(x|z), and variational\ndistribution (\u201cencoder\u201d) q(z|x), are parameterised using an LSTM recurrent neural network \u2013 see for\nexample [1]. This generative model \u2013 combined with the highly structured teacher-forcing training\ntechnique \u2013 is so powerful that the maximum ELBO is achieved without making appreciable use of\nthe latent variable in the model. Indeed, if trained using the SGVB algorithm [6], the model learns to\nignore the latent representation and effectively relies solely on the decoder to generate good sentences.\nThis is evidenced by the KL term in the objective function converging to zero, indicating that the\napproximate posterior distribution of the latent variable is trivially converging to its prior distribution.\nThe dependency between what is represented by latent variables, and the capacity of the decoding\ndistribution (i.e., its ability to model the data without using the latent) is a general phenomenon. [13]\nused a lower capacity dilated CNN decoder to generate sentences, preventing the KL term going\nto zero. [4, 5] have discussed this in the context of image processing. A clear explanation of this\nphenomenon in terms of Bit-Back Coding is given in [2].\nA mechanism to avoid the model ignoring the latent entirely, while allowing a high capacity decoder\nis discussed in [1] and uses an alternative training procedure called \u201cKL annealing\u201d \u2013 slowly turning\non the KL term in the ELBO during training. KL annealing allows the model to use its latent variable\nto some degree by forcing the model into a local maximum of its objective function. Modifying\nthe training procedure in this way to preferentially obtain local maxima suggests that the objective\nfunction used in [1] may not be ideal for modelling language in such a way as to create a model that\nleverages its latent variables.\n2\nTraining generative models with AutoGen\nWe propose a new generative latent-variable model inspired by the autoencoder framework. Autoen-\ncoders are trained to reconstruct data through a low-dimensional bottleneck layer, and as a result,\nconstruct a dimensionally-reduced representation from which the data can be reconstructed. By\nencouraging reconstruction in our model, we force the latent variable to represent the input data,\novercoming the issues in [1] of the latent variable being ignored, as discussed in Section 1.\nHowever, using an autoencoder alone does not enable generation from a prior distribution, as in the\ncase of VAEs. To leverage both generation as well as high-\ufb01delity reconstruction from the latent\nvariable, we propose to maximize the likelihoods of both:\nLAutoGen =\nX\nn\nlog p(x = xn)\n|\n{z\n}\ngeneration (VAE)\n+ log p(x\u2032 = xn|x = xn)\n|\n{z\n}\nreconstruction (autoencoder)\n(2)\nwhere x\u2032 represents the reconstruction and the training data is denoted by {xn}. Thus the input\ndata x and the output x\u2032 are tied, much like an autoencoder. Crucially, optimizing LAutoGen does\nnot correspond to optimizing the log likelihood of the data, nor would a lower bound on LAutoGen\ncorrespond to the ELBO used in VAEs, due to the addition of the autoencoder term. Instead, LAutoGen\nrepresents the log likelihood of different model that combines both VAEs and autoencoders.\n2\n", "Table 3: Sentences generated from the prior, z \u223cN(0, I), for the Standard VAE and AutoGen.\nSentences are not \u201ccherry picked\u201d: they are produced in the same way as those in Table 1.\nVAE GENERATION\nVAE GENERATION (ANNEALING)\nAUTOGEN GENERATION (m = 1)\nTHE ONLY THING THAT\nMATTERED.\nSHE JUST LOOKED UP.\nTHEY DON\u2019T SHOW THEMSELVES\nIN MIND , OR SOMETHING TO HIDE.\nHE GAVE HER GO.\nSHE FELT HER LIPS TOGETHER.\nHER EYES WIDEN, FROWNING.\n\u201cGOOD\nMORNING,\u201d\nI\nTHOUGHT.\nMY HANDS BEGAN TO FILL THE VOID\nOF WHAT WAS HAPPENING TO ME.\nTHE LIGHTS LIT UP AROUND ME.\nSHE TURNED TO HER-\nSELF.\nAT FIRST I KNEW HE WOULD HAVE\nTO.\nI JUST FEEL LIKE FUN.\nTable 4: Results from a blind survey testing generation quality. Respondents were asked \u201cdoes\nthis sentence make sense\u201d for a randomized list of sentences evenly sampled from the four models.\nResults are split into two sentence lengths L in order to mitigate the bias of the Standard VAE models\nto generate short sentences.\nMODEL\n% MEANINGFUL (L \u226410)\n% MEANINGFUL (L > 10)\nVAE\n75%\nN/A\nVAE (ANNEALING)\n76%\n32%\nAUTOGEN (m = 1)\n50%\n32%\nAUTOGEN (m = 2)\n29%\n5%\nconclude that the Standard VAE with annealing is better at generating short sentences than AutoGen\n(m = 1). However, both models achieve equal results on generation quality for longer sentences.\nWe also see that AutoGen (m = 2) generates signi\ufb01cantly worse sentences than other models, as\nexpected. All results that differ by more 1 percentage point in the table are statistically signi\ufb01cant\nwith con\ufb01dence greater than 99%.\n3.4\nLatent manifold structure\nFinally, with high-\ufb01delity reconstructions from the latent, one would expect to be able to witness\nthe smoothness of the latent space well. This seems to be the case, as can be seen in Table 5, where\nwe show the reconstructions of a linear interpolation between two encoded sentences for Standard\nVAE with annealing and for AutoGen (m = 1). The AutoGen interpolation seems to be qualitatively\nsmoother, in the sense that, while neighbouring sentences are more similar, there are fewer instances\nof reconstructing the same sentences at subsequent interpolation steps.\nTable 5: Latent variable interpolation. Two sentences, x1 and x2 (\ufb01rst and last sentences in the\ntable) are randomly selected from the test dataset, which provide zi \u223cq(z|xi). Sentences are then\ngenerated along 10 evenly spaced steps from z1 to z2. This interpolation was not \u201ccherry picked\u201d:\nthis was our \ufb01rst generated interpolation; we use the same sentence \ufb01lters as all previous tables.\nVAE (ANNEALING)\nAUTOGEN (m = 1)\n\u201cI\u2019LL DO ANYTHING, BLAKE.\u201d\n\u201cI\u2019LL DO ANYTHING, BLAKE.\u201d\n\u201cI\u2019LL BE RIGHT BACK THEN.\u201d\n\u201cI\u2019LL DO IT, THOUGH.\u201d\n\u201cI\u2019LL TELL ME LIKE THAT.\u201d\n\u201cI\u2019LL SAY IT, SIR.\u201d\nI DONT KNOW WHAT TO SAY.\n\u201cI\u2019VE DONE IT ONCE.\u201d\nI DONT KNOW WHAT TO SAY.\nI DONT THINK THAT WAS IT.\nI DONT THINK ABOUT THAT WAY.\nI WISH SO, THOUGH.\nI\u2019LL BE RIGHT NOW.\nI BET IT\u2019S OKAY.\nI WAS SO MUCH.\nI KNOW HOW DAD.\nI LOOKED AT HIM.\nI LAUGHED AT JACK.\nI LOOKED AT HIM.\nI LOOKED AT SAM.\nI LOOKED AT ADAM.\nI LOOKED AT ADAM.\n7\n", "Improving latent variable descriptiveness with\nAutoGen\nAlex Mansbridge\nUniversity College London\nAlan Turing Institute\nRoberto Fierimonte\nUniversity College London\nIlya Feige\nASI Data Science\nDavid Barber\nUniversity College London\nAlan Turing Institute\nAbstract\nPowerful generative models, particularly in Natural Language Modelling, are com-\nmonly trained by maximizing a variational lower bound on the data log likelihood.\nThese models often suffer from poor use of their latent variable, with ad-hoc an-\nnealing factors used to encourage retention of information in the latent variable.\nWe discuss an alternative and general approach to latent variable modelling, based\non an objective that combines the data log likelihood as well as the likelihood of\na perfect reconstruction through an autoencoder. Tying these together ensures by\ndesign that the latent variable captures information about the observations, whilst\nretaining the ability to generate well. Interestingly, though this approach is a priori\nunrelated to VAEs, the lower bound attained is identical to the standard VAE bound\nbut with the addition of a simple pre-factor; thus, providing a formal interpretation\nof the commonly used, ad-hoc pre-factors in training VAEs.\n1\nIntroduction\nGenerative latent variable models are probabilistic models of observed data x of the form p(x, z) =\np(x|z)p(z), where z is the latent variable. These models are widespread in machine learning and\nstatistics. They are useful both because of their ability to generate new data and because the\nposterior p(z|x) provides insight into the low dimensional representation z corresponding to the high\ndimensional observation x. These latent z values are then often used in downstream tasks, such as\ntopic modelling [3], multi-modal language modeling [7], and image captioning [8, 9].\nLatent variable models, particularly in the form of Variational Autoencoders (VAEs) [6, 10], have\nbeen successfully employed in natural language modelling tasks using varied architectures for both\nthe encoder and the decoder [1, 3, 11, 13, 12]. However, an architecture that is able to effectively\ncapture meaningful semantic information into its latent variables is yet to be discovered.\nA \u201cStandard VAE\u201d approach to language modelling was given by [1], the graphical model for\nwhich is shown in Figure 1(a). This forms a generative model p\u03b8(x|z)p(z) of sentence x, based on\nlatent variable z, where \u03b8 are the parameters of the generative model. Since the integral p(x) =\nR\np\u03b8(x|z)p(z)dz is typically intractable, a common approach is to maximize the Evidence Lower\nBound (ELBO) on the log likelihood,\nlog p(x) \u2265\u27e8log p\u03b8(x|z)\u27e9\u2212DKL [q(z|x)||p(z)]\n(1)\nwhere expectation \u27e8\u00b7\u27e9is with respect to the variational \u201cencoder\u201d q(z|x), and DKL [\u00b7||\u00b7] represents\nthe Kullback-Leibler (KL) divergence. Summing over all datapoints x gives a lower bound on the\nlikelihood of the full dataset.\nPreprint. Work in progress.\narXiv:1806.04480v1  [stat.ML]  12 Jun 2018\n"], "summary": "The cited paper by Mansbridge et al. (2018) introduces AutoGen, an RNN-based variational autoencoder that addresses the issue of latent variable underutilization in VAEs for language generation, a problem similar to the \"information under-representation\" and \"posterior collapse\" you aim to solve. While your work focuses on discrete variational attention and an autoregressive prior, Mansbridge et al. propose adding high-fidelity reconstructions to the objective function to force the latent variable to capture more information. They also discuss \"KL annealing\" as a method to encourage latent variable use, which is a common technique in VAEs that your work seeks to improve upon by leveraging discreteness. Their findings on improved reconstruction and latent space smoothness, despite some trade-offs in generation quality, offer a contrasting approach to enhancing latent variable descriptiveness compared to your discrete attention mechanism.", "citation": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio (2015). Generating Sentences from a Continuous Space. arXiv:1511.06349. https://arxiv.org/abs/1511.06349"}, {"paper_id": 10, "text": ["merits of discreteness, we demonstrate that our design is free\nfrom posterior collapse regardless of latent sequences length.\nConsequently, the representation power of DAVAM can be\nsigni\ufb01cantly enhanced without posterior collapse.\nWe evaluate DAVAM on several benchmark datasets on\nlanguage modeling. The experimental results demonstrate the\nsuperiority of our proposed method in text generation over its\ncounterparts.\nOur contributions can thus be summarized as:\n1) To the best of our knowledge, this is the \ufb01rst work that\nproposes auto-regressive variational attention to improve\nVAEs for text modeling, which signi\ufb01cantly enriches the\ninformation representation of latent space.\n2) We further design discrete latent space for the proposed\nvariational attention, which effectively addresses the\nposterior collapse issue during the optimization.\nII. BACKGROUND\nA. Variational Antoencoders for Text Modeling\nVariational Autoencoders (VAEs) [1] are a well known\nclass of generative models. Given sentences x = x1:T with\nlength T, we seek to infer latent variables z that explain the\nobservation. To achieve this, we need to maximize the marginal\nlog-likelihood log p\u03b8(x), which is usually intractable due to\nthe complex posterior p(z|x). Consequently an approximate\nposterior q\u03c6(z|x) (i.e. the encoder) is introduced, and the\nevidence lower bound (ELBO) of the marginal likelihood is\nmaximized as follows:\nlog p\u03b8(x) \u2265Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)]\n|\n{z\n}\nreconstruction loss\n\u2212DKL(q\u03c6(z|x)\u2225p(z))\n|\n{z\n}\nKL divergence\n,\n(1)\nwhere p\u03b8(x|z) represents likelihood function conditioned on\nz, also known as the decoder. In the context of text modeling,\nboth encoder and decoder are usually implemented by deep\nrecurrent models such as LSTMs [10], parameterized by \u03c6 and\n\u03b8 respectively.\nB. Challenges\na) Information Underrepresentation: Information under-\nrepresentation is a common issue in applying VAEs for text\nmodeling. Conventional VAEs build latent variables based on\nthe last hidden state of LSTM encoder, i.e. z = zT . During\nthe decoding process, we \ufb01rst sample zT , from which new\nsentences \u02c6x = \u02c6x1: \u02c6T can be generated:\np(\u02c6x|z) = p\u03b8(\u02c6x1|zT )\n\u02c6T\nY\nt=2\np\u03b8(\u02c6xt|\u02c6xt\u22121, zT ),\n(2)\nwhere \u02c6T is the length of reconstructed sentence \u02c6x. However,\nthe representation of zT is generally insuf\ufb01cient to summarize\nthe semantic dependencies in x, and thus deteriorates the\nreconstruction.\nb) Posterior Collapse: Posterior collapse usually arises\nas DKL(q\u03c6(z|x)\u2225p(z)) diminishes to zero, where the lo-\ncal optimal gives q\u03c6(z|x) = p(z). Posterior collapse hap-\npens inevitably as the ELBO contains both the recon-\nstruction loss Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)] and the KL-divergence\nDKL(q\u03c6(z|x)\u2225p(z)), as shown in Equation (1). When posterior\ncollapse happens, x becomes independent of z as p(x)p(z) =\np(x)q\u03c6(z|x) = p(x) p(x,z)\np(x) = p(x, z). Therefore, the encoder\nlearns a data-agnostic posterior without any information from x,\nwhile the decoder fails to perform valid generation but purely\nbased on random noise.\nIII. METHODOLOGY\nWe now present our solutions to address the aforementioned\nchallenges. In order to enrich the latent space, we propose\nan auto-regressive variational attention model to capture the\nsemantic dependencies in the input space. We \ufb01rst instantiate\nvariational attention with the Gaussian distribution and show\nthat it suffers from posterior collapse. Then to solve the\nchallenge, we further discretize the latent space with one-\nhot categorical distribution, leading to discrete auto-regressive\nvariational attention models (DAVAM), as illustrated in Figure 2.\nWe carefully analyze the superiority of DAVAM to avoid\nposterior collapse.\nA. Gaussian Auto-regressive Variational Attention Models\nTo enrich the representation of latent space z, we seek\nto incorporate the attention mechanism into VAEs. Speci\ufb01-\ncally, we denote the encoder hidden states as he\n1:T , and the\ndecoder hidden states as hd\n1: \u02c6T . We build a latent sequence\nz = z1:T upon encoder hidden states he\n1:T . To facilitate\nsuch variational attention model, one can choose the con-\nventional Gaussian distribution [1] for variational posteriors,\ni.e. q(z|x) = QT\nt=1 q(zt|x) where q\u03c6(zt|x) = N(\u00b5t, \u03c3tI).\nWe name the resulting model as Gaussian Auto-regressive\nVariational Attention Model (GAVAM).\nGiven\nz1:T ,\nsimilar\nto\nattention-based\nsequence-to-\nsequence (seq2seq) models [14], the attentional context vectors\nci and scores at i-th decoding step can be computed by\nci =\nT\nX\nt=1\n\u03b1i,tzt, \u03b1i,t =\nexp(\u02dc\u03b1i,j)\nPT\nj=1 exp(\u02dc\u03b1i,j)\n,\n(3)\nwhere \u02dc\u03b1i,t = v\u22a4tanh(Wezt+Wdhd\ni\u22121+b) is the unnormalized\nscore, t \u2208{1, 2, ..., T} is the encoder time step, and We, Wd\nare the corresponding parameters. By taking ci as extra input\nto the decoder, the generation process is reformulated as\np(\u02c6x|c, z) = p(\u02c6x1|c1, z1:T )\n\u02c6T\nY\n\u02c6t=2\np(\u02c6x\u02c6t|\u02c6x\u02c6t\u22121, c\u02c6t, z1:T ).\nUnlike Equation 2, at each time step, the decoder receives\nsupervision from the context vector, which is a weighted sum of\nthe latent sequence z1:T . Consequently, the variational posterior\nq\u03c6(z1:T |x) encodes the semantic dependency from the obser-\nvations, such that the issue of information underrepresentation\ncan be effectively mitigated.\n", "TABLE III\nSAMPLED SHORT, MEDIUM AND LONG SENTENCES AS WELL AS THEIR GPT-2 PPL SCORES FOR MEASURING FLUENCY.\nMethods\nSamples\nPPL\u2193\npretraining\n\u2022 [s] i can say what can be the least in terms also in any form of power stream [/s]\n5.97\n+FBP\n\u2022 [s] how you de\ufb01ne yourself ? birth control. you will \ufb01nd yourself a dead because of your periods. [/s]\n5.43\nVAE\n\u2022 [s] are they allowed to join (francisco) in\nUNK. giants in the \ufb01rst place.? check out other answers.\n5.21\ndo you miss the economy and not taking risks in the merchant form, what would you tell? go to\nthe yahoo home page and ask what restaurants follow this one. [/s]\nVAE+Attn\n\u2022 [s] explain some make coming you think and represents middle line girl coming. [/s]\n8.83\n\u2022 [s] who just live this in you to get usual usual help the idea for out to use guess guess thats UNK\n7.16\n\u2022 [s] is masterbating masterbating anyone hi fact fact forgive forgive forgive virgin chlorine does\n5.01\n\u2019re hydrogen download \u2019re whats \u2019re does solve \u2019re whats solve 2y germany germany monde\npourquoi \u2019m does fun pourquoi \u2019m \u2019re \u2019m \u2019re solve \u2019m does solve pourquoi \u2019m \u2019m \u2019m \u2019m solve \u2019m \u2019m [/s]\nGAVAM\n\u2022 [s] generally do problems problems, do you have problems [/s]\n6.57\n\u2022 [s] plz can you get a UNK envelope in e ? for me for my switched for eachother i would switched to i\n6.50\ni think thats . i need to assume . [/s]\n\u2022 [s] what to do, yoga is there to place and pa? i can de\ufb01nately, but that, you will the best, but the the only\n5.18\namount right? the best range, it though, to do n\u2019t do to be to be out, and [/s]\nDAVAM\n\u2022 [s] what is the meaning of time management? [/s]\n4.52\n(K=512)\n\u2022 [s] what should you be thankful for thanksgiving dinner and how to get some money with a thanksgiving\n4.25\ndinner? [/s]\n\u2022 [s] is anyone willing to donate plasma if you are allergic to cancer or anything else? probably you can.\n3.87\ni\u2019ve never done any thing but it is only that dangerous to kill bacteria. i have heard that it doesn\u2019t have\nany effect on your immune system. [/s]\nTABLE IV\nGPT-2 PERPLEXITY SCORES (\u2193) WITH STANDARD DEVIATION (\u00b1) FOR\nGENERATION QUALITY (ROW 1-5), AND THE RECONSTRUCTION LOSS (\u2193)\nON YAHOO DATASET FOR LANGUAGE MODELING (LAST ROW).\nLength\nVAE+Attn\nGAVAM\nDAVAM\n10\n7.75\u00b11.71\n7.32\u00b11.64\n6.58\u00b11.17\n20\n7.04\u00b11.46\n6.94\u00b11.40\n6.49\u00b11.09\n30\n6.70\u00b11.31\n6.75\u00b11.38\n5.97\u00b10.87\n40\n6.54\u00b11.50\n6.28\u00b11.36\n5.79\u00b10.81\n50\n6.42\u00b11.61\n6.10\u00b11.33\n5.55\u00b10.97\nRec\n10.85\n350.14\n259.68\n1k\n2k\n4k\nThe Size of Training Samples\n25\n30\n35\n40\n45\n50\n55\n60\n65\nPPL\nNo Aug\n0.5x Aug\n1x Aug\n2x Aug\n4x Aug\nFig. 3.\nThe perplexity scores under different sizes of augmented training\nsentences on SNLI dataset.\nbetween language modeling and generation, we also report the\nreconstruction loss on Yahoo. The results are listed in Table IV.\nIt can be found that while VAE+Attn has a superior advantage\nin language modeling, it has the worst GPT-2 perplexity scores\nsince i.i.d. Gaussian noises contain no sequential information.\nGAVAM has minor improvement over VAE+Attn on GPT-\n2 scores thanks to the auto-regressive prior, but it performs\npoorly on language modeling due to posterior collapse. Finally,\nDAVAM generally achieves the lowest perplexity scores with\nreasonable ability in language modeling. This indicates the\nsuperiority of the auto-regressive prior for generation from\nscratch, and the power of discreteness to avoid posterior\ncollapse in \ufb01tting observations.\n2) Generation Diversity: Diversity is another dimension to\nmeasure the success of language generation. We follow [29] to\ncompute the entropy and the percentage of distinct unigrams or\nbigrams, which are denoted as Ent., Dist-1, and Dist-2 respec-\ntively. From Table V, it can be observed that pretraining+FBP\nVAE achieves the highest diversity scores. However, VAE+Attn,\nachieves the poorest diversity scores due to repeated words,\nas shown in Table III. GAVAM has only minor improvement\nover VAE+Attn, and repeated words frequently occur as well.\nFinally, our DAVAM can generate diverse sentences despite\nthe scores being slightly lower than pretraining+FBP VAE.\nThis is due to the training of auto-regressive prior yields over-\ncon\ufb01dent choices of latent codebooks, which can better capture\nsequential dependency with higher generation quality, but at\nthe sacri\ufb01ce of less generation diversity.\n3) Generation from Scratch as Data Augmentation: Given\nthe superior generation quality and diversity of DAVAM, we\nnow apply it for data augmentation to improve language\nmodels that are trained over limited corpus. By amortizing the\ntraining instances into model parameters, DAVAM is able to\ngenerate sentences directly from random noises (i.e. generation\nfrom scratch). Speci\ufb01cally, we selectively choose the corpus\nsize in {1k, 2k, 4k} on SNLI dataset, and use a pre-trained\nDAVAM model to augment {0.5\u00d7, 1\u00d7, 2\u00d7, 4\u00d7} times of th\ncorpus. Figure 3 shows the corresponding improvement of\nperplexity scores. It can be found that the perplexity decreases\nproportionally to the training size. Therefore, our DAVAM can\nbe applied to language models that are trained with limited\ncorpus for further improvement.\n", "Discrete Auto-regressive Variational Attention\nModels for Text Modeling\nXianghong Fang\u2217\u2020, Haoli Bai\u2217\u2020, Jian Li\u2020, Zenglin Xu\u2021, Michael Lyu\u2020, Irwin King\u2020\n\u2020Department of Computer Science and Engineering, The Chinese University of Hong Kong\n\u2021School of Computer Science and Engineering, Harbin Institute of Technology, Shenzhen\nEmail: xianghong fang@163.com, {hlbai, jianli, lyu, king}@cse.cuhk.edu.hk, xuzenglin@hit.edu.cn\n\u2217: Equal Contribution\nAbstract\u2014Variational autoencoders (VAEs) have been widely\napplied for text modeling. In practice, however, they are troubled\nby two challenges: information underrepresentation and poste-\nrior collapse. The former arises as only the last hidden state of\nLSTM encoder is transformed into the latent space, which is\ngenerally insuf\ufb01cient to summarize the data. The latter is a long-\nstanding problem during the training of VAEs as the optimization\nis trapped to a disastrous local optimum. In this paper, we\npropose Discrete Auto-regressive Variational Attention Model\n(DAVAM) to address the challenges. Speci\ufb01cally, we introduce\nan auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency\nfrom the input. We further design discrete latent space for the\nvariational attention and mathematically show that our model is\nfree from posterior collapse. Extensive experiments on language\nmodeling tasks demonstrate the superiority of DAVAM against\nseveral VAE counterparts. Code will be released.\nIndex Terms\u2014Text Modeling, Information Underrepresenta-\ntion, Posterior Collapse\nI. INTRODUCTION\nAs one of the representative deep generative models, vari-\national autoencoders (VAEs) [1] have been widely applied\nin text modeling [2]\u2013[9]. Given input text x \u2208X, VAEs\nlearn the variational posterior q\u03c6(z|x) through the encoder and\nreconstruct output \u02c6x from latent variables z via the decoder\np\u03b8(x|z). Both encoder and decoder are usually implemented by\ndeep recurrent networks such as LSTMs [10] in text modeling.\nDespite the success of VAEs, two long-standing challenges exist\nfor such variational models: information underrepresentation\nand posterior collapse.\nThe challenge of information underrepresentation refers to\nthe limited expressiveness of the latent space z. As shown in\nthe left of Figure 1, current VAEs build a single latent variable\nz = zT based on the last hidden state of LSTM encoder [5], [6],\n[11], [12]. However, this is generally insuf\ufb01cient to summarize\nthe input sentence [13]. Thus the generated sentences from the\ndecoder are often poorly correlated. Notably, the sequence of\nencoder hidden states re\ufb02ects the semantic dependency of the\ninput sentence, and the whole hidden context may bene\ufb01t the\ngeneration. Therefore, a potential solution is to enhance the\nrepresentation power of VAEs via the attention mechanism [14],\n[15], a superior component in discriminative models. However,\nthe attention module cannot be directly deployed in generative\nmodels like VAEs, as the attentional context vectors are hard\nAttention\n\u00af\nLSTM Encoder\nq\u03c6(z|x)\n\u00af\nLSTM Decoder\np\u2713(x|z)\nx\n\u02c6x\n\u00af\nLSTM Encoder\nq\u03c6(z|x)\n\u00af\nLSTM Decoder\np\u2713(x|z)\n\u02c6x\nz1:T\nzT\nPrior\nAutoregressive     \nPrior\nx\nN(0, I)\nFig. 1. Illustration of conventional VAEs (left) and our proposed auto-regressive\nvariational attention models (right).\nto compute from randomly sampled latent variables during the\ngeneration phase.\nPosterior collapse is another well-known problem during the\ntraining of VAEs [16]. It occurs as the variational posterior\nq\u03c6(z|x) converges to the prior distribution p(z), thus the\ndecoder receives no supervision from the input x. Previous\nefforts alleviate this issue by either annealing the KL divergence\nterm [11], [16], [17], revising the model [18]\u2013[20], or modifying\nthe training procedure [6], [12]. Nevertheless, they primarily\nfocus on a single latent variable for language modeling,\nwhich still suffer from the information underrepresentation\nas mentioned before. To derive more powerful latent space, the\nchallenge of posterior collapse should be carefully handled.\nIn this paper, we propose Discrete Auto-regressive Varia-\ntional Attention Model (DAVAM) to address the aforemen-\ntioned challenges. First, to mitigate the information under-\nrepresentation of VAEs, we introduce a variational attention\nmechanism together with an auto-regressive prior (dubbed as\nauto-regressive variational attention). The variational attention\nassigns a latent sequence z = z1:T over each encoder hidden\nstate to capture the semantic dependency from the input, as is\nshown in the right of Figure 1. During the generation phase, the\nauto-regressive prior generates well-correlated latent sequence\nfor computing the attentional context vectors. Second, we utilize\ndiscrete latent space to tackle the posterior collapse in VAEs.\nWe show that the proposed auto-regressive variational attention\nmodels, when armed with conventional Gaussian distribution,\nface high risks of posterior collapse. Inspired by the recently\nproposed Vector Quantized Variational Autoencoder (VQ-\nVAE) [21], [22], we design a discrete latent distribution over\nthe variational attention mechanism. By analyzing the intrinsic\narXiv:2106.08571v1  [cs.LG]  16 Jun 2021\n", "\ud835\udc52\"\ud835\udc52#\ud835\udc52$\n\ud835\udc52%\n\u210e\"'\n\u210e#'\n\u210e$'\n\u210e(\n'\nAttention\n11\n107\n23\n7\n\ud835\udc52\"\"\n\ud835\udc52\"./\n\ud835\udc52#$\n\ud835\udc52/\nCode Book\nLSTM Encoder\nAuto-regressive \nPrior\nz1:T \u21e0q\u03c6(z|x)\nz1:T \u21e0p (z)\nLSTM Decoder\n\u210e\"0\n\u210e#0\n\u210e$0\n\u210e(1\n0\nx1:T\nSTE\n\u02c6x1: \u02c6T\nFig. 2. The overall architecture of the proposed DAVAM. Given observations x = x1:T , the encoder hidden states he\n1:T are quantized to code book {ek}K\nk=1\nbased on index sequence z1:T from the posterior. The quantized hidden states ez1:T are then forwarded to the attention module with decoder hidden states\nhd\n1: \u02c6\nT . During back-propagation, the gradients of ez1:T are directly copied to he\n1:T with STE. To generate new sentences from DAVAM, we start from the\nauto-regressive prior to sample a new latent sequence z1:T . The sequence z1:T is then used to index the code book for attention computation during decoding.\na) Auto-regressive Prior: A key difference between varia-\ntional auto-regressive attention models and conventional VAEs\nis the choice of a prior distribution. During the generation, the\nlatent sequence z1:T are sampled from the prior unconditionally,\nand are then fed to the attention module together with hd\n1: \u02c6T .\nThe most adopted prior N(0, I), however, is non-informative to\ngenerate well-correlated latent sequence for the attention as that\nduring training. Therefore the decoder receives no informative\nsupervision that gives reasonable generation.\nTo solve that, we deploy an auto-regressive prior p\u03c8(z1:T ) =\np\u03c8(z1) QT\nt=2 p\u03c8(zt|z1:t\u22121) parameterized by \u03c8 to capture\nthe underlying semantic dependencies. Speci\ufb01cally, we take\np\u03c8(zt|z1:t\u22121) = N(\u02c6\u00b5t, \u02c6\u03c3tI), where (\u02c6\u00b5t, \u02c6\u03c3tI) is produced by\na PixelCNN, a superior model in learning sequential data [23].\nb) Posterior Collapse in GAVAM:\nThe training of\nGAVAM can be easily troubled by posterior collapse due to two\naspects. To see this, similar to Equation 1, the minimization\nof the ELBO now can be written as:\nmin\n\u03c6,\u03b8,\u03c8 \u2212Ez1:T \u223cq\u03c6[log p\u03b8(x|z1:T )]\n(4)\n+\nT\nX\nt=1\nDKL(q\u03c6(zt|x)\u2225p\u03c8(zt|z1:t\u22121)).\nOn the one hand, the KL divergence scales linearly to the\nsequence length of T, which makes the training unstable across\ndifferent input lengths. On the other hand, and more seriously,\nboth \u03c6 and \u03c8 are used to minimize the KL divergence, which\ncan easily trap the learned posteriors. To demonstrate this, for\nexample, the KL divergence between two Gaussian distributions\ncan be written as:\nT\nX\nt=1\nDKL(q\u03c6(zt|x)\u2225p\u03c8(zt|z1:t\u22121))\n(5)\n=\nT\nX\nt=1\nD\nX\nd=1\n1\n2(log \u02c6\u03c32\ntd\n\u03c32\ntd\n\u22121 + \u03c32\ntd + (\u02c6\u00b5td \u2212\u00b5td)2\n\u02c6\u03c32\ntd\n),\nwhere D is the latent dimension of zt. Whenever \u03c32\ntd \u2192\u02c6\u03c32\ntd\nand \u00b5td \u2192\u02c6\u00b5td before q\u03c6(z1:T |x) encodes anything from x,\nboth q\u03c6(z1:T |x) and p\u03c8(z1:T ) get stuck in local optimal and\nlearn no semantic dependency for reconstruction.\nB. Discrete Auto-regressive Variational Attention Models\nInspired by recent studies [21], [24] that demonstrate the\npromising effects of discrete latent space, we explore its\npotential in handling posterior collapse over the variational at-\ntention, leading to discrete auto-regressive variational attention\nmodel (DAVAM).\nSpeci\ufb01cally, we introduce a code book {ek}K\nk=1 with size\nof K, where each ek is a vector in the latent space. We expect\nthe combination of code book can represent the semantic\ndependency from observed sentence x. We now substitute\nthe Gaussian distributed z1:T with discrete indices over code\nbook that follows one-hot categorical distribution:\nq\u03c6(zt = k|x) =\n(\n1\nk = arg minj \u2225he\nt \u2212ej\u22252\n0\notherwise\n.\n(6)\nGiven index zt, we transform the the encoder hidden state he\nt\nto the nearest ezt. Then we use ezt instead of zt in Equation 3\nto compute attention scores \u03b1t and the context vectors ci.\nCorrespondingly, as z1:T are discrete indices, we assign\ncategorical distribution for the auto-regressive prior, i.e.,\np\u03c8(zt|z1:t\u22121) = Cat(\u03b3t). The categorical parameter can be\nobtained from the PixelCNN model given historical records\nz1:t\u22121, i.e, \u03b3t = PixelCNN\u03c8(z1:t\u22121) \u2208[0, 1]K.\na) Advantages of Discreteness: Thanks to the nice prop-\nerties of discreteness, the optimization of DAVAM does not\nsuffer from posterior collapse. Speci\ufb01cally, the KL divergence\n"], "summary": "The cited paper, \"Discrete Auto-regressive Variational Attention Models for Text Modeling,\" directly addresses the same core problems of information under-representation and posterior collapse in VAEs for language generation that your work aims to solve. It proposes a similar solution by introducing discrete variational attention with an auto-regressive prior, which aligns closely with your approach. Specifically, their DAVAM model leverages discrete latent space to avoid posterior collapse and an auto-regressive prior to capture sequential dependencies, mirroring the key innovations in your \"Discrete Variational Attention Models for Language Generation.\" Their experimental results on language modeling benchmarks further support the efficacy of discrete latent spaces and auto-regressive priors in VAEs for text generation.", "citation": "Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, Jianfeng Gao (2020). Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space. arXiv:2004.04092. https://arxiv.org/abs/2004.04092"}, {"paper_id": 1, "text": ["Straight to the Gradient:\nLearning to Use Novel Tokens for Neural Text Generation\nXiang Lin 1 Simeng Han 1 Sha\ufb01q Joty 1 2\nAbstract\nAdvanced large-scale neural language models\nhave led to signi\ufb01cant success in many language\ngeneration tasks. However, the most commonly\nused training objective, Maximum Likelihood Es-\ntimation (MLE), has been shown problematic,\nwhere the trained model prefers using dull and\nrepetitive phrases. In this work, we introduce\nScaleGrad, a modi\ufb01cation straight to the gradient\nof the loss function, to remedy the degeneration\nissue of the standard MLE objective. By directly\nmaneuvering the gradient information, ScaleGrad\nmakes the model learn to use novel tokens. Empir-\nical results show the effectiveness of our method\nnot only in open-ended generation, but also in\ndirected generation tasks. With the simplicity in\narchitecture, our method can serve as a general\ntraining objective that is applicable to most of the\nneural text generation tasks.\n1. Introduction\nText generation has been one of the most important research\nproblems in natural language processing (NLP). Thanks to\nthe advances in neural architectures, models are now capa-\nble of generating texts that are of better quality than before\n(Brown et al., 2020). However, despite the countless ef-\nforts that have been made to improve neural architectures,\nmodels trained with the standard Maximum Likelihood Es-\ntimation (MLE) objective are known to prefer generating\ndull and highly repetitive texts. For instance, in open-ended\ngeneration tasks, such as story continuation or open dia-\nlogue generation, it has been observed that even with large\npre-trained models like GPT-2 (Radford et al., 2019), high\nfrequency tokens largely dominate the generation (Welleck\net al., 2020; Holtzman et al., 2020). Similar observation has\nbeen reported in directed generation tasks such as summa-\n1Nanyang Technological University, Singapore 2Salesforce\nResearch Asia, Singapore.\nCorrespondence to:\nXiang Lin\n<linx0057@e.ntu.edu.sg>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nrization (See et al., 2017), image captioning (Melas-Kyriazi\net al., 2018; Wang & Chan, 2019) and machine translation\n(Tu et al., 2016; Stahlberg & Byrne, 2019).\nThe methods proposed to solve the degeneration issues with\nneural text generation can be primarily categorized into two\ngroups: (i) training based methods, which include incorpo-\nrating auxiliary losses (See et al., 2017; Welleck et al., 2020;\nLi et al., 2020) and coverage vector (See et al., 2017; Tu\net al., 2016); (ii) decoding based methods, such as stochastic\nbeam search (Kool et al., 2019), top-k sampling (Fan et al.,\n2018), nucleus or top-p sampling (Holtzman et al., 2020),\nand inverse probability weighting (Zhang et al., 2021).\nThough decoding based methods, in particular nucleus and\ntop-k sampling, perform well in practice in open-ended\ngeneration tasks, signi\ufb01cantly reducing the degeneration\nproblem, they do not address the fundamental modeling is-\nsue that the token-level probabilities produced by the neural\nmodel are problematic (Welleck et al., 2020). In addition,\nour experiments demonstrate that sampling methods also\nfail to generate high-quality texts in directed generation\ntasks such as abstractive text summarization.\nIn this work, based on the known observation that the text\ngeneration models trained with MLE objective tend to gener-\nate repetitive tokens or phrases, we introduce a novel method\ncalled ScaleGrad for neural text generation training, by di-\nrectly maneuvering the gradients to make the model learn\nto use novel tokens during training. Our method lies in the\ntraining based group, which aims to address the fundamen-\ntal modeling problem, that is, the token-level distribution\npredicted by the generation model.\nIn a concurrent work, Wang et al. (2020) introduce a tem-\nperature scaling approach called Contextual Temperature to\nimprove general language modeling. In this approach, the\ntemperature value in the softmax function is parameterized\nby a neural network that is jointly trained with the main\nmodel. Though the objective of their work is not explicitly\nrelated to text degeneration, their analysis shows tempera-\nture scaling essentially changes the gradient updates that\neach token receives during training, which further motivates\nour work.\nWe conduct experiments with different neural architectures\narXiv:2106.07207v1  [cs.CL]  14 Jun 2021\n", "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation\nS., Wallach, H., Fergus, R., Vishwanathan, S., and Gar-\nnett, R. (eds.), Advances in Neural Information Process-\ning Systems 30, pp. 5998\u20136008. Curran Associates, Inc.,\n2017.\nURL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf.\nVedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider:\nConsensus-based image description evaluation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 4566\u20134575, 2015.\nWang, P.-H., Hsieh, S.-I., Chang, S.-C., Chen, Y.-T., Pan,\nJ.-Y., Wei, W., and Juan, D.-C. Contextual temperature\nfor language modeling. arXiv, 2020. URL https://\narxiv.org/abs/2012.13575.\nWang, Q. and Chan, A. B. Describing like humans: On\ndiversity in image captioning. In 2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pp. 4190\u20134198, 2019.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and\nWeston, J. Neural text generation with unlikelihood train-\ning. In International Conference on Learning Represen-\ntations, 2020. URL https://openreview.net/\nforum?id=SJeYe0NtvH.\nWilliams, R. J. and Zipser, D. A learning algorithm for con-\ntinually running fully recurrent neural networks. Neural\nComputation, 1(2):270\u2013280, 1989.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M.,\nMacherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,\nL., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens,\nK., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J.,\nRiesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes,\nM., and Dean, J. Google\u2019s neural machine translation\nsystem: Bridging the gap between human and machine\ntranslation. CoRR, abs/1609.08144, 2016. URL http:\n//arxiv.org/abs/1609.08144.\nZhang, X., Sun, M., Liu, J., and Li, X. Improving diversity\nof neural text generation via inverse probability weight-\ning. arXiv, 2021. URL https://arxiv.org/abs/\n2103.07649.\nZhao, W., Peyrard, M., Liu, F., Gao, Y., Meyer, C. M.,\nand Eger, S. MoverScore: Text generation evaluating\nwith contextualized embeddings and earth mover dis-\ntance. In Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 563\u2013578, Hong Kong,\nChina, November 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1053. URL https:\n//www.aclweb.org/anthology/D19-1053.\nZhao, Y., Ni, C., Leung, C.-C., Joty, S., Chng, E. S., and\nMa, B. Preventing early endpointing for online auto-\nmatic speech recognition. In ICASSP 2021 - 2021 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pp. 6813\u20136817, 2021. doi:\n10.1109/ICASSP39728.2021.9413613.\n", "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation\nincluding LSTM (Hochreiter & Schmidhuber, 1997) and\nTransformer (Vaswani et al., 2017) across different tasks\nin opened-ended and directed text generation. Through\nextensive analysis we demonstrate that ScaleGrad consis-\ntently improves the generation quality according to both\nhuman evaluation and automatic metrics. Compared to\nother training based methods, ScaleGrad is architecturally\nsimpler and easier to \ufb01t into current neural models (\u00a73.2),\nwhile possessing a wide applicability to different text gener-\nation tasks (\u00a74.2 and \u00a75.2). The source code is available at\nhttps://github.com/shawnlimn/ScaleGrad.\n2. Background\n2.1. Neural text generation\nThe NLP tasks involving text generation can be broadly\ncategorized into two types: directed generation and open-\nended generation (Holtzman et al., 2020). In the former case,\nthe output text can be seen as a constrained transformation of\nthe input. Examples include text summarization, machine\ntranslation, and image captioning. In the latter case, the\ninput context only provides a certain degree of constraints\nsuch that the model is allowed to generate the following\ntexts with a considerable degree of freedom. Story/text\ncontinuation and dialogue generation fall in this category.\nNeural models frame text generation tasks as some form of\nconditional language modeling, which is typically trained\nto maximize the log likelihood (equivalently, minimize the\nnegative log likelihood) of the training data. The Maximum\nLikelihood Estimation or MLE objective for an input-output\npair (x, y) can be expressed as follows.\nLMLE = \u2212\nT\nX\nt=1\nlog P\u03b8(yt|y<t, x)\n(1)\nwhere \u03b8 denotes model parameters, T is the length of the\noutput sequence y, and x is the task-speci\ufb01c input condition,\ne.g., source document in summarization, image in image\ncaptioning, conversation history in dialogue generation and\n\u2205in text continuation. Teacher Forcing (Williams & Zipser,\n1989), where current step\u2019s target token is passed as the\nnext input to the decoder rather than the predicted token, is\nusually used to train the models for faster convergence.\nDegeneration\nDegeneration has been a key problem in\nneural text generation models for open-ended tasks, where\nthe model generates texts that are repetitive, overly generic\n(dull), incoherent and gibberish. It can happen at different\nlevels of granularity \u2013 token, phrase, sentence and paragraph.\nThe problem has not been mitigated even with large-scale\npre-trained models like GPT-2 Large (Radford et al., 2019;\nHoltzman et al., 2020). Degeneration has also been observed\nin directed generation tasks even though the output in these\ntasks is con\ufb01ned by the input. For instance, in text summa-\nrization, most of the advanced models such as BertSum (Liu\n& Lapata, 2019), BART (Lewis et al., 2020) and ProphetNet\n(Qi et al., 2020) make use of tri-gram blocking (Paulus et al.,\n2018) within beam search to remove duplicate trigrams\nduring decoding, which improves the generation quality in\nterms of the automatic metric. This implies that even with\ninvolvement of large-scale pre-trained models, degeneration\nstill exists. Similar issues have been reported in machine\ntranslation (Koehn & Knowles, 2017; Stahlberg & Byrne,\n2019), image-description generation (Melas-Kyriazi et al.,\n2018; Wang & Chan, 2019) and next utterance generation\nin conversations (Jiang et al., 2020).\n2.2. Combating neural text degeneration\nOut of the methods proposed to tackle neural text degenera-\ntion, top-k sampling (Fan et al., 2018) and nucleus sampling\n(Holtzman et al., 2020) stand out as representatives of de-\ncoding based methods and unlikelihood training (Welleck\net al., 2020) as a representative training based method. Dur-\ning each decoding step, nucleus and top-k sampling use\ndifferent functions to \ufb01lter the candidate tokens, thus refor-\nmalizing the probability distribution. Then they sample the\nnext token from the new distribution instead of maximiz-\ning the actual likelihood. Randomness brought by these\nsampling methods reduces duplicate tokens in the output.\nHowever, decoding strategy solely does not solve the under-\nlying modeling problem with MLE training, as pointed out\nby Welleck et al. (2020). Our analysis in \u00a75.2 also reveals\nthat sampling methods fail to generate high-quality texts in\ndirected generation tasks.\nTo address the issue with MLE, Welleck et al. (2020) pro-\npose the neural unlikelihood (UL) training method. During\ntraining, at each decoding step t, UL adds an auxiliary loss\nto the original cross entropy loss as follows.\nLt\nUL = \u2212log P\u03b8(yt|y<t)\n|\n{z\n}\nMLE\n\u2212\u03b1\nX\nc\u2208Ct\nlog(1 \u2212P\u03b8(c|y<t))\n|\n{z\n}\nUL\n(2)\nwhere \u03b1 is a hyper-parameter and Ct is the set of nega-\ntive tokens at decoding step t, which is constructed by\nprevious context tokens that are not the current token,\nCt = {y1, . . . , yt\u22121} \\ yt. The auxiliary UL loss decreases\nthe total loss based on the \u201cunlikely\u201d probabilities of nega-\ntive tokens, thus implicitly reducing the probability assigned\nto the repetitive tokens. UL training targets at improving\nthe underlying modeling problem, which accords with our\ngoal. Therefore, we mainly compare our method with UL\ntraining.1 We discuss how our method is different from UL\ntraining from the gradient perspective in \u00a73.3.\n1Welleck et al. (2020) also propose a sequence-level UL. Since\nour work focuses on token-level modeling, we compare with their\ntoken-level UL training in this work.\n", "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation\n3. Methodology: learning to use novel tokens\nTraining a text generation model with MLE objective treats\neach token in the gold (ground truth) sequence equally. It\nhas been shown that with this approach, the model exhibits\nthe tendency to generate repetitive tokens/phrases during\ninference (Welleck et al., 2020; Holtzman et al., 2020). To\nmitigate this degeneration problem, we argue that the model\nshould focus more on learning to use novel tokens, rather\nthan treating all the tokens in a sequence equally. Our main\nidea is to maintain a dynamic list of novel tokens at each\ndecoding step during training and encourage the model to\nlearn to use tokens from this list for generation.\nFormally, let y = (y1, . . . , yt, . . . , yT ) be the ground-truth\n(target) token sequence that the model is learning to gener-\nate in an auto-regressive manner, one token at a time. At\ntime step t, we de\ufb01ne the token \u02dcyt\ni in the vocabulary V as a\nnovel token, if \u02dcyt\ni has not been generated before, i.e., \u02dcyt\ni /\u2208\n{y1, . . . , yt\u22121}. By the de\ufb01nition, we have a dynamic set\nof novel tokens St\nnovel \u2286V at each decoding step t in train-\ning, which shrinks over time as new tokens are observed\nin the ground-truth sequence (see Appendix B for an illus-\ntration). Note that the set of non-novel tokens at each step\n(i.e., V\\St\nnovel) is equivalent to the set of negative tokens Ct\nin UL (Eq. 2) except that it may contain the current target\ntoken yt, if it was observed before. To encourage the model\nto focus on learning to use novel tokens, we propose an\narchitecturally-simple yet effective method that can \ufb01t into\nmost of the auto-regressive generation models. Our method,\nrequiring no carefully-designed components, is derived di-\nrectly from the gradient analysis of the loss function.\n3.1. Gradient analysis for MLE training\nLet us \ufb01rst consider the gradient analysis of the model\ntrained with MLE. Let ot \u2208R|V| denote the pre-softmax\nscores (i.e.,\nlogits) over the vocabulary at time step t,\nwhere ot\ni is the score for the token with index i. Simi-\nlarly, let pt\nk = [softmax(ot)]k represent the probability of\nthe ground truth token with index k in the vocabulary. The\npartial derivative of the MLE objective (Eq. 1) at time step t\nwith respect to the logit ot\ni can be shown as (omitting t and\n\u2018MLE\u2019 subscript for simplicity):\n\u2207oiL = \u2202L\n\u2202pk\n\u00b7 \u2202pk\n\u2202oi\n= pi \u22121(i = k)\n(3)\nwhere pi = [softmax(o)]i and 1(\u00b7) is the Indicator function\n(derivation is given in Appendix A). Speci\ufb01cally, the gradi-\nent of the loss w.r.t. the ground truth token logit ok is (pk\u22121)\nand for any other token logit oi is pi. As the gradient-based\noptimization proceeds, the gradient converges to \u03f5, a num-\nber that is close enough to 0. Another interpretation is that\nthe gradient of the loss is supposed to be close to 0 around\na (local) minimum. Therefore, to reach the minimum, or\nto make the gradient close to 0, the model would try to\nincrease the probability of ground truth token pk and reduce\nthe probability of non-ground truth token pi in the MLE\ntraining.\nFrom Eq. 3, it is clear that the gradient that every token oi \u2208\nV receives is directly related to its generation probability pi.\nTherefore, we hypothesize that directly manipulating the\ngeneration probabilities of tokens, thereby controlling their\ngradients, can help us achieve our goal, which is to train the\nmodel so that it is encouraged to use novel tokens.\n3.2. Our method: ScaleGrad\nTo encourage the model to learn to use novel tokens for\ngeneration, we can control the gradient to force the model\nto either increase the probability of novel tokens or decrease\nthe probability for non-novel tokens. Based on this basic\nidea, we propose an effective training method keeping it in\nthe simplest form. Speci\ufb01cally, at each decoding step of\ntraining, we re-normalize the softmax output (the probabil-\nity distribution over the vocabulary) in a way such that the\nmodel is informed of the current set of novel tokens and\nencouraged to use them. Assuming that pt is the softmax\noutput at step t and St\nnovel is the corresponding set of novel\ntokens at that step, we re-compute the probability distribu-\ntion as follows (again omitting t for notational simplicity):\n\u02dcpi =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u03b3 \u00b7 pi\nP|Snovel|\nj=1\n\u03b3 \u00b7 pj + P|V\u2032|\nj=1 pj\n,\nif i \u2208Snovel\npi\nP|Snovel|\nj=1\n\u03b3 \u00b7 pj + P|V\u2032|\nj=1 pj\n,\notherwise\n(4)\nwhere V\u2032 = V \\ St\nnovel is the non-novel tokens set at step t\nand \u03b3 \u2208(0, 1) is the only hyper-parameter in our method\nthat controls to what degree we want to encourage the model\nto focus on novel tokens; a smaller value of \u03b3 incurs more\naggressive push for using novel tokens.\nThe effect of the above change is that we directly re-scale\nthe generation probability (after re-normalization) of the\ntokens. For i \u2208Snovel, the effective probability becomes\n\u02dcpi = \u03bbi \u00b7pi with \u03bbi \u2208(0, 1), and for i /\u2208Snovel, the effective\nprobability becomes \u02dcpi = \u03b1i \u00b7 pi with \u03b1i > 1.2 Since \u03bbi \u00b7 pi\nand \u03b1i \u00b7 pi are new re-normalized probabilities, they both\nare naturally bounded in [0, 1]. Consequently, assuming that\nthe ground truth token is indexed at k, the modi\ufb01ed loss\nfunction at step t for our proposed method becomes:\nLSG = \u2212\n|V|\nX\ni=1\n1(i = k)\nh\n1(i \u2208Snovel) log(\u03bbi \u00b7 pi)\n+ 1(i /\u2208Snovel) log(\u03b1i \u00b7 pi)\ni (5)\n2Note that \u03bbi and \u03b1i are functions of pi rather than constant\nnumbers. E.g., \u03bbi = \u03b3/(P|Snovel|\nj=1\n\u03b3 \u00b7 pj + P|V\u2032|\nj=1 pj).\n"], "summary": "The cited paper, \"Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation,\" focuses on addressing the issue of dull and repetitive text generation in neural language models, a problem that can arise from the Maximum Likelihood Estimation (MLE) objective. While my work tackles information under-representation and posterior collapse in VAEs for language generation, the cited paper's method, ScaleGrad, aims to improve generation quality by encouraging the model to use novel tokens through gradient manipulation. This contrasts with my approach of using discrete variational attention and an auto-regressive prior to enhance the latent space. Although our methods differ, both papers contribute to improving the quality and diversity of generated language, with the cited paper's focus on token-level probability manipulation offering a complementary perspective to my latent space and attention mechanism innovations.", "citation": "Hareesh Bahuleyan (2018). Natural Language Generation with Neural Variational Models. arXiv:1808.09012. https://arxiv.org/abs/1808.09012"}, {"paper_id": 47, "text": ["\u00b5-Forcing: Training Variational Recurrent Autoencoders for Text Generation\n3\n\u2022 We propose an effective method to address the uninformative latent variables problem for\nVRAEs. This method can flexibly and stably control the trade-off between the KL term and the\nreconstruction term, making the model learn dense and meaningful latent representations.\nFurthermore, our proposed method can perform well without using other strategies, such as\nKL annealing.\n\u2022 For sentence generation, the experiments indicate that our proposed method outperforms\nseveral strong baselines. We show that our method can generate diverse meaningful sentences\nand learn interpretable latent variables.\n2\nRELATED WORK\nWhen applied to text generation or complex datasets such as ImageNet [6], VAEs suffer from two\nmajor problems: blurry samples and uninformative latent variables. Lots of approaches have been\nproposed to address these issues. Our work falls into this category, but focuses on text generation\nwhere the second issue dominates.\nRecently, much work has been done to come up with more powerful posterior distributions.\n[21] learn highly non-Gaussian posterior densities by transforming simple densities into complex\nones with sequences of invertible transformations. [18] introduce generative adversarial networks\n(GAN) [9] to variational inference, which can match the posterior distribution with an arbitrary\nprior distribution. These methods have shown to be effective in improving variational inference\nand solving the blurry sample issue partially. However, as [3, 25] observed, these approaches seem\nto do little on the uninformative latent variables problem.\nThe uninformative latent variables issue is formally studied in [4], which casts the problem of\noptimizing VAE into designing an efficient coding scheme. Their work shed light on the reason\nof the uninformative latent variables problem from the perspective of coding theory, but without\nproposing any principle method to address it. [32] further formally study these problems of VAE,\nand propose a family of VAE based models. They demonstrate that all of these models maximize the\nmutual information between input and latent variables and achieve better performance on image\ngeneration.\nAs for sentence generation, recent attempts that use autoregressive conditional likelihood in VAEs\nsuffer from seriously uninformative latent variables issue. Existing solutions to this problem can be\ndivided into two categories: model-based and regularizer-based methods. For model-based methods,\n[23] proposes a novel hybrid convolutional-recurrent model (hybrid-VAE) with an additional\nauxiliary reconstruction term to address the uninformative latent variables issue for text generation.\nThis architecture is attractive for its computational efficiency but less flexible. [29] extends the\nhybrid-VAE by introducing dilated convolutions to improve the variational model for text generation.\n[10] proposes a stochastic recurrent model in which each step in the sequence is associated with\na latent variable. To ease the training, they add an auxiliary cost to force each latent variable to\nreconstruct the state of the backward recurrent network. In order to solve the KL vanishing and\ninconsistent training objective for dialogue generation, [24] firstly learns to autoencoder discrete\ntexts into continuous embeddings which are sampled by transforming Gaussian noise and are\ntrained with a separate model. Then the model learns to generalize latent representations by\nreconsturcting the encoded embedding.\nFor the regularizer-based method, [3] uses the KL cost annealing method to enforce the VRAEs\nto learn to encode as much information in latent variables as it can in the early stage of training.\nIn addition, they weaken the decoder by randomly removing some conditional information (the\nground-truth previous word) during training to force the model to rely on the latent variables.\nIn practice, these tricks are not always effective. Another popular strategy is free bits [16]. This\nmethod reserves some space of KL divergence for every dimension of latent variables. Similarly, [29]\nACM Trans. Asian Low-Resour. Lang. Inf. Process., Vol. 1, No. 1, Article . Publication date: May 2019.\n", "8\nLiu, D. et al\n(a) KL loss on COSR\n(b) Reconstruction loss on COSR\n(c) The distribution of \u00b5 values of baseline\non COSR\n(d) The distribution of \u00b5 values of \u00b5-\nForcing on COSR\n(e) KL loss on APRC\n(f) Reconstruction loss on APRC\n(g) The distribution of \u00b5 values of base-\nline on APRC\n(h) The distribution of \u00b5 values of \u00b5-\nForcing on APRC\nFig. 2. The subfigure 2(a)-2(d) presents the results on COSR test set while the subfigure 2(e)-2(h) presents the\nresults on APRC test set. The subfigure 2(a) and 2(e) shows the KL loss of \u00b5-Forcing and baseline on the test\nset. The subfigure 2(b) and 2(f) shows the reconstruction loss (NLL) of \u00b5-Forcing and baseline on the test set.\nThe distribution of \u00b5 values of baseline on the test set are shown in the subfigure 2(c) and 2(g). While the\ndistribution of \u00b5 values of \u00b5-Forcing on the test set are shown in subfigure 2(d) and 2(h) [best viewed in color].\nto zero on both test sets. As the KL term annealed, we can find that the reconstruction errors\nACM Trans. Asian Low-Resour. Lang. Inf. Process., Vol. 1, No. 1, Article . Publication date: May 2019.\n", "2\nLiu, D. et al\n1\nINTRODUCTION\nNatural language generation has been a popular research topic over the past decades. Unsupervised\nlearning plays an important role in this field. In unsupervised settings, the standard RNN-based\nmodels such as RNN-based language models [26] and sequence auto-encoders [5] generate each\nword of a sentence conditioned on its previous generated words and hidden state. However, they\ndo not explicitly include latent variables to capture meaningful latent features and represent the\nfull sentence. As discussed by [3], these RNN-based models do not generally learn smooth and\ninterpretable latent variables for sentence representation, which is often the main purpose of\nunsupervised learning. Their sentence encoding vectors cannot be used to sample novel sentences\nfor RNN decoders.\nAs one kind of generative model, Variational Autoencoders (VAEs) [17, 22], have shown great\npromise in image and text generation. The VAEs integrate stochastic latent variables z into the\nauto-encoder architecture. By imposing a prior standardized normal distribution on the latent\nvariables, the VAEs learn latent variables not as single isolated points, but as soft dense regions in\nlatent space which makes it be able to generate plausible examples from every point in the latent\nspace. The VAE models have been successfully used to generate plausible images [11, 28]. However,\nit often performs poorly on text generation.\nFor text generation, autoregressive density estimators such as LSTM RNNs [12], which are highly\nexpressive, are usually employed as the decoder parts of the VAE-based models. Such models are\ncalled Variational Recurrent Autoencoders (VRAEs) [8]. VRAEs tackle the problem of controlled\ngeneration of text. They are able to generate realistic sentence examples as if they are drawn from\nthe input data distribution by simply feeding noise vectors through the decoder. Additionally,\nthe latent representations obtained by applying the encoder to input examples give fine-grained\ncontrol over the generation process that is harder to achieve with more conventional autoregressive\nmodels. These latent variables make it possible to control various fine-grained attributes over the\ngeneration process, such as controlling the sentiment or writing style of generated sentences.\nNevertheless, VRAEs face some optimization challenges. As argued in [1, 3, 4, 32], the core\ndifficulty of training VRAEs is that the models would suffer from serious uninformative latent\nvariables (also called KL vanishing) issue: the VRAEs tend to totally ignore the latent variables\nand only use the decoder part to model the data. In practice, the VRAEs would collapse into plain\nlanguage models and can only generate repeating and dull samples.\nTo mitigate this uninformative latent variables problem, [3] propose a trick called KL cost\nannealing. However, the training of VRAEs still be prone to collapse on large corpus with this trick.\nAs pointed by [30], this hand-tuned method make the process of training VRAEs still difficult and\nare not very efficient.\nWe propose a regularizer based approach called \u00b5-Forcing to address the uninformative latent\nvariables problem. An additional regularizer is added to the objective function of original VRAE\nwhich prevents the VRAE collapses into a trivial solution and guides the VRAE to explore its model\ncapacity to learn a better latent representation. This method stems from the following intuition:\nwhen the model collapses into a trivial solution, the approximation posterior distribution q(z|x)\nof every data point x, which is usually assumed to be N(\u00b5,\u03c3 2), degrades to the same N(0, 1).\nHowever, for reasonable latent representations, different data points x should have different latent\nrepresentations q(z|x). The proposed method introduces a mild constraint on the \u00b5 of q(z|x) to\nforce the model to find a non-trivial solution where the learned latent variables z contain useful\ninformation.\nSpecifically, the contributions of this paper can be summarized as follows:\nACM Trans. Asian Low-Resour. Lang. Inf. Process., Vol. 1, No. 1, Article . Publication date: May 2019.\n", "4\nLiu, D. et al\nreserves space for the total KL divergence instead of for for every dimension. In order to solve the\nuninformative latent variables problem, [33] forces the latent variables to predict the bag-of-words\nvector of the reconstruction sentence. Nevertheless, this method needs to incorporate another\nneural network to predict the bag-of-words vector, which will significantly increase the number of\nparameters of the model. More recently, [1] presents a theoretical framework for understanding\nrepresentation learning using latent variable models in terms of the rate-distortion tradeoff, and\nconfirms that the VAE models with expressive decoders can ignore the latent code. They propose\na simple solution that reduces the KL penalty term \u03bb to \u03bb < 1 to this problem. However, their\nexperiments are based on image generation, and we experimentally find that when applying this\nmethod to VRAE model for text generation, we need to carefully adjust the value of \u03bb and employ\nthe KL cost annealing trick to avoid the KL vanishing problem.\nCompared with the model-based methods, the regularizer-based methods are more flexible\nand scalable. Our approach is also a regularizer-based method which directly injects constraint\non the posterior of latent variables. The experimental results demonstrate that the proposed\nmethod outperforms several regularizer-based baselines. In addition, our experiments show that the\nproposed method can be also applied to other model-based methods and improve the performance,\nsuch as hybrid-VAE [23].\n3\nVARIATIONAL RECURRENT AUTOENCODERS\nVAE framework is a neural network based method for training generative latent variable models\nwhich integrates stochastic latent variables z into the auto-encoder architecture. Let x be an\nobserved variable. Given a set of observed data points X = {x(1), ...,x(N )}, the goal is to estimate\nthe parameters \u03b8 that maximize the marginal log-likelihood:\nlogp\u03b8(X) =\nN\n\u00d5\nn=1\nlog\n\u222b\nz\np(z)p\u03b8(x(n)|z)dz.\nIn general, we assume that the prior distribution p(z) is normal distribution N(z; 0, I). Due to\nthe presence of integral in the marginal log-likelihood, it is intractable to directly compute or\ndifferentiate the marginal log-likelihood. A common solution is to optimize the evidence lower\nbound (ELBO) on the marginal log-likelihood by introducing an approximate posterior distribution\nq\u03d5(z|x):\nlogp\u03b8(x) \u2265Eq\u03d5(z |x)\nh\nlogp\u03b8(x|z)\ni\n\u2212DKL\n\u0000q\u03d5(z|x) \u2225p(z)\u0001.\n(1)\nHere two neural networks with parameter \u03d5 and \u03b8 are respectively employed for modeling the\nposterior distribution q\u03d5(z|x) and the conditional distribution p\u03b8(x|z). In general, we assume that\nq\u03d5(z|x) is multivariate diagonal Gaussian distribution:\nq\u03d5(z|x) = N \u0000z; \u00b5\u03d5(x), \u03a3\u03d5(x)\u0001.\nwhere \u00b5\u03d5 and \u03a3\u03d5 are implemented via neural networks with parameters\u03d5. However, samplingz from\nq\u03d5(z|x) is a non-continuous operation and has no gradient. The solution is the reparameterization\ntrick which first samples \u03f5 \u223cN(0, I), and then computes z = \u00b5\u03d5(x) + \u03a3\n1\n2\n\u03d5(x) \u2217\u03f5. Through this trick,\nACM Trans. Asian Low-Resour. Lang. Inf. Process., Vol. 1, No. 1, Article . Publication date: May 2019.\n"], "summary": "The cited paper, \"\u00b5-Forcing: Training Variational Recurrent Autoencoders for Text Generation,\" addresses the common problem of uninformative latent variables (posterior collapse) in VAEs for text generation, a core issue also tackled by your work. While your approach uses discrete variational attention and an autoregressive prior to enhance the latent space and avoid posterior collapse, the cited paper proposes a regularizer-based method called \u00b5-Forcing. This method adds a constraint to the objective function to prevent the VRAE from ignoring the latent variables, thereby encouraging the model to learn meaningful latent representations without relying on ad-hoc techniques like KL annealing. Both papers aim to improve the utility of latent variables in VAEs for language generation, with the cited paper focusing on a regularizer-based solution for continuous latent spaces, contrasting with your discrete attention mechanism.", "citation": "Alex Mansbridge, Roberto Fierimonte, Ilya Feige, David Barber (2018). Improving latent variable descriptiveness with AutoGen. arXiv:1806.04480. https://arxiv.org/abs/1806.04480"}, {"paper_id": 23, "text": ["Inspired by prior work indicating that anchor\nwords can effectively capture and control high-level\ngeneration structure, we investigate to what extent\nhigh-level control can be learned in a fully unsuper-\nvised fashion, directly from natural story data. We\ndesign a hierarchical latent variable model (Figure\n2) that induces sequences of anchor words that ex-\nplain observed stories, while at the same time learn-\ning to generate entire stories by \ufb01rst generating\nanchor sequences. For training, we use amortized\nvariational learning (Kingma and Welling, 2014),\nwhere an inference network is used to approximate\nthe posterior on anchor sequences.\nAt test time, given a title, we \ufb01rst sample a se-\nquence of anchor words using the prior model con-\nditioned on only the title, and then generate the\nactual story using the decoder conditioning only on\nthe title and the sampled anchor words.\nTo induce a useful latent generation plan and to\neffectively condition on a sampled plan, we pro-\npose a constrained story decoder and constrained\ninference network. Speci\ufb01cally, our constrained\ndecoder begins a story sentence by deterministic\ncopying the corresponding anchor word, and then\ngenerates words to the left and then to the right\n(Figure 3). For this decoder, the corresponding true\nposterior on anchor words is sparse: the anchor\nword must be chosen from the observed sentence.\nThus, we constrain the output vocabulary of the\ncorresponding inference network to the words of\nthe input sentence. We observe that the proposed\nconstrained inference network does not suffer from\nmode collapse, leading to models which can ef-\nfectively learn useful anchor words. Further, we\nalso contrast this approach with a model whose\ndecoder is not constrained to use each anchor word\nin each sentence. The true posterior in this case is\nover the full vocabulary. We conduct experiments\nwith both constrained and unconstrained decoders\nand inference networks, and \ufb01nd that the best re-\nsults are achieved through the combination of an\nunconstrained decoder with a constrained inference\nnetwork \u2013 indicating, perhaps, that while it is more\neffective to use \ufb02exible models, using a constrained\ninference network can add a useful inductive bias,\nleading the model to mimic the constraint of the\ninference network.\nWe experiment with two English story datasets,\nand observe that our best models achieve favorable\nscores relative to several baselines when evaluated\non perplexity, diversity, coherency, and control-\nFigure 2:\nModel Overview:\nWe consider multi-\nsentence text generation via a latent generation plan\nrealized through a sequence of anchor words with\none word per sentence. [We show sequence models\nwith \ufb01rst-order Markov assumption for simplicity, even\nthough all sequence models in our approach are auto-\nregressive with full context.]\nlable story generation as per various automatic and\nhuman evaluations.\nFinally, we note that our modelling approach\nfor story generation has an interesting connection\nwith work that treats text as a latent variable in\ndeep generative models (Miao and Blunsom, 2016;\nWen et al., 2017). We treat a latent sequence of\nanchor words as a form of hierarchical control over\ngenerated outputs, while related work treats the\nlatent sequence itself as sequential text that is the\noutput of the model.\n2\nModel\nOur goal is to generate a story x, consisting of mul-\ntiple sentences x1, x2, ..xK, given a title t. Our\nmodel\u2019s generative process is depicted in Figure 2\nand operates as follows: First, a sequence of anchor\nwords representing a generation plan is sampled\nfrom an auto-regressive prior conditioned on the\ntitle. Next, for each anchor word, a sentence is\ngenerated conditioned on the anchor words and\npreviously generated sentences using a decoder.\nDuring training, the sequence of anchor words is\nunobserved and treated as a latent variable. As de-\nscribed in more detail later, we will explore several\nchoices of decoder \u2013 those that treat anchor words\nas an explicit token in the sentence to be generated,\n", "Method\nInference N/W\nDecoder\nPPL\u2193\nNLL\u2193\nDIV\u2191\nDIV-B\u2193\ntest\ntest\ndev\nplan\nstory\nstory\nNo Plan\nROC-DATA\nNA\nNA\nNA\nNA\nNA\nNA\n9.01\n0.23\nNOPLAN-LM\nNA\nUnconstrained\n17.3\n154.0\n160.7\nNA\n7.70\n0.50\nWith Plan\nSUPERVPLAN\nNA 1\nUnconstrained\n\u226428.3\n\u2264180.3\n\u2264187.6\n8.71\n7.74\n0.49\nLAP-CINF-UDEC\nConstrained\nUnconstrained\n\u226421.3\n\u2264168.9\n\u2264176.5\n9.24\n7.93\n0.45\nLAP other variants:\nLAP-CINF-CDEC\nConstrained\nConstrained\n\u226420.9\n\u2264166.9\n\u2264174.1\n9.24\n7.98\n0.44\nLAP-UINF-UDEC\nUnconstrained\nUnconstrained\n\u226417.5\n\u2264154.2\n\u2264160.9\n0.01\n7.67\n0.52\nTable 1: Automated metrics: We report Negative Log Likelihood (NLL), perplexity (PPL) (computed using im-\nportance weighted samples for models with latent variables), and diversity (DIV and DIV-B). LAP-CINF-UDEC\nperforms better than SUPERVPLAN on perplexity as well as diversity. We also experiment with two other vari-\nants for LAP. LAP-UINF-UDEC, which does not constrain the inference network, suffers from posterior collapse.\nLAP-CINF-CDEC, which uses the constrained decoder, achieves perplexity and diversity results that are compara-\nble to LAP-CINF-UDEC.\nwith our model. To do this, we separately train\nan inference network (with the same architecture\nas that used by the LAP-CINF-UDEC model) to\napproximate the posterior on anchor words for\nthe trained SUPERVPLAN (by keeping the trained\nmodel parameters \ufb01xed). This approximate pos-\nterior is then used to compute an upper bound on\nNLL and perplexity.\nThe proposed model LAP-CINF-UDEC per-\nforms better than the baseline SUPERVPLAN,\nwhich uses separately tagged generation plans (Ta-\nble 1). However, the proposed method\u2019s perplexity\nis close to that of NOPLAN-LM, which does not\nconsider any generation plan. This is not uncom-\nmon for deep latent variable models \u2013 since their\nheld-out likelihood is intractable, and most approx-\nimations yield upper bounds on perplexity, their\nreported perplexity is always pessimistic. Among\nLAP variants, we observe that LAP-UINF-UDEC\nsuffers from posterior collapses, and behaves simi-\nlarly to NOPLAN-LM since the latent variables z\nare not informative or useful. Finally, LAP-CINF-\nCDEC performs similar on likelihood evaluations\ncompared to the LAP-CINF-UDEC model with an\nunconstrained decoder .\n4.4\nDiversity\nWe generate story samples for all the titles in the\ntest split. We employ two evaluations to report di-\nversity in the generated outputs:\nDIV We compute the geometric mean of empiri-\ncal unigram, bigram, and trigram distribution en-\ntropy from the generated set of stories (Jhamtani\net al., 2018). For methods which use generation\nplans, we also compute this diversity metric on an-\nchor word sequences. Table 1 shows the results for\nvarious models. LAP-CINF-UDEC performs bet-\nter than SUPERVPLAN, achieving higher diversity\nfor both story and plans. Among the LAP vari-\nants, using the non-constrained inference network\n(LAP-UINF-UDEC) leads to worse results on story\ndiversity, and fares poorly in plan diversity (due to\nposterior collapse). LAP-CINF-CDEC again per-\nforms similarly to LAP-CINF-UDEC.\nDIV-B We also report inter-story BLEU4 scores\n(Zhu et al., 2018). We compute samples from var-\nious methods for 1000 titles in the test split. For\neach generated story, the remaining 999 are treated\nas references. Thus, lower values indicate higher\ndiversity in the generated stories. Table 1 shows\nthe results. LAP-CINF-UDEC performs better than\nSUPERVPLAN, though is still far from the values\nfor human written stories in the ROC dataset itself.\n4.5\nHuman Evaluations\nWe conduct human evaluations on Amazon Me-\nchanical Turk to evaluate the quality of generated\nstories given the title. We evaluate the story sam-\nples with respect to: (1) coherence, which mea-\nsures the logical and coherent narrative \ufb02ow in a\nstory, and (2) \ufb01delity to title, which measures the\ndegree to which the story is relevant to the given\ntitle. Given two stories from two different meth-\nods, we request human annotators to provide their\npreference (or mark as tie).\n1We retro\ufb01t an inference network to a trained SUPERV-\nPLAN to approximate PPL and NLL for evaluation purposes\nonly. Training the SUPERVPLAN model does not involve any\n", "Figure 3: Simpli\ufb01ed demonstration of generation of a sentence conditioned on anchor words and preceding sen-\ntences for the two types of decoders: (1) Unconstrained decoder is based on the story generation model of (Yao\net al., 2019), which may or may not use the corresponding anchor word. (2) Constrained decoder is forced to use\nanchoring words in corresponding sentences, generating words to the left and then to the right of an anchor word.\n[Again, we show sequence models with a \ufb01rst-order Markov assumption for simplicity, even though all sequence\nmodels are auto-regressive with full context. ]\ngenerating surrounding context to the left and right,\nand those that simply treat the anchor words as\nconditioning information. In the former case, the\nposterior must be sparse. In the latter case, our\nchoice of variational learning scheme will bias (but\nnot force) the model to use anchor words in output\nstory sentences. We shall refer to our proposed\nmodel as Latent Anchor Plan model ( LAP).\n2.1\nAnchor Sequence Prior\nWe model the sequence of anchor words repre-\nsenting the generation plan via a sequence of dis-\ncrete random variables z1, z2, .., zK. Since our aim\nis to induce latent plans, we assume z are unob-\nserved. We consider an auto-regressive prior model\np\u03c6(z|t) = Q\ni p\u03c6(zi|z<i, t) where each anchor\nword is conditioned on preceding anchor words\nand the title t.\n2.2\nStory Decoder\nOur decoder p\u03b8(x|t, z) generates a story given the\ntitle t and anchor words z. As mentioned earlier,\nzi is aligned to the sentence xi. We consider two\ndecoders: (1) an unconstrained decoder which is\nnot bound to use zi in xi, and (2) a constrained\ndecoder which assumes zi is present in xi, and\nconstructs words to the left and then to the right of\nthe anchor word zi.\nUnconstrained Decoder:\nOur unconstrained\ndecoder is based on Yao et al. (2019)\u2019s decoder\nwhich does not use any explicit alignment of\nanchor words to corresponding sentences (Figure\n3). The decoder is fed the title and the anchor\nwords appended together, and is trained to generate\nthe multi-sentence text. The decoder is not bound\nto use the anchor word zi for xi, but may have\nincentive to do so depending on the training\nFigure 4: Constrained Inference Network: Proposed\nmodel is trained through amortized variational learn-\ning using an inference network. One of the proposed\nmodels is trained using a constrained inference network\nwhich assigns non-zero probability to only the words\npresent in corresponding sentences.\nobjective, as discussed later. At the same time, the\nunconstrained decoder has higher \ufb02exibility and\ncan skip using an anchor word if it doesn\u2019t \ufb01t with\nthe preceding context.\nConstrained Decoder: We consider a constrained\ndecoder that always uses zi while generating xi.\nThis is achieved by \ufb01rst copying zi, then gener-\nating to the left until the sentence start, and then\nto the right. Such a decoder is bound to use the\ncorresponding anchor word by design, and will po-\ntentially demonstrate higher control of the anchor\nwords on the story.\nOur decoder architecture follows from Yao et al.\n(2019), who use a 3-layer LSTM recurrent model.\nOur \ufb01nal reported model uses 1000 dimensional\nhidden layer, with tied input and output word em-\nbeddings. Moreover, the prior model shares the\nunderlying LSTM modules with the decoder. Since\nour goal is to induce a latent discrete plan and com-\npare with keyword tagging based methods, we stick\nto the same choice of decoder as in prior work.\n", "Narrative Text Generation with a Latent Discrete Plan\nHarsh Jhamtani 1\nTaylor Berg-Kirkpatrick 2\n1 School of Computer Science, Carnegie Mellon University\n2 Computer Science and Engineering. University of California San Diego\njharsh@cs.cmu.edu, tberg@ucsd.eng.edu\nAbstract\nPast work on story generation has demon-\nstrated the usefulness of conditioning on a gen-\neration plan to generate coherent stories. How-\never, these approaches have used heuristics or\noff-the-shelf models to \ufb01rst tag training sto-\nries with the desired type of plan, and then\ntrain generation models in a supervised fash-\nion. In this paper, we propose a deep latent\nvariable model that \ufb01rst samples a sequence\nof anchor words, one per sentence in the story,\nas part of its generative process. During train-\ning, our model treats the sequence of anchor\nwords as a latent variable and attempts to in-\nduce anchoring sequences that help guide gen-\neration in an unsupervised fashion. We con-\nduct experiments with several types of sen-\ntence decoder distributions \u2013 left-to-right and\nnon-monotonic, with different degrees of re-\nstriction. Further, since we use amortized vari-\national inference to train our model, we in-\ntroduce two corresponding types of inference\nnetwork for predicting the posterior on anchor\nwords. We conduct human evaluations which\ndemonstrate that the stories produced by our\nmodel are rated better in comparison with base-\nlines which do not consider story plans, and\nare similar or better in quality relative to base-\nlines which use external supervision for plans.\nAdditionally, the proposed model gets favor-\nable scores when evaluated on perplexity, di-\nversity, and control of story via discrete plan.\n1\nIntroduction\nMaintaining long-term narrative \ufb02ow and consis-\ntency are important concerns when aiming to gener-\nate a plausible story (Porteous and Cavazza, 2009;\nHou et al., 2019). Prior work on narrative text gen-\neration has focused on generating consistent stories\nvia story outlines using keywords or key phrases\n(Xu et al., 2018; Yao et al., 2019), event-based\nrepresentations (Riedl and Young, 2010; Martin\net al., 2018; Fan et al., 2019), plot graphs (Li et al.,\nFigure 1: Our aim is to generate a story given a title.\nWe propose models which \ufb01rst generate a high level\nstory plan realized via a sequence of anchor words.\n2013) or a sentence representing theme (Chen et al.,\n2019).\nYao et al. (2019) note that compared to speci\ufb01c\nevent based representations, using keywords to\nform the outline is more generalizable and widely\napplicable. In this work, we consider a sequence\nof anchor words as a means to model story out-\nlines. For example, in Figure 1, given a story title\n\u2018Winning the Race\u2019, our model \ufb01rst predicts a se-\nquence of anchor words which represents a high\nlevel story plan. Thereafter, a decoder conditions\non the title and generated sequence of anchor words\nto generate the \ufb01nal story. We assume an alignment\nbetween the anchor words and the story sentences \u2013\nthe ith anchor word corresponds to the ith sentence\nin the story.\nHowever, stories do not naturally occur with\na tagged set of such anchor words or keywords.\nMany prior works use off the shelf tools to \ufb01rst\nlabel stories with plan outlines, thus using external\nsupervision for learning plot structures. For exam-\nple, Yao et al. (2019) use the RAKE heuristic (Rose\net al., 2010) to \ufb01rst identify the most important key-\nword in each sentence, and then use this to train a\nmodel in a supervised fashion. This approach leads\nto improved coherency and control, but creates a re-\nliance on such heuristics and does not jointly learn\nanchor words along with the generator.\narXiv:2010.03272v1  [cs.CL]  7 Oct 2020\n"], "summary": "Jhamtani et al. (2020) propose a deep latent variable model for story generation that learns a sequence of discrete anchor words as a latent plan, similar to your use of discrete latent variables for language generation. Their work also addresses the challenge of inducing useful latent spaces in an unsupervised fashion, which aligns with your goal of enhancing the latent space for language generation. Specifically, they explore different decoder and inference network constraints to improve the utility of their discrete latent plan, and note that an unconstrained inference network can suffer from posterior collapse, a problem your work aims to avoid through discreteness. While their focus is on story generation with discrete anchor words, their exploration of discrete latent variables and their impact on model performance, including issues like posterior collapse, provides a relevant comparison point for your research.", "citation": "Ond\u0159ej C\u00edfka, Aliaksei Severyn, Enrique Alfonseca, Katja Filippova (2018). Eval all, trust a few, do wrong to none: Comparing sentence generation models. arXiv:1804.07972. https://arxiv.org/abs/1804.07972"}, {"paper_id": 40, "text": ["automatic evaluation metrics and human evalua-\ntions\n(Wu et al., 2016).\nThere are also stud-\nies focusing on text generation from structured\ndata such as SQL-to-text (Xu et al., 2018). Pre-\nvious pre-training for text generation is usually\ndone by independently pre-training encoder-side\nor decoder-side language models (Ramachandran\net al., 2016). Concurrent to our work,\nEdunov\net al. augment encoder representation with ELMo-\nstyle models, MASS (Song et al., 2019) masks\ncontinuous text fragments for pre-training, and\nUNILM (Dong et al., 2019) proposes to pre-train\nfor both language understanding and generation\ntasks.\n6\nConclusion\nThis paper presents a new transfer learning ap-\nproach for seq2seq text generation named PoDA.\nIt involves two steps: \ufb01rst, pre-train a customized\nseq2seq denoising autoencoder on large-scale un-\nlabeled text corpora; then, \ufb01ne-tune on in-domain\nlabeled data. The pre-training step is independent\nof downstream tasks and jointly learns both en-\ncoder and decoder representations. PoDA is sim-\nple, intuitive and doesn\u2019t require changing net-\nwork architecture during the \ufb01ne-tuning stage. Ex-\nperiments on several abstractive summarization\nand grammatical error correction datasets demon-\nstrate that PoDA leads to better performance and\nfaster convergence.\nFor future work, we would like to validate our\nmodel on other tasks such as response generation,\nexplore more effective unsupervised sequence-to-\nsequence pre-training methods, and handle cross-\nlingual tasks such as machine translation.\nAcknowledgments\nWe want to thank three anonymous reviewers for\ntheir valuable comments, and EMNLP-IJCNLP\n2019 organizers for their efforts.\nReferences\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and\nHugo Larochelle. 2007. Greedy layer-wise training\nof deep networks. In Advances in neural informa-\ntion processing systems, pages 153\u2013160.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632\u2013642.\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\n2018a. Retrieve, rerank and rewrite: Soft template\nbased neural summarization. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 152\u2013161.\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li.\n2018b. Faithful to the original: Fact aware neural\nabstractive summarization. In Thirty-Second AAAI\nConference on Arti\ufb01cial Intelligence.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nShamil Chollampatt and Hwee Tou Ng. 2018a. A mul-\ntilayer convolutional encoder-decoder neural net-\nwork for grammatical error correction.\narXiv\npreprint arXiv:1801.08831.\nShamil Chollampatt and Hwee Tou Ng. 2018b. Neural\nquality estimation of grammatical error correction.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2528\u20132539.\nGeorge E Dahl, Dong Yu, Li Deng, and Alex Acero.\n2012.\nContext-dependent pre-trained deep neural\nnetworks for large-vocabulary speech recognition.\nIEEE Transactions on audio, speech, and language\nprocessing, 20(1):30\u201342.\nDaniel Dahlmeier and Hwee Tou Ng. 2012.\nBetter\nevaluation for grammatical error correction. In Pro-\nceedings of the 2012 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n568\u2013572. Association for Computational Linguis-\ntics.\nDaniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.\n2013. Building a large annotated corpus of learner\nenglish: The nus corpus of learner english. In Pro-\nceedings of the eighth workshop on innovative use\nof NLP for building educational applications, pages\n22\u201331.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\nJun Deng, Zixing Zhang, Erik Marchi, and Bjorn\nSchuller. 2013.\nSparse autoencoder-based feature\ntransfer learning for speech emotion recognition. In\n2013 Humaine Association Conference on Affective\nComputing and Intelligent Interaction, pages 511\u2013\n516. IEEE.\n", "(Lin, 2004) or BLEU (Papineni et al., 2002), be-\ncause it may over\ufb01t evaluation metrics and barely\nshow improvements in human evaluations\n(Wu\net al., 2016).\n3\nExperiments\n3.1\nSetup\nThe network architecture used by our experiments\nhas 97 million parameters. It consists of 6 lay-\ners of encoder blocks, 6 layers of decoder blocks,\nand 1 pointer-generator layer.\nThe hidden size\nof each positionwise feedforward layer is 4096.\nWe use 8 heads for all multi-head attention lay-\ners. The vocabulary consists of the top 50k most\nfrequent words (case sensitive), and the dimension\nof word embedding is 512. We tie the parame-\nters of encoder word embeddings, decoder word\nembeddings, and the output softmax layer. NAG\n(Nesterov Accelerated Gradient) optimizer is used\nwith initial learning rate 2 \u00d7 10\u22123. Dropout of 0.2\nis applied for all self-attention layers, positionwise\nfeedforward layers and input embedding layers.\nThe gradient norm is clipped to have a maximum\nvalue of 2. We follow the Transformer implemen-\ntation from fairseq4.\nFor task-speci\ufb01c \ufb01ne-tuning, unless explicitly\nspeci\ufb01ed, we reuse the hyperparameters from the\npre-training stage. After each training epoch, we\ncompute the validation loss and halve the learning\nrate whenever the validation loss stops decreasing.\nThe training procedure terminates if the learning\nrate drops below 10\u22124. Exponential moving av-\nerage (EMA) with decay rate 0.9995 is used to\nmake the training stabilized. At inference time,\nwe use standard beam search decoding based on\nthe length-normalized log-likelihood. For ensem-\nble models, we use different random seeds and\npre-trained checkpoints for \ufb01ne-tuning. Ensemble\ndecoding is used by averaging the output probabil-\nities from different models at every decoding step.\nWhen reporting experimental results, \u201cPoDA\nw/o pre-training\u201d refers to the proposed architec-\nture in Section 2.1 trained only on the supervised\ndata, and \u201cPoDA w/o \ufb01ne-tuning\u201d only pre-trains\non unlabeled data. PoDA \ufb01rst pre-trains a denois-\ning autoencoder and then \ufb01ne-tunes on the super-\nvised data.\n4https://github.com/pytorch/fairseq\n3.2\nAbstractive Summarization\nDatasets\nWe use two summarization datasets:\nCNN/Daily Mail5 (See et al., 2017) and Gigaword\n(Rush et al., 2015) dataset. The of\ufb01cial split for\ntraining, validation, and test is shown in Table 2.\nCorpus\n# of examples\ntrain\nvalid\ntest\nCNN/Daily Mail\n287, 113\n13, 368\n11, 490\nGigaword\n3, 803, 957\n189, 651\n1, 951\nTable 2: Dataset statistics for abstractive summariza-\ntion.\nThe CNN/Daily Mail dataset contains approxi-\nmately 300k news articles with an average of 781\nwords for each article, and each article is paired\nwith summaries with 56 words on average. We use\nthe preprocessing script6 provided by See et al.\n(2017). The articles are truncated to 800 words\nfor both training and testing. The summaries are\ntruncated to 130 words for training.\nThe Gigaword is a headline-generation dataset\nconsisting of nearly 4 million examples. Headline\ngeneration can be seen as a sentence summariza-\ntion task. Each example in Gigaword consists of\none sentence with an average length of 31.3 words,\nwhich is much shorter than CNN/Daily Mail, and\none short headline with an average length of 8.3\nwords. The Gigaword dataset provided by Rush\net al. (2015) is already tokenized and lower-cased.\nSince our vocabulary is case-sensitive, such incon-\nsistency is expected to hurt our system\u2019s perfor-\nmance.\nEvaluation We report evaluation results in terms\nof of ROUGE-1, ROUGE-2 and ROUGE-L (Lin,\n2004) using the pyrouge7 package.\nFor the\nCNN/Daily Mail dataset, PGNet\n(See et al.,\n2017), Lead3 (See et al., 2017), rnn-ext + RL (?),\nNeuSum (Zhou et al., 2018) are used as baselines.\nFor the Gigaword dataset, ABS+\n(Rush et al.,\n2015), CGU (Lin et al., 2018), FTSum (Cao et al.,\n2018b), and Re3Sum (Cao et al., 2018a) are used\nas baselines.\nResults for CNN/Daily Mail\nConsidering the\ncharacteristics of news articles, baselines such as\nLead3 (simply choose the \ufb01rst 3 sentences) can\nachieve strong performance in terms of ROUGE\n5We use the non-anonymized version, which is considered to\nbe more realistic.\n6https://github.com/abisee/cnn-dailymail\n7https://github.com/andersjo/pyrouge\n", "Denoising based Sequence-to-Sequence Pre-training for Text Generation\nLiang Wang1, Wei Zhao1, Ruoyu Jia1, Sujian Li2, Jingming Liu1\n1Yuanfudao AI Lab, Beijing, China\n2Key Laboratory of Computational Linguistics, Peking University, MOE, China\n{wangliang01,zhaowei01,jiary,liujm}@fenbi.com\nlisujian@pku.edu.cn\nAbstract\nThis\npaper\npresents\na\nnew\nsequence-to-\nsequence (seq2seq) pre-training method PoDA\n(Pre-training of Denoising Autoencoders),\nwhich learns representations suitable for text\ngeneration tasks. Unlike encoder-only (e.g.,\nBERT) or decoder-only (e.g., OpenAI GPT)\npre-training approaches, PoDA jointly pre-\ntrains both the encoder and decoder by denois-\ning the noise-corrupted text, and it also has the\nadvantage of keeping the network architecture\nunchanged in the subsequent \ufb01ne-tuning stage.\nMeanwhile, we design a hybrid model of\nTransformer and pointer-generator networks\nas the backbone architecture for PoDA. We\nconduct experiments on two text generation\ntasks: abstractive summarization, and gram-\nmatical error correction.\nResults on four\ndatasets show that PoDA can improve model\nperformance over strong baselines without us-\ning any task-speci\ufb01c techniques and signi\ufb01-\ncantly speed up convergence. 1\n1\nIntroduction\nMethods based on unsupervised pre-training and\nsupervised \ufb01ne-tuning for NLP have achieved phe-\nnomenal successes in the last two years. Most of\nthe proposed methods in the literature choose lan-\nguage modeling or its variant as the pre-training\ntask.\nAfter the pre-training stage, ELMo\n(Pe-\nters et al., 2018) and CoVe\n(McCann et al.,\n2017) directly use the learned representations as\nadditional features for downstream tasks, while\nBERT (Devlin et al., 2018), ULMFiT (Howard\nand Ruder, 2018), XLM (Lample and Conneau,\n2019), and OpenAI GPT (Radford et al., 2018,\n2019) require \ufb01ne-tuning both pre-trained param-\neters and task-speci\ufb01c parameters on labeled data.\nThe state-of-the-art performances have been sig-\nni\ufb01cantly advanced for classi\ufb01cation and sequence\n1The code and pre-trained models are available at https:\n//github.com/yuantiku/PoDA.\nlabeling tasks, such as natural language inference\n(Bowman et al., 2015), named-entity recognition,\nSQuAD question answering\n(Rajpurkar et al.,\n2016) etc.\nHowever, little attention has been paid to pre-\ntraining for seq2seq text generation\n(Sutskever\net al., 2014).\nA typical seq2seq network con-\nsists of a bidirectional encoder, a unidirectional\ndecoder and attention between the encoder and de-\ncoder. Previous work mainly focuses on encoder-\nonly or decoder-only pre-training. For example,\nBERT pre-trains a bidirectional encoder, and Ope-\nnAI GPT pre-trains a language model which is es-\nsentially a unidirectional decoder. Ramachandran\net al. (2016) propose to train two independent lan-\nguage models for the encoder and decoder respec-\ntively. All of the aforementioned methods are only\nable to partially pre-train the seq2seq networks,\nand therefore are unable to unleash the full poten-\ntial of transfer learning for text generation.\nIn this paper, we present PoDA, a denoising\nbased pre-training method that is able to jointly\npre-train all components of seq2seq networks.\nLike denoising autoencoders, PoDA works by de-\nnoising the noise-corrupted text sequences. Any\nnoising function that \ufb01ts in the seq2seq frame-\nwork can be used. We experiment with three types\nof noises: randomly shuf\ufb02e, delete or replace the\nwords in a given sequence. It is noted PoDA is\nsimple, easy-to-implement and applicable to virtu-\nally all seq2seq architectures, including ConvS2S\n(Gehring et al., 2017) and Transformer (Vaswani\net al., 2017).\nHere, we adopt the hybrid archi-\ntecture of Transformer and pointer-generator net-\nworks (See et al., 2017). Transformer is effective\nat modeling long-distance dependencies, highly\nparallelizable and demonstrates good performance\nempirically.\nPointer-generator network incorpo-\nrates copying mechanism (Gu et al., 2016; Gul-\ncehre et al., 2016) which is helpful for most text\narXiv:1908.08206v1  [cs.CL]  22 Aug 2019\n", "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nLi Dong,\nNan Yang,\nWenhui Wang,\nFuru Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou,\nand Hsiao-Wuen Hon. 2019.\nUni\ufb01ed\nlanguage model pre-training for natural language\nunderstanding and generation.\narXiv preprint\narXiv:1905.03197.\nSergey Edunov, Alexei Baevski, and Michael Auli.\n2019.\nPre-trained language model representa-\ntions for language generation.\narXiv preprint\narXiv:1903.09722.\nDumitru Erhan, Yoshua Bengio, Aaron Courville,\nPierre-Antoine Manzagol, Pascal Vincent, and Samy\nBengio. 2010. Why does unsupervised pre-training\nhelp deep learning?\nJournal of Machine Learning\nResearch, 11(Feb):625\u2013660.\nMariano Felice, Zheng Yuan, \u00d8istein E Andersen, He-\nlen Yannakoudakis, and Ekaterina Kochmar. 2014.\nGrammatical error correction using hybrid systems\nand type \ufb01ltering.\nIn Proceedings of the Eigh-\nteenth Conference on Computational Natural Lan-\nguage Learning: Shared Task, pages 15\u201324.\nMarkus Freitag and Scott Roy. 2018.\nUnsupervised\nnatural language generation with denoising autoen-\ncoders. arXiv preprint arXiv:1804.07899.\nTao Ge, Furu Wei, and Ming Zhou. 2018a. Fluency\nboost learning and inference for neural grammatical\nerror correction. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1055\u20131065.\nTao Ge, Furu Wei, and Ming Zhou. 2018b. Reaching\nhuman-level performance in automatic grammatical\nerror correction: An empirical study. arXiv preprint\narXiv:1807.01270.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\ntional sequence to sequence learning. arXiv preprint\narXiv:1705.03122.\nRoman Grundkiewicz and Marcin Junczys-Dowmunt.\n2018. Near human-level performance in grammati-\ncal error correction with hybrid machine translation.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), volume 2, pages 284\u2013290.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016.\nIncorporating copying mechanism in\nsequence-to-sequence learning.\narXiv preprint\narXiv:1603.06393.\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,\nBowen Zhou, and Yoshua Bengio. 2016. Pointing\nthe unknown words. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1,\npages 140\u2013149.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data.\nIn Proceedings of NAACL-\nHLT, pages 1367\u20131377.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 328\u2013339.\nMarcin Junczys-Dowmunt,\nRoman Grundkiewicz,\nShubha Guha, and Kenneth Hea\ufb01eld. 2018.\nAp-\nproaching neural grammatical error correction as a\nlow-resource machine translation task. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), volume 1, pages 595\u2013606.\nYunsu Kim, Jiahui Geng, and Hermann Ney. 2018.\nImproving unsupervised word-by-word translation\nwith language model and denoising autoencoder.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n862\u2013868.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66\u201371.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nJiwei Li, Thang Luong, and Dan Jurafsky. 2015.\nA\nhierarchical neural autoencoder for paragraphs and\ndocuments.\nIn Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), volume 1, pages 1106\u20131115.\nChin-Yew Lin. 2004.\nRouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nJunyang Lin, Xu Sun, Shuming Ma, and Qi Su.\n2018. Global encoding for abstractive summariza-\ntion. arXiv preprint arXiv:1805.03989.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294\u20136305.\n"], "summary": "The cited paper, \"Denoising based Sequence-to-Sequence Pre-training for Text Generation\" by Wang et al. (2019), proposes PoDA, a denoising autoencoder approach for pre-training seq2seq models for text generation. While your work focuses on discrete variational attention models to address information under-representation and posterior collapse in VAEs for language generation, PoDA offers a general pre-training strategy for seq2seq architectures, including those that might incorporate VAE components. Specifically, PoDA's joint pre-training of encoder and decoder for text generation contrasts with your VAE-based approach, which focuses on enhancing latent space representation and addressing VAE-specific training challenges. Therefore, PoDA could potentially serve as a pre-training method for the seq2seq backbone of your proposed discrete variational attention model, offering a complementary approach to improve overall generation performance.", "citation": "Zhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu, Tianyang Zhan, Gholamreza Haffari (2022). Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. arXiv:2202.13363. https://arxiv.org/abs/2202.13363"}, {"paper_id": 30, "text": ["OPTIMUS: Organizing Sentences via\nPre-trained Modeling of a Latent Space\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, Jianfeng Gao\nMicrosoft Research, Redmond\n{chunyl, xiag, v-liyua, bapeng, xiul, yizzhang, jfgao}@microsoft.com\nAbstract\nWhen trained effectively, the Variational Au-\ntoencoder (VAE) (Kingma and Welling, 2013;\nBowman et al., 2016) can be both a powerful\ngenerative model and an effective representa-\ntion learning framework for natural language.\nIn this paper, we propose the \ufb01rst large-scale\nlanguage VAE model OPTIMUS 1. A univer-\nsal latent embedding space for sentences is\n\ufb01rst pre-trained on large text corpus, and then\n\ufb01ne-tuned for various language generation and\nunderstanding tasks. Compared with GPT-2,\nOPTIMUS enables guided language generation\nfrom an abstract level using the latent vectors.\nCompared with BERT, OPTIMUS can gener-\nalize better on low-resource language under-\nstanding tasks due to the smooth latent space\nstructure. Extensive experimental results on\na wide range of language tasks demonstrate\nthe effectiveness of OPTIMUS.\nIt achieves\nnew state-of-the-art on VAE language model-\ning benchmarks.\n1\nIntroduction\nPre-trained language models (PLMs) have substan-\ntially advanced the state-of-the-art across a variety\nof natural language processing (NLP) tasks (Peters\net al., 2018; Devlin et al., 2019; Yang et al., 2019;\nRadford et al., 2019; Liu et al., 2019; Keskar et al.,\n2019; Shoeybi et al., 2019). PLMs are often trained\nto predict words based on their context on massive\ntext data, and the learned models can be \ufb01ne-tuned\nto adapt to various downstream tasks.\nPLMs can generally play two different roles: (i)\na generic encoder such as BERT (Devlin et al.,\n2019) to provide contextualized representations for\nlanguage understanding tasks, and (ii) a powerful\ndecoder such as GPT-2 (Radford et al., 2019) to\ngenerate text sequences in an auto-regressive man-\nner. In a bid to combine language understanding\n1Organizing sentences via Pre-Trained Modeling of a\nUniversal Space\nand generation tasks in one uni\ufb01ed framework, sev-\neral model variants have been proposed, including\nUniLM (Dong et al., 2019), BART (Lewis et al.,\n2019), and T5 (Raffel et al., 2019). Although signif-\nicant performance improvement has been reported\non a wide range of NLP tasks, these models lack\nof explicit modeling of structures in a compact la-\ntent space, rendering it dif\ufb01cult to control language\ngeneration/representation from an abstract level.\nVariational Autoencoders (VAEs) (Kingma and\nWelling, 2013; Rezende et al., 2014) provide a\ntractable method to train latent-variable genera-\ntive models. In NLP, latent variables may assume\nthe role of higher-level sentence representations,\nwhich govern a lower-level word-by-word genera-\ntion process, thus facilitating controlled text gener-\nation (Bowman et al., 2016; Hu et al., 2017). By\nrepresenting sentences in a low-dimensional latent\nspace, VAEs allow easy manipulation of sentences\nusing the corresponding compact vector represen-\ntations, such as feature regularization speci\ufb01ed by\nprior distributions, and guided sentence generation\nwith interpretable vector operators. Despite the at-\ntractive theoretical strengths, the current language\nVAEs are often built with shallow network archi-\ntectures, such as two-layer LSTMs (Hochreiter and\nSchmidhuber, 1997). This limits the model\u2019s ca-\npacity and leads to sub-optimal performance.\nIn this paper, we propose OPTIMUS, the \ufb01rst\nlarge-scale pre-trained deep latent variable models\nfor natural language. OPTIMUS is pre-trained using\nthe sentence-level (variational) auto-encoder objec-\ntives on large text corpus. This leads to a universal\nlatent space to organize sentences (hence named\nOPTIMUS). OPTIMUS enjoys several favorable\nproperties: (i) It combines the strengths of VAE,\nBERT and GPT, and supports both natural language\nunderstanding and generation tasks. (ii) Compar-\ning to BERT, OPTIMUS learns a more structured\nsemantic space due to the use of the prior distri-\narXiv:2004.04092v4  [cs.CL]  11 Oct 2020\n", "Encoder\nz\nx\nDecoder\nx\nh[CLS]\nh\nWD\nWE\nWM\n/\n\u2713\nBERT\n\u03c6\nGPT-2\nFigure 1: Illustration of OPTIMUS architecture.\n\u2022 VAE. The full VAE objective is considered\n(\u03b2 > 0). It tends to learn a smooth latent\nspace due to LR.\nInformation Bottleneck Principle\nFrom an in-\nformation theory perspective, information bottle-\nneck (IB) provides a principled approach to \ufb01nd the\ntrade-off between predictive power and complexity\n(compactness) when summarizing observed data in\nlearned representations. We show that our OPTI-\nMUS pre-training objectives effectively practice the\nIB principle as follows.\nThe objective in (4) shows the \u03b2-VAE loss for\none single sentence x. The training objective over\nthe dataset q(x) can be written as:\nF\u03b2 = \u2212FE + \u03b2FR\n(7)\nwhere FE = Eq(x),z\u223cq(z|x)[log p(\u02dcx|z)] is the ag-\ngregated reconstruction term (\u02dcx is the reconstruc-\ntion target), and FR = Eq(x)[KL(q(z|x)||p(z))]\nis the aggregated KL term. With the detailed proof\nshown in Section A of Appendix, we see that F\u03b2\nis an upper bound of IB:\nF\u03b2 \u2265\u2212Iq(z, \u02dcx) + \u03b2Iq(z, x) = LIB,\n(8)\nwhere LIB is the Lagrange relaxation form of IB\npresented by Tishby et al. (2000), Iq(\u00b7, \u00b7) is the\nmutual information (MI) measured by probability\nq. The goal of IB is to maximize the predictive\npower of z on target \u02dcx, subject to the constraint\non the amount of information about original x that\nz carries. When \u03b2 = 0, we have the AE variant\nof our OPTIMUS, the model fully focuses on maxi-\nmizing the MI to recover sentences from the latent\nspace. As \u03b2 increases, the model gradually transits\ntowards \ufb01tting the aggregated latent distribution\nq(z) =\nR\nx q(z|x)q(x)dx to the given prior p(z),\nleading the VAE variant of our OPTIMUS.\n4.2\nModel Architectures\nThe model architecture of OPTIMUS is composed\nof multi-layer Transformer-based encoder and de-\ncoder, based on the original implementation de-\nscribed in (Vaswani et al., 2017). The overall ar-\nchitecture is illustrated in Figure 1. To leverage\nthe expressiveness power of existing PLMs, we\ninitialize our encoder and decoder with weights of\nBERT \u03c6BERT and GPT-2 \u03b8GPT-2, respectively. This\nprocedure is seamless, as all of these models are\ntrained in a self-supervised/unsupervised manner.\nWe denote the number of layers (i.e., Trans-\nformer blocks) as L, the hidden size as H, and the\nnumber of self-attention heads as A. Speci\ufb01cally,\nwe consider BERTBASE (L=12, H=768, A=12, To-\ntal Parameters=110M) and GPT-2 (L=12, H=768,\nA=12, Total Parameters=117M). We hope that our\napproach can provide a practical recipe to inspire\nfuture work to integrate larger pre-trained encoder\nand decoder for higher performance models.\nConnecting BERT & GPT-2\nTwo technical\nquestions remain, when pre-training OPTIMUS\nfrom BERT & GPT-2: (i) How to represent sen-\ntences, since the two PLMs employ different tok-\nenization schemes? (ii) How to adapt a pre-trained\nGPT-2 to arbitrary conditional input without re-\ntraining the model again? Controllable GPT-2 mod-\nels have been studied in (Keskar et al., 2019; Zellers\net al., 2019; Peng et al., 2020a,b) when prescribed\ncontrol codes/tokens are provided, but it is still\nunknown how to ground GPT-2 to arbitrary condi-\ntional inputs.\nTokenization\nIn BERT, WordPiece Embeddings\n(WPE) is used for tokenization (vocabulary size is\n28996 for the cased version). In GPT-2, the mod-\ni\ufb01ed Byte Pair Encoding (BPE) (Radford et al.,\n2019) is used for tokenization (vocabulary size is\n50260). A given token is represented as hEmb, by\nsumming the corresponding token, position and\nsegment embeddings 3. For a sentence, we present\nit in both types of tokenization: the input of en-\ncoder is WPE, and the output of decoder is BPE to\ncompute the reconstruction loss.\nLatent Vector Injection\nSimilar to BERT, the\n\ufb01rst token of every sentence is always a special\nclassi\ufb01cation token ([CLS]). The last-layer hid-\nden state h[CLS] \u2208RH corresponding to this to-\nken is used as the sentence-level representation.\nIt further constructs the latent representation z =\nWEh[CLS], where z \u2208RP is a P-dimensional\nvector and WE \u2208RP\u00d7H is the weight matrix. To\nfacilitate z in GPT-2 decoding without re-training\nthe weights, we consider two schemes, illustrated\nin Figure 2:\n3OPTIMUS does not require segment embeddings, but we\nremain it due to BERT initialization.\n", "where x<t indicates all tokens before t, and \u03b8 is\nthe model parameter. In NLMs, each one-step-\nahead conditional in (1) is modeled by an expres-\nsive family of neural networks, and is typically\ntrained via maximum likelihood estimate (MLE).\nPerhaps the most well-known NLM instance is\nGPT-2 (Radford et al., 2019), which employs Trans-\nformers (Vaswani et al., 2017) for each conditional,\nand \u03b8 is learned on a huge amount of OpenWeb\ntext corpus. GPT-2 has shown surprisingly realistic\ntext generation results, and low perplexity on sev-\neral benchmarks. GPT-3 (Brown et al., 2020) was\nrecently proposed to further scale up NLMs to 175\nbillion parameters, showing impressive results on\nfew-shot learning on multiple language tasks.\nHowever, the only source of variation in NLMs,\nGPT2 and GPT3 is modeled in the conditionals\nat every step: the text generation process only de-\npends on previous word tokens, and there is limited\ncapacity for the generation to be guided by the\nhigher-level structures that are likely presented in\nnatural language, such as tense, topics or sentiment.\n4\nPre-trained Latent Space Modeling\n4.1\nPre-training Objectives\nTo facilitate high-level guidance in sentence gener-\nation, OPTIMUS organizes sentences in a universal\nlatent (or semantic) space, via pre-training on large\ntext corpora. Each sample in this space can be inter-\npreted as outlines of the corresponding sentences,\nguiding the language generation process performed\nin the symbolic space (Subramanian et al., 2018).\nThis naturally \ufb01ts within the learning paradigm of\nlatent variable models such as VAEs (Kingma and\nWelling, 2013; Bowman et al., 2016), where the\nlatent representations capture the high-level seman-\ntics/patterns. It consists of two parts, generation\nand inference, enabling a bidirectional mapping\nbetween the latent space and symbolic space.\nGeneration\nThe generative model (decoder)\ndraws a latent vector z from the continuous latent\nspace with prior p(z), and generates the text se-\nquence x from a conditional distribution p\u03b8(x|z);\np(z) is typically assumed a multivariate Gaussian,\nand \u03b8 represents the neural network parameters.\nThe following auto-regressive decoding process is\nusually used:\np\u03b8(x|z) =\nT\nY\nt=1\np\u03b8(xt|x<t, z).\n(2)\nIntuitively, VAE provides a \u201chierachical\u201d gener-\nation procedure: z \u223cp(z) determines the high-\nlevel semantics, followed by (2) to produce the\noutput sentences with low-level syntactic and lex-\nical details. This contrasts with (1) in the explicit\ndependency on z.\nInference\nSimilar to GPT-2, parameters \u03b8 are\ntypically learned by maximizing the marginal\nlog likelihood log p\u03b8(x) = log\nR\np(z)p\u03b8(x|z)dz.\nHowever, this marginal term is intractable to com-\npute for many decoder choices. Thus, variational\ninference is considered, and the true posterior\np\u03b8(z|x) \u221dp\u03b8(x|z)p(z) is approximated via the\nvariational distribution q\u03c6(z|x) is (often known as\nthe inference model or encoder), implemented via\na \u03c6-parameterized neural network. It yields the\nevidence lower bound objective (ELBO):\nlog p\u03b8(x) \u2265LELBO =\n(3)\nEq\u03c6(z|x)\n\u0002\nlog p\u03b8(x|z)\n\u0003\n\u2212KL(q\u03c6(z|x)||p(z))\nTypically, q\u03c6(z|x) is modeled as a Gaussian\ndistribution, and the re-parametrization trick is used\nfor ef\ufb01cient learning (Kingma and Welling, 2013).\nA Taxonomy of Autoencoders\nThere is an alter-\nnative interpretation of the ELBO: the VAE objec-\ntive can be viewed as a regularized version of the\nautoencoder (AE) (Goodfellow et al., 2016). It is\nthus natural to extend the negative of LELBO in (3)\nby introducing a hyper-parameter \u03b2 to control the\nstrength of regularization:\nL\u03b2 = LE + \u03b2LR, with\n(4)\nLE = \u2212Eq\u03c6(z|x)\n\u0002\nlog p\u03b8(x|z)\n\u0003\n(5)\nLR = KL(q\u03c6(z|x)||p(z))\n(6)\nwhere LE is the reconstruction error (or negative\nlog-likelihood (NLL)), and LR is a KL regularizer.\nThe cost function L\u03b2 provides a uni\ufb01ed perspective\nfor understanding various autoencoder variants and\ntraining methods. We consider two types of latent\nspace with the following objectives:\n\u2022 AE. Only LE is considered (\u03b2 = 0), while\nthe Gaussian sampling in q\u03c6(z|x) remains.\nIn other words, the regularization is removed,\nand a point-estimate is likely to be learned\nto represent the text sequence\u2019s latent feature.\nNote our reconstruction is on sentence-level,\nwhile other PLMs (Devlin et al., 2019; Yang\net al., 2019) employ masked LM loss, per-\nforming token-level reconstruction.\n", "bution in training. As a result, the language repre-\nsentations learned by OPTIMUS are more universal\n/ general in that they can be more easily adapted\nto a new domain/task. (iii) Different from GPT-2,\nwhich generates human-like text but may lack effec-\ntive means of controlling its high-level semantics\n(such as tense, topics, sentiment), OPTIMUS can\nbe easily deployed for guided text generation. The\neffectiveness of OPTIMUS has been demonstrated\nwith extensive experiments on language modeling,\ndialog response generation, text style transfer and\nlow-resource language understanding. It achieves\nlower perplexity than GPT-2 on standard bench-\nmarks, produces strong performance on guided text\ngeneration, and improves BERT on feature-based\nlanguage understanding tasks. The code and pre-\ntrained models are released on Github2.\nAlong the way to build the \ufb01rst big VAE lan-\nguage model, there are several technical contribu-\ntions/implications that are novel: (i) Latent vector\ninjection: this work demonstrates two schemes to\ndiscuss how to effectively inject conditioning vec-\ntors into GPT-2 without re-training it. (ii) The\ndesign idea to combine BERT/GPT-2 serves as a\npractical recipe to inspire people to integrate and\nreuse existing PLMs for larger and complex mod-\nels. (iii) Pre-training on massive datasets itself is\nan effective approach to reduce KL vanishing, as\ndemonstrated by the state of-the-art performance\non four VAE language modeling datasets. (iv) The\nproof of VAE objective from the lens of IB, show-\ning that VAE is a principled approach to balance\nthe compactness and usability of learned represen-\ntations. (v) Improved performance on several lan-\nguage tasks shows the importance and necessity of\npre-training a latent space.\n2\nRelated Work\nDifference\nwith\nprior\nPLMs.\nLarge-scale\nTransformer-based PLMs have recently achieved\nstate-of-the-art performance on various natural lan-\nguage understanding and generation tasks (Devlin\net al., 2019; Yang et al., 2019; Radford et al., 2019;\nLiu et al., 2019; Keskar et al., 2019). Prior to\nTransformer-based PLMs, non-generative methods\nhave seen some early success in pre-training\nsequence models for supervised downstream tasks\nincluding standard sequence auto-encoders (Dai\nand Le, 2015; Li et al., 2015), skip-thought\nmodels (Kiros et al., 2015) and paragraph vector\n2https://github.com/ChunyuanLI/Optimus\nmodels (Le and Mikolov, 2014) etc. However, all\nof these models do not generally learn a smooth,\ninterpretable feature space for sentence encoding,\nor generating novel sentences. In this work, we\naim to \ufb01ll the gap to learn such a universal latent\nspace in the \ufb01eld of Transformer-based PLMs.\nLatent variable language modeling.\nLanguage\nVAEs have inspired new applications in NLP,\nvia exploiting many interesting properties of the\nmodel\u2019s latent space (Bowman et al., 2016; Kim\net al., 2018b). Its modeling capacity and empir-\nical performance is somewhat limited, partially\ndue to the KL vanishing issue described in Sec-\ntion 4.3. Several attempts have been made to al-\nleviate this issue, including different KL anneal-\ning/thresholding schemes (Bowman et al., 2016; Fu\net al., 2019; Higgins et al., 2017; Li et al., 2019),\ndecoder architectures (Yang et al., 2017; Dieng\net al., 2018), auxiliary loss (Zhao et al., 2017),\nsemi-amortized inference (Kim et al., 2018a), ag-\ngressive encoder training schedule (He et al., 2019),\nbatch normalized inference (Zhu et al., 2020) and\n\ufb02exible posterior (Fang et al., 2019). Subrama-\nnian et al. (2018) have shown some promise that\ngeneral encoder can bene\ufb01t language generation.\nTransformers (Vaswani et al., 2017) are recently\nconsidered in VAEs for classi\ufb01cation (Gururangan\net al., 2019) and storytelling (Wang and Wan, 2019).\nPre-training VAEs has been recently considered in\nconditional text generation to amortize the training\nof decoders and to allow easy adaptation in new\ngeneration tasks (Duan et al., 2019).\nAll these efforts utilize simple LSTM (Hochre-\niter and Schmidhuber, 1997) and shallow Trans-\nformer (Vaswani et al., 2017) architectures, thus\nwith limited capacity. Our paper is the \ufb01rst big VAE\nmodel at the same scale of recent PLMs such as\nBERT and GPT-2. More importantly, we show that\npre-training a meaningful latent space on a large\ntext corpus can largely reduce the KL vanishing\nissue, and lead to new state-of-the-art performance.\n3\nBackground on NLMs & GPT-2\nTo generate a text sequence of length T,\nx\n=\n[x1, \u00b7 \u00b7 \u00b7 , xT ], neural language models\n(NLM) (Mikolov et al., 2010) generate every token\nxt conditioned on the previous word tokens:\np(x) =\nT\nY\nt=1\np\u03b8(xt|x<t),\n(1)\n"], "summary": "The cited paper, \"OPTIMUS,\" proposes a large-scale VAE model that combines BERT and GPT-2 to create a universal latent space for sentences, addressing the limitations of previous shallow VAE architectures. While my work focuses on discrete variational attention models to tackle posterior collapse and information under-representation, OPTIMUS also aims to mitigate posterior collapse through pre-training on massive datasets and demonstrates the benefits of a well-structured latent space for language generation. This paper provides a strong baseline and a contrasting approach to my discrete latent space method, particularly in its use of continuous latent variables and large-scale pre-training.", "citation": "Bo Pang, Erik Nijkamp, Tian Han, Ying Nian Wu (2021). Generative Text Modeling through Short Run Inference. arXiv:2106.02513. https://arxiv.org/abs/2106.02513"}, {"paper_id": 54, "text": ["Improve Variational Autoencoder for Text Generation\nwith Discrete Latent Bottleneck\nYang Zhao1 , Ping Yu1 , Suchismit Mahapatra2 , Qinliang Su3 , Changyou Chen1\n1University at Buffalo, 2Criteo, 3Duke University\nAbstract\nVariational autoencoders (VAEs) are essential tools\nin end-to-end representation learning.\nHowever,\nthe sequential text generation\u2019s common pitfall\nwith VAEs is that the model tends to ignore latent\nvariables with a strong auto-regressive decoder. In\nthis paper, we propose a principled approach to al-\nleviate this issue by applying a discretized bottle-\nneck to enforce an implicit latent feature match-\ning in a more compact latent space. We impose\na shared discrete latent space where each input is\nlearned to choose a combination of latent atoms as\na regularized latent representation. Our model en-\ndows a promising capability to model underlying\nsemantics of discrete sequences and thus provide\nmore interpretative latent structures. Empirically,\nwe demonstrate our model\u2019s ef\ufb01ciency and effec-\ntiveness on a broad range of tasks, including lan-\nguage modeling, unaligned text style transfer, dia-\nlog response generation, and neural machine trans-\nlation.\n1\nIntroduction\nAuto-encoder models, which try to learn a function that maps\neach input to a latent representation and then back to the orig-\ninal data space, are widely used in various NLP tasks such\nas machine translation [Vaswani et al., 2017] and dialog re-\nsponse generation [Olabiyi and Mueller, 2019]. Variational\nautoencoders (VAEs) [Kingma and Welling, 2013], by con-\ntrast, aim to learn a probabilistic generative model that can\ngenerate new instances similar to the original data. Mean-\nwhile, VAEs are endowed with \ufb02exible latent representations\n(e.g. style and semantic features) with which one can eas-\nily draw diverse and relevant samples from distribution fol-\nlowing a decoding scheme. VAEs have achieved tremendous\nsuccess in generating high-quality images, videos, and speech\n[van den Oord et al., 2017; Razavi et al., 2019]. At the same\ntime, VAEs have also been applied in NLP to improve tradi-\ntional maximum-likelihood-estimation (MLE) based models,\nachieving impressive progress in language modeling [Miao\net al., 2016; Li et al., 2020], controllable text generation [Hu\net al., 2017], neural machine translation [Shah and Barber,\n2018], and many other applications.\nA well-known pitfall with VAEs, especially in applica-\ntions of sequence-to-sequence (Seq2Seq) modeling, is a phe-\nnomenon called latent variable collapse (or posterior col-\nlapse) [Bowman et al., 2015], where an encoder yields mean-\ningless posteriors that collapse to the prior. With this pitfall,\nVAEs usually fail to learn meaningful representations of in-\ndividual data samples. Several attempts have been made to\nalleviate this issue [He et al., 2019; Fang et al., 2019]. How-\never, most of these approaches are heuristic in nature.\nOur solution to this problem is motivated by two possible\nexplanations of posterior collapse: i) Recent research shows\nthat the prior plays a vital role in density estimation [Taka-\nhashi et al., 2019]. Although Gaussian prior and posterior\nare broadly adopted, such simpli\ufb01ed priors tend to incur pos-\nterior collapse for \ufb02awed density estimations. To overcome\nthis issue, we argue that a \ufb02exible prior should be learned\nsimultaneously during training. ii) Related work has also\nshown that the posterior collapse is caused by a lack of useful\nlatent codes [Fu et al., 2019]. Thus, designing an effective\nway of learning good representations without supervision\nis the key to address the problem. In this paper, based on\nthe above two arguments, we propose to enforce a discrete\nlatent space for VAEs, namely DB-VAE. The discrete space\nconsists of learnable atoms that are shared by all data inputs.\nThe discrete latent space automatically brings in at least two\nbene\ufb01ts: i) The discrete atoms are evolving that are sam-\npled from the prior as the training proceeds instead of being\n\ufb01xed; ii) The discretization performs implicit feature match-\ning and enforces a semantically meaningful nearest neigh-\nbor structure. The contributions of our paper are summa-\nrized as follows:\n\u2022 We propose the concept of discretized bottleneck VAEs\nfor RNN-based Seq2Seq models, which can implicitly\nalleviate the long-standing posterior-collapse issue.\n\u2022 We show how to inject the discretized bottleneck in\nSeq2Seq models on a variety of NLP tasks. Our DB-\nVAE can accurately model discrete text without sacri\ufb01c-\ning reliance on latent representations being employed as\na simple plugin. We also \ufb01nd that under our framework,\nthe discrete bottleneck can capture more sentence-level\nsemantic features.\n\u2022 Inference of the proposed DB-VAE requires a nearest-\nneighbor (NN) search for the discrete atoms in a la-\ntent space.\nWe extend NN to the k-NN setting and\narXiv:2004.10603v2  [cs.LG]  25 Feb 2021\n", "A.3\nExtension: RNN-based NMT Model\nWe \ufb01nally evaluate our model with the proposed top-k NN\nsearch on the German-English translation task. Our model\nis built on a baseline RNNsearch architecture [Bahdanau et\nal., 2014]. The recently proposed variational attention model\n[Deng et al., 2018] is also adopted as a baseline.\nWe use the IWLST14 dataset [Cettolo et al., 2014], which\nis a standard benchmark for experimental NMT models.\nDataset and practical settings are given in SM A.3. Results\naveraged by 5 different runs are reported in Table 5 and in\nFigure 8 (in SM). Note the attention mechanism is used in\nRNNsearch, where each progressed state in the decoder side\nhas direct access to the state on the encoder side. Although we\nonly discretize the encoder\u2019s \ufb01nal hidden state as formulated\nin Section 3.1, a notable improvement on the PPL and BLEU\nscore is still observed. Following Algorithm 2, as we increase\nthe value of k from 1 to 10, the BLEU score continues in-\ncreasing until k reaches 5. The reason might be that the top-5\nlatent codes have already encoded most source-target com-\nbinations. Besides, the BLEU score is as low as 26.1 when\nwe choose the farthest latent code from the codebook instead.\nThese validate the effectiveness of our proposed top-k infer-\nence strategy, which applies to most RNN-based autoencoder\nmodels.\nIWLST14 dataset contains around 153K, 7K and 7K sen-\ntences for training, validation and testing, respectively. The\nsame preprocessing as in [Ott et al., 2018] is applied. As\nfor the architecture, both the encoder and the decoder have\none layer, each with 512-dimensional embedding. For BLEU\nevaluation, the beam size in beam search is 5. The library\nFairseq [Ott et al., 2019] is adopted as the codebase. The\ncodebook size K is set to 220, and only the \ufb01nal hidden state\nof the encoder passes through the discretized bottleneck.\nFigure 8: BLEU score on IWLST14\n", "erated words.\n[Kim et al., 2018] offers a semi-amortized\napproach that uses stochastic variational inference to re-\n\ufb01ne an inference network iteratively.\nThis method, how-\never, is expensive to train.\nSimilarly, [He et al., 2019]\npropose a simple yet effective training algorithm that ag-\ngressively optimizes the inference network with more up-\ndates.\nOther threads of solutions introduce more compli-\ncated priors in the latent space [Tomczak and Welling, 2017;\nZiegler and Rush, 2019]. [Makhzani et al., 2015; Joulin et al.,\n2016] further replace the KL regularizer with an adversarial\nregularizer. Our work outperforms these methods without in-\ncreasing additional training burdens.\nIn the case of discrete representations in VAE, the most\nrelated work is [Zhao et al., 2018]. It applies the Gumbel-\nSoftmax trick [Jang et al., 2016] to train discrete variables,\nresulting in effective and interpretable dialog generation. Our\napproach has broader applicability and is ready to be ex-\ntended to more NLP tasks.\n5\nExperiments\nWe conduct extensive experiments to demonstrate the effec-\ntiveness and ef\ufb01ciency of the proposed DB-VAE on vari-\nous language processing tasks, including language modeling\n(LM), unaligned text-style transfer, dialog-response genera-\ntion, and neural machine translation (NMT). Besides, we also\nevaluate how the codebook size K affects a model\u2019s perfor-\nmance. The code for reproducing these results will be made\npublicly available upon acceptance.\n5.1\nLanguage Modeling\nFollowing [Yang et al., 2017], we evaluate our model for lan-\nguage modeling, mainly on two large-scale document corpus,\nYahoo, and Yelp. Detailed statistics of the two datasets are\ngiven in Table 6 in the Supplementary Material (SM) A.1.\nWe \ufb01rst used a simple synthetic dataset [He et al., 2019] con-\nsisting of 16k training sentences and 4k testing sentences to\nevaluate how the codebook size affects the model\u2019s perfor-\nmance.\nThe Impact of Codebook Size K:\nWe \ufb01rst investigate\nthe impact of codebook size K on the model\u2019s behavior.We\nobserve when the codebook size is smaller than 216, the bot-\ntleneck seems too tight to induce a right amount of represen-\ntation power. Because validation ppl\u2019s are very close when\nK \u2265216, we adopt K = 216 in all our experiments (a trade-\noff between memory and performance) unless explicitly de-\nclared.\nBaseline and Training Details:\nFour representative LM\nmodels are chosen as baselines, including LSTM-LM, the\nstandard VAE, SA-VAE [Kim et al., 2018], and Lag-VAE\n[He et al., 2019], the current state-of-the-art. For fair com-\nparisons, both the inference and generative networks are im-\nplemented as a 1-layer LSTM with 1024 hidden units for all\nmodels. The word embedding dimension is set to 1024 and\nthe latent size to 32. The SGD optimizer with the same set-\nting is applied to all models. The latent variable is used to\ninitialize the decoder\u2019s hidden state and fed as additional in-\nput at each time step. We adopt two sliced codebooks since\nthe single codebook fails to work in the experiments.\nLM Results:\nThe results in terms of perplexity (PPL) and\ntraining time are shown in Table 1. We follow the implemen-\ntation in [Li et al., 2020] to report the PPL. As expected, our\nmodel achieves the best performance in all the metrics. Re-\nmarkably, our model runs almost as fast as the standard VAE.\nThe faster convergence of Lag-VAE initially is because it ag-\ngressively trains an encoder, where approximately 50\u00d7 more\ndata are used to train the encoder in one epoch. We can apply\nDB-VAE-r for text generation at evaluation.\nModels\nYelp\nYahoo\nPPL\nTime\nPPL\nTime\nLSTM-LM\n40.64\n-\n60.75\n-\nVAE\n40.56\n5.4\n61.21\n6.9\nSA-VAE\n40.39\n56.3\n61.59\n69.2\nLag-VAE\n37.92\n20.3\n56.78\n15.3\nDB-VAE-r (Ours)\n37.41\n5.4\n55.87\n7.0\nDB-VAE-q (Ours)\n37.26\n5.4\n55.24\n7.0\nTable 1: Comparisons on language modeling.\nLatent Space Visualization:\nFor better understanding, we\nvisualize the latent representations of the whole dataset using\nt-SNE projection [Maaten and Hinton, 2008] in Figure 2. It\nis seen that our model can learn a much smoother and more\nseparable transition from 0-star to 4-star reviews. To visualize\nthe codebook utilization, we also compute the v (12) on a\nrandom batch of testing data after each training epoch. As\nshown in Figure 4 in the SM A, the usage of the codebook\nbecomes more balanced as the training goes on.\nFigure 2: t-SNE embeddings of latent space on Yelp corpus. Left:\nLag-VAE, Right: DB-VAE. 0-4 represents the review score, from\nnegative to positive.\nCodebook Interpolation:\nParticularly in text modeling,\nwhen performing a convex combination between any two\nlatent codes z1 and z2, the interpolation is equivalent to\n\u02dcx\u03bb = g\u03c6(\u03bbz1 + (1 \u2212\u03bb)z2). Ideally, adjusting \u03bb from 0 to\n1 will generate a series of sentences, where x\u03bb will be less\nsemantically similar with the sentence corresponding to z1\nand much more semantically similar to that of z2 [Berthelot\net al., 2018]. Table 7 in the SM A shows the generated sen-\ntences when \u03bb ranges from 0.0 to 1.0 with a stepsize of 0.1.\nIndeed, intermediate sentences x\u03bb produced by the proposed\nmodel can provide a semantically smooth morphing between\nthe two endpoints.\nComparison with VQ-VAE:\nWe conduct an ablation\nstudy of comparing with original VQ-VAE on the Yelp\ndataset. In both cases of using one codebook and skipping\n", "\u03bb\nGenerated intermediate sentences\n0.0\nhad a great experience at this place ! i had a great experience with the staff and the staff was very friendly and helpful\n! i had a great experience and i will de\ufb01nitely be back !\n0.1\nhad a great experience here ! the staff was very friendly and helpful ! i had a great time and i will de\ufb01nitely be back !\n0.2\nstopped in for a quick bite before heading out to the airport . i had the chicken and waf\ufb02es and it was delicious ! i\nwould de\ufb01nitely recommend this place to anyone looking for a great breakfast !\n0.3\nstopped in for a quick bite before heading out to the airport . i had the chicken and waf\ufb02es and it was delicious ! the\nservice was fast and friendly . i will de\ufb01nitely be back !\n0.4\nstopped in for a quick bite before heading out to the airport . i had the chicken and waf\ufb02es and it was delicious ! the\nservice was friendly and fast . i \u2019ll be back !\n0.5\nmy husband and i stopped in for a quick bite before heading out to the airport . we were seated right away and we\nwere seated right away . our server was very friendly and helpful . the food was pretty good and the service was great\n.\n0.6\nmy husband and i stopped in for a quick bite before heading out to the airport . we were seated right\n0.7\nthis was my \ufb01rst time here and i will de\ufb01nitely be back . the service was fast and friendly and the food was delicious .\ni \u2019ll be back .\n0.8\nthis was my \ufb01rst time here and i will de\ufb01nitely be back . the service was good , the food was good , and the prices\nwere reasonable . i \u2019ll be back .\n0.9\nthis place was pretty good . i had the chicken and waf\ufb02es and it was pretty good . i \u2019d de\ufb01nitely go back .\n1.0\nthis place was pretty good . i had the pulled pork sandwich and it was pretty good , but nothing special . the fries were\npretty good though .\nTable 7: Detailed interpolation results\nModel\nBLEU\u2191\nBOW Embedding\u2191\nintra-dist\u2191\ninter-dist\u2191\nR\nP\nF1\nA\nE\nG\ndist-1\ndist-2\ndist-1\ndist-2\nSeqGAN\n0.270\n0.270\n0.270\n0.907\n0.495\n0.774\n0.747\n0.806\n0.075\n0.081\nCVAE\n0.265\n0.222\n0.242\n0.923\n0.543\n0.811\n0.938\n0.973\n0.177\n0.222\nCVAE-BOW\n0.256\n0.224\n0.239\n0.923\n0.540\n0.812\n0.949\n0.976\n0.165\n0.206\nVHRED\n0.271\n0.260\n0.265\n0.892\n0.507\n0.786\n0.633\n0.711\n0.071\n0.089\nWAE-GMP\n0.372\n0.286\n0.323\n0.952\n0.591\n0.853\n0.754\n0.892\n0.313\n0.597\nDI-VAE\n0.323\n0.190\n0.239\n0.874\n0.600\n0.814\n0.947\n0.963\n0.500\n0.718\nDB-VAE\n0.373\n0.276\n0.317\n0.944\n0.615\n0.839\n0.954\n0.997\n0.467\n0.787\nTable 8: Performance comparison on dialog response generation, DailyDialog Dataset\n"], "summary": "The cited paper, \"Improve Variational Autoencoder for Text Generation with Discrete Latent Bottleneck,\" directly relates to your work by also addressing the posterior collapse problem in VAEs for text generation, specifically through the use of a discrete latent space. While your work focuses on discrete variational attention, this paper proposes a discretized bottleneck (DB-VAE) to enforce implicit latent feature matching and alleviate posterior collapse. Both papers highlight the benefits of discrete latent spaces for language generation and demonstrate improved performance over continuous counterparts. The cited paper also explores the impact of codebook size, which is relevant to your use of categorical distribution.", "citation": "Yookoon Park, Jaemin Cho, Gunhee Kim (2018). A Hierarchical Latent Structure for Variational Conversation Modeling. arXiv:1804.03424. https://arxiv.org/abs/1804.03424"}, {"paper_id": 4, "text": ["Improving Variational Autoencoders with Density\nGap-based Regularization\nJianfei Zhang1,2\nJun Bai1,2\nChenghua Lin3\nYanmeng Wang4\nWenge Rong1,2\n1State Key Laboratory of Software Development Environment, Beihang University, China\n2School of Computer Science and Engineering, Beihang University, China\n3Department of Computer Science, University of Shef\ufb01eld, United Kingdom\n4Ping An Technology, China\n{zhangjf,ba1_jun,w.rong}@buaa.edu.cn\nc.lin@sheffield.ac.uk, wangyanmeng219@pingan.com.cn\nAbstract\nVariational autoencoders (VAEs) are one of the most powerful unsupervised learn-\ning frameworks in NLP for latent representation learning and latent-directed gener-\nation. The classic optimization goal of VAEs is to maximize the Evidence Lower\nBound (ELBo), which consists of a conditional likelihood for generation and a\nnegative Kullback-Leibler (KL) divergence for regularization. In practice, opti-\nmizing ELBo often leads the posterior distribution of all samples converging to\nthe same degenerated local optimum, namely posterior collapse or KL vanishing.\nThere are effective ways proposed to prevent posterior collapse in VAEs, but we\nobserve that they in essence make trade-offs between posterior collapse and the\nhole problem, i.e., the mismatch between the aggregated posterior distribution and\nthe prior distribution. To this end, we introduce new training objectives to tackle\nboth problems through a novel regularization based on the probabilistic density gap\nbetween the aggregated posterior distribution and the prior distribution. Through\nexperiments on language modeling, latent space visualization, and interpolation,\nwe show that our proposed method can solve both problems effectively and thus\noutperforms the existing methods in latent-directed generation. To the best of our\nknowledge, we are the \ufb01rst to jointly solve the hole problem and posterior collapse.\n1\nIntroduction\nAs one of the most powerful likelihood-based generative models, variational autoencoders (VAEs) [21,\n32] are designed for probabilistic modeling directed by continuous latent variables, which are\nsuccessfully applied in many NLP tasks, e.g., dialogue generation [45, 14], machine translation [34,\n12], recommendation [10], and data augmentation [43, 39]. One of the major advantages of VAEs is\nthe \ufb02exible latent representation space, which enables easy manipulation of high-level semantics on\ncorresponding representations, e.g., guided sentence generation with interpretable vector operators.\nDespite the attractive theoretical strengths, VAEs are observed to suffer from a well-known problem\nnamed posterior collapse or KL vanishing [21, 24], an optimum state of VAEs when the posterior\ndistribution contains little information about the corresponding datapoint, which is particularly\nobvious when strong auto-regressive decoders are implemented [46, 4].\nAnother challenge for VAEs is the hole problem, the state when the aggregated (approximate) posterior\nfails to \ufb01t the prior distribution, and thus the inference from the prior distribution becomes no longer\nsuitable to describe the global data distribution [33], which can lead to poor generation quality in\nVAEs [1, 25].\nPreprint. Under review.\narXiv:2211.00321v1  [cs.LG]  1 Nov 2022\n", "Table 1: Statistics of sentences in the datasets\nDataset\nTrain\nValid\nTest\nVocab size\nLength (avg \u00b1 std)\nYelp\n100,000\n10,000\n10,000\n19997\n98.01 \u00b1 48.86\nYahoo\n100,000\n10,000\n10,000\n20001\n80.76 \u00b1 46.21\nShort-Yelp\n100,000\n10,000\n10,000\n8411\n10.96 \u00b1 3.60\nSNLI\n100,000\n10,000\n10,000\n9990\n11.73 \u00b1 4.33\n4\nExperiments\n4.1\nExperimental Setup\nDatasets We consider four public available datasets commonly used for VAE-based language\nmodeling tasks in our experiments: Yelp [42], Yahoo [42, 44], a downsampled version of Yelp [35]\n(we denote this as Short-Yelp), and a downsampled version of SNLI [5, 23]. The statistics of these\ndatasets are illustrated in Table 1. It can be viewed that Yelp and Yahoo contain long sentences while\nShort-Yelp and SNLI contain short sentences.\nBaselines We consider a wide range of VAEs for solving posterior collapse in text generation, where\nthe hyperparameters are set according to Zhu et al. [46]:\n\u2022 VAEs with modi\ufb01ed training strategies (i.e., KL annealing): VAE with linear KL annealing\nin the \ufb01rst 10 epochs (default) [6]; VAE with linear KL annealing for 10 epochs at the start\nof every 20 epochs (cyclic-VAE) [13];\n\u2022 VAEs with speci\ufb01c model structures: VAE with additional Bag-of-Words loss (bow-\nVAE) [45], and VAE with skip connection from the latent variable z to the vocabulary\nclassi\ufb01er for generation (skip-VAE) [9];\n\u2022 VAEs with hard restrictions on the posterior distribution: \u03b4-VAE with the committed rate\n\u03b4 = 0.15 [31]; BN-VAEs with the scale of BN layer \u03b3 \u2208{0.6, 0.7, 0.9, 1.2, 1.5, 1.8} [46];\nvMF-VAEs with the distribution\u2019s concentration \u03ba \u2208{13, 25, 50, 100, 200} [8, 41];\n\u2022 VAEs with weakened KL regularization:\nFB-VAEs (free-bits) with the target KL\n\u03bbKL \u2208{4, 9, 16, 25, 36, 49} [20]; \u03b2-VAEs with the weight of the KL term in ELBo\n\u03b2 \u2208{0.0, 0.1, 0.2, 0.4, 0.8} [18].\nCon\ufb01gurations We completely follow Zhu et al. [46] in the models\u2019 backbone structures, data\npre-processing, and training procedure, which we describe in detail in Appendix B.\n4.2\nLanguage Modeling\nWe evaluate the performance of our methods and the baselines on language modeling, where the\nfollowing metrics are reported: the prior log likelihood priorLL(\u03b8) and the posterior log likelihood\npostLL(\u03b8, \u03c6) for generation quality; the KL term in ELBo KL(\u03c6), the mutual information MI(\u03c6)\nof z and n and the number of active units AU(\u03c6) [7] for posterior collapse; and the number of\nconsistent units CU(\u03c6) (we propose) for the hole problem. The corresponding expressions and\nexplanations are presented in Appendix C.\nWe illustrate part of the results on Yahoo in Table 2 and all results on all datasets in Appendix D. It\ncan be observed that: (1) models with modi\ufb01ed training strategies or speci\ufb01c model structures can\nalleviate the problem of posterior collapse but has limited effect according to MI(\u03c6) and AU(\u03c6);\n(2) models with hard restrictions or weakened KL regularization on the posterior can solve posterior\ncollapse better through harder restrictions or further weakening according to the increase of KL(\u03c6),\nMI(\u03c6), and AU(\u03c6), but the decrease of CU(\u03c6) indicates that their posterior latent spaces tend to\nbe increasingly inconsistent with that of the prior; (3) in contrast, our proposed DG-VAE has similar\nperformance to the vanilla VAE when |b| = 1, and with the increase of |b|, it can solve posterior\ncollapse effectively and avoid the hole problem at the same time.4\n4There\u2019s a little difference between DG-VAE (|b| = 32) and DG-VAE (default): DG-VAE (|b| = 32) ignores\ndata batch B if |B| < 32 while DG-VAE (default) accepts it through adapting to its batch size.\n7\n", "divergence DJS; Wasserstein Auto-Encoder (WAE) [36, 2] regularizes the aggregated posterior\ndistribution through minimizing Maximum Mean Discrepancy (MMD); Implicit VAE with Mutual\nInformation regularization (iVAEMI) regularizes the aggregated posterior through a dual form of KL\ndivergence DKL on the basis of Implicit VAE (iVAE) [11]. These methods have the same weakness\nthat their approximations of the divergence between two continuous distributions are depicted by\nmerely sampling sets from the distributions, which involves noise from random sampling and can\nhardly be zero, even for the same distributions.\nIn contrast, our method approximates the divergence between two continuous distributions in the\nperspective of their mismatch in PDFs, which we quantify through the density gap that can be zero if\nand only if they are the same, which we describe in section 3.\nWe validate our method against the aforementioned methods through experiments on a synthetic\ndataset, the details and results of which are presented in Appendix A.\n3\nMethodology\nDensity Gap-based Discrepancy\nOne of the straight manifestations of holes in latent space is the\nmismatch of probabilistic density between q\u03c6(z) and p\u03b8(z). We quantify this mismatch at a speci\ufb01c\nposition z in the latent space through DG(\u03b8, \u03c6; z), which we refer to as Density Gap. Here we only\nconsider z \u2208{z|q\u03c6(z) > 0}.1 We assume q\u03c6(z) and p\u03b8(z) are differentiable and p\u03b8(z) > 0.\nDG(\u03b8, \u03c6; z) = log q\u03c6(z)\np\u03b8(z) = log\n1\nN\nPN\nn=1 q\u03c6(z|xn)\np\u03b8(z)\n(4)\nIt can be inferred that the expectation of DG(\u03b8, \u03c6; z) on q\u03c6(z) equals to the KL divergence between\nq\u03c6(z) and p\u03b8(z), as illustrated in Eq. 5, which is a strict divergence, i.e., Ez\u223cq\u03c6(z)[DG(\u03b8, \u03c6; z)] =\n0 iff q\u03c6(z) = p\u03b8(z).\nEz\u223cq\u03c6(z)[DG(\u03b8, \u03c6; z)] = Ez\u223cq\u03c6(z)[log q\u03c6(z)\np\u03b8(z)] = DKL(q\u03c6(z)\u2225p\u03b8(z)) \u22650\n(5)\nSo, we can approximate and optimize DKL(q\u03c6(z)\u2225p\u03b8(z)) via Monte Carlo, as illustrated in Eq. 6,\nwhere zs\nidd\n\u223cq\u03c6(z) denotes the sth random sample from the aggregated posterior distribution.\nDKL(q\u03c6(z)\u2225p\u03b8(z)) \u22481\nS\nS\nX\ns=1\nDG(\u03b8, \u03c6; zs)\n(6)\nIt should be noted that DKL(q\u03c6(z)\u2225p\u03b8(z)) approximated by this is an overall divergence, as it consid-\ners the posterior distribution of all datapoints as a whole, instead of averaging DKL(q\u03c6(z|x)\u2225p\u03b8(z))\nacross all datapoints as ELBo does.\nOn that basis, we can implement LD(\u03b8, \u03c6; x) with D = DKL, which is equivalent to replacing\nthe KL term in ELBo with DKL(q\u03c6(z)\u2225p\u03b8(z)) approximated by Eq. 6. According to the decom-\nposition (illustrated in Eq. 7) of the KL term in ELBo given by Hoffman et al. [19], maximizing\nLDKL(\u03b8, \u03c6; xn) on the whole dataset, X = {xn}N\nn=1, q\u03c6(n) \u2261\n1\nN , is equivalent to maximizing\nELBo as well as Iq\u03c6(n,z)[n, z],2 the mutual information of z and n in their joint distribution q\u03c6(n, z),\nas illustrated in Eq. 8.\n1\nN\nN\nX\nn=1\nDKL(q\u03c6(z|xn)\u2225p\u03b8(z)) = DKL(q\u03c6(z)\u2225p\u03b8(z)) + Iq\u03c6(n,z)[n, z]\nwhere Iq\u03c6(n,z)[n, z] = Eq\u03c6(n,z)[log\nq\u03c6(n, z)\nq\u03c6(n)q\u03c6(z)]\nwhere q\u03c6(n, z) = q\u03c6(n)q\u03c6(z|n) = 1\nN q\u03c6(z|xn)\n(7)\n1Although we can have q\u03c6(z) = 0, z \u2208RDim when the latent variable follows a von Mises-Fisher (vMF)\ndistribution, we do not need to consider such points in regularization.\n2Posterior collapse (or KL vanishing) can be solved effectively by maximizing this mutual information term\nas it is a lower bound of the vanished KL divergence term in ELBo according to Eq. 7.\n4\n", "meanwhile, DG-VAE outperforms BN-VAEs, FB-VAEs and \u03b2-VAEs in the quality of interpolation\non the Yahoo dataset as it can solve posterior collapse and avoid the hole problem at the same time.\nIn summary, the existing methods for solving posterior collapse in VAEs either have limited effect\nor can effectively solve posterior collapse at the cost of bringing the hole problem. In contrast,\nour proposed DG-VAE can effectively solve posterior collapse and avoid the hole problem at the\nsame time, which is demonstrated by the posterior centers spread in latent space and the aggregated\nposterior distribution consistent with the prior distribution. Furthermore, our proposed DG-VAE\noutperforms the existing methods in the quality of latent-guided generation due to these improvements\nin latent space.\n5\nDiscussion\nConclusion\nIn this work, we perform systematic experiments to demonstrate posterior collapse\nand the hole problem in existing continuous VAEs for text generation. To solve both problems at the\nsame time, we propose a density gap-based regularization on the aggregated posterior distribution\nto replace the KL regularization in ELBo, and prove it in essence maximizes the ELBo as well\nas the mutual information between the latent and the input. Experiments on real-world datasets\nprove the effectiveness of our method in solving both problems and its improvement in latent-guided\ngeneration.\nLimitation & Future work\nBoth the theory and the ablation study show that the effectiveness of\nour proposed method depends on the aggregation size |b|, which is still limited by the batch size\nduring training. Therefore a promising future direction is to \ufb01nd a solution to break this limit, such\nlike the memory bank mechanism in contrastive learning [38].\nAcknowledgments and Disclosure of Funding\nThis work was supported in part by the State Key Laboratory of the Software Development Environ-\nment of China under Grant SKLSDE-2021ZX-16.\nReferences\n[1] Jyoti Aneja, Alexander G. Schwing, Jan Kautz, and Arash Vahdat. A contrastive learning\napproach for training variational autoencoder priors. In Proceedings of the 2021 Annual\nConference on Neural Information Processing Systems, pages 480\u2013493, 2021.\n[2] Hareesh Bahuleyan, Lili Mou, Hao Zhou, and Olga Vechtomova. Stochastic Wasserstein\nautoencoder for probabilistic sentence generation. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 4068\u20134076, 2019.\n[3] Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. In Proceedings\nof the 22nd International Conference on Arti\ufb01cial Intelligence and Statistics, pages 66\u201375,\n2019.\n[4] Sam Bond-Taylor, Adam Leach, Yang Long, and Chris G. Willcocks. Deep generative modelling:\nA comparative review of VAEs, GANs, normalizing \ufb02ows, energy-based and autoregressive\nmodels. CoRR, abs/2103.04922, 2021.\n[5] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing, pages 632\u2013642, 2015.\n[6] Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal J\u00f3zefowicz, and Samy\nBengio. Generating sentences from a continuous space. In Proceedings of the 20th SIGNLL\nConference on Computational Natural Language Learning, pages 10\u201321, 2016.\n[7] Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.\nIn Proceedings of the 4th International Conference on Learning Representations, 2016.\n10\n"], "summary": "The cited paper, \"Improving Variational Autoencoders with Density Gap-based Regularization,\" addresses the posterior collapse and \"hole problem\" in continuous VAEs for text generation, which are issues also central to your work. While your research proposes a discrete variational attention model to tackle these problems, the cited paper introduces a novel density gap-based regularization method for continuous VAEs. Both papers aim to enhance the latent space and mitigate posterior collapse in VAEs for language generation, but they explore different approaches: yours with discrete latent variables and attention, and theirs with a specific regularization technique for continuous latent spaces. Their work provides a comparative analysis of various VAE improvements, including those related to modified training strategies and model structures, which could offer valuable context for your experimental comparisons.", "citation": "Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang, Jianfeng Gao, Lawrence Carin (2019). Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models. arXiv:1902.00154. https://arxiv.org/abs/1902.00154"}]}