{"Title": "BERT: Pre-training of Deep Bidirectional Transformers", "Abstract": "We introduce BERT, a new method for pre-training language representations which obtains state-of-the-art results on a wide array of natural language processing tasks."}
{"Title": "GPT-3: Language Models are Few-Shot Learners", "Abstract": "We show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches."}
{"Title": "Attention Is All You Need", "Abstract": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."}
{"Title": "ImageNet Classification with Deep Convolutional Neural Networks", "Abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes."}
{"Title": "Deep Residual Learning for Image Recognition", "Abstract": "We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously."}
