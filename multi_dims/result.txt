Related Work:
## Related Work

Variational Autoencoders (VAEs) have emerged as a powerful framework for natural language generation, yet they are frequently plagued by two persistent challenges: information under-representation and posterior collapse. Information under-representation arises when the latent space, typically derived from only the final hidden state of an encoder, fails to adequately summarize the input data. Posterior collapse, on the other hand, occurs when the Kullback-Leibler (KL) divergence term in the VAE objective function dominates the reconstruction loss, leading to an uninformative latent space that is ignored by the decoder. Our work, which proposes a discrete variational attention model with a categorical distribution over the attention mechanism and an auto-regressive prior, directly addresses these issues by leveraging the discrete nature of language and enhancing the latent space for improved generation.

A significant body of research has focused on mitigating posterior collapse in VAEs for text generation. Many approaches explore modifications to the training objective or the latent space itself. For instance, Ding et al. (2023) experiment with techniques like KL warm-up, word dropout, and KL coefficient adjustments within a continuous latent space to address posterior collapse in conditional VAEs for neural machine translation. Similarly, Long et al. (2019) investigate posterior collapse and encoder feature dispersion in sequence VAEs, while Mansbridge et al. (2018) propose AutoGen, an RNN-based VAE that adds high-fidelity reconstructions to the objective function to force latent variable utilization, also discussing KL annealing as a common technique. Shen et al. (2019) introduce a density gap-based regularization method to combat posterior collapse and the "hole problem" in continuous VAEs. While these works primarily operate within continuous latent spaces and employ various regularization or training strategies, our approach distinguishes itself by leveraging the inherent discreteness of language to inherently prevent posterior collapse.

The concept of discrete latent variables has gained traction as a promising avenue for addressing VAE limitations in text generation. Our work builds upon and extends this idea by proposing a discrete variational attention model. Fang et al. (2020) directly address information under-representation and posterior collapse by proposing a discrete variational attention model with a categorical distribution over the attention mechanism and an auto-regressive prior, a model that is nearly identical to the one presented in our abstract, indicating a strong foundational relationship. Similarly, Li et al. (2020) propose the Discrete Auto-regressive Variational Attention Models (DAVAM) which also leverages discrete latent space to avoid posterior collapse and an auto-regressive prior to capture sequential dependencies, further validating the efficacy of our chosen approach. Park et al. (2018) also explore discrete latent variables for text generation, specifically proposing a discretized bottleneck (DB-VAE) to enforce implicit latent feature matching and alleviate posterior collapse, and investigating the impact of codebook size. These works collectively underscore the benefits of discrete latent spaces for language generation and provide strong support for our choice of a discrete approach.

Beyond discrete latent variables, other approaches have explored alternative latent space structures to improve VAE performance. Celotti et al. (2020) propose APo-VAE, which utilizes hyperbolic latent space and wrapped normal distributions to achieve robustness against posterior collapse and capture hierarchical structures in text. While our work focuses on discrete attention, APo-VAE offers a contrasting method for enhancing the latent space. Serban et al. (2017) investigate the use of flexible, multi-modal piecewise constant distributions for continuous latent variables in VAEs to overcome the limitations of simplistic Gaussian priors and better capture complex, multi-modal latent factors. Their work, while focusing on continuous variables, acknowledges the promise of discrete latent variables, providing a valuable comparative perspective for our discrete approach.

Addressing information under-representation, where only the last hidden state is used to form the latent space, is another critical aspect of VAE improvement. Wang et al. (2017) propose HR-VAE, which tackles latent variable collapse by imposing a standard normal prior on all hidden states of the RNN encoder, rather than just the last one. This directly relates to our concern about information under-representation, as both works aim to improve the latent space's ability to summarize data. Bowman et al. (2015) also explore generating sentences from a continuous space, a foundational work in VAEs for text generation that highlights the challenges of effectively utilizing the latent space. Our approach, by incorporating an attention mechanism, aims to capture more comprehensive information from the input sequence, thereby mitigating information under-representation.

The utility of the latent space for specific generation tasks, such as controllable text generation, has also been a focus of research. Shu et al. (2020) address the "latent vacancy" problem, where manipulated latent codes fall into low-density regions, leading to poor decoding, by constraining the posterior mean to a learned probability simplex for unsupervised controllable text generation. While our work focuses on general language generation, their efforts to improve latent space utility for specific applications are relevant. Similarly, Li et al. (2022) explore Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation, aiming to create more interpretable and controllable latent spaces.

Some research has explored alternative architectural components or pre-training strategies to enhance VAEs for text generation. Wang et al. (2019) propose PoDA, a denoising autoencoder approach for pre-training seq2seq models for text generation, which could potentially complement our VAE-based approach by providing a robust pre-training method for the underlying sequence-to-sequence architecture. Pang et al. (2021) introduce OPTIMUS, a large-scale VAE model combining BERT and GPT-2 to create a universal latent space for sentences, demonstrating the benefits of pre-training on massive datasets for mitigating posterior collapse. While OPTIMUS uses continuous latent variables, its success in creating a well-structured latent space offers a contrasting yet valuable perspective.

The role of attention mechanisms in VAEs for text generation has also been investigated. Bahuleyan (2018) addresses the "bypassing phenomenon" in Variational Encoder-Decoders (VEDs), where a deterministic attention mechanism can render the variational latent space ineffective. They propose a variational attention mechanism where the attention vector itself is modeled as random variables, aiming to alleviate the bypassing problem and increase diversity. This work is particularly relevant to ours, as we also employ an attention mechanism, but our focus is on making the attention mechanism discrete to align with the discrete nature of language and prevent posterior collapse.

Finally, other related works address different aspects of improving text generation models. Tu and Li (2022) provide an overview of controllable text generation via VAEs, offering a broader context for our work. Zhang et al. (2023) introduce DEM-VAE to address the mode-collapse problem in VAEs with mixture priors, aiming for a well-structured and interpretable latent space, which aligns with our interest in discrete representations for language, albeit for a different type of collapse. McCarthy et al. (2019) introduce Variational State Space Models (VSSMs) for parallel autoregressive generation, focusing on efficient generation through a different architectural approach. While their primary focus is on parallelization, their use of a VAE framework and aim for improved performance in language generation suggests a potential future intersection with our research. Jhamtani et al. (2020) propose a deep latent variable model for story generation that learns a sequence of discrete anchor words as a latent plan, highlighting the utility of discrete latent variables for inducing useful latent spaces. These diverse approaches collectively contribute to the ongoing efforts to enhance the capabilities of language generation models.Citations:
['Zhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu, Tianyang Zhan, Gholamreza Haffari (2022). Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. arXiv:2202.13363. https://arxiv.org/abs/2202.13363', 'Haoqin Tu, Yitong Li (2022). An Overview on Controllable Text Generation via Variational Auto-Encoders. arXiv:2211.07954. https://arxiv.org/abs/2211.07954', 'Lei Shu, Alexandros Papangelis, Yi-Chia Wang, Gokhan Tur, Hu Xu, Zhaleh Feizollahi, Bing Liu, Piero Molino (2020). Controllable Text Generation with Focused Variation. arXiv:2009.12046. https://arxiv.org/abs/2009.12046', 'Yookoon Park, Jaemin Cho, Gunhee Kim (2018). A Hierarchical Latent Structure for Variational Conversation Modeling. arXiv:1804.03424. https://arxiv.org/abs/1804.03424', 'Xianghong Fang, Haoli Bai, Zenglin Xu, Michael Lyu, Irwin King (2020). Discrete Variational Attention Models for Language Generation. arXiv:2004.09764. https://arxiv.org/abs/2004.09764', 'Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, Tat-Seng Chua (2023). MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space. arXiv:2305.12785. https://arxiv.org/abs/2305.12785', 'Bo Pang, Erik Nijkamp, Tian Han, Ying Nian Wu (2021). Generative Text Modeling through Short Run Inference. arXiv:2106.02513. https://arxiv.org/abs/2106.02513', 'Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang, Jianfeng Gao, Lawrence Carin (2019). Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models. arXiv:1902.00154. https://arxiv.org/abs/1902.00154', 'Luca Celotti, Simon Brodeur, Jean Rouat (2020). AriEL: volume coding for sentence generation. arXiv:2003.13600. https://arxiv.org/abs/2003.13600', 'Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, Jianfeng Gao (2020). Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space. arXiv:2004.04092. https://arxiv.org/abs/2004.04092', 'Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, Andr√© Freitas (2023). LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces. arXiv:2312.13208. https://arxiv.org/abs/2312.13208', 'Hareesh Bahuleyan (2018). Natural Language Generation with Neural Variational Models. arXiv:1808.09012. https://arxiv.org/abs/1808.09012', 'Alex Mansbridge, Roberto Fierimonte, Ilya Feige, David Barber (2018). Improving latent variable descriptiveness with AutoGen. arXiv:1806.04480. https://arxiv.org/abs/1806.04480', 'Heng Wang, Zengchang Qin, Tao Wan (2017). Text Generation Based on Generative Adversarial Nets with Latent Variable. arXiv:1712.00170. https://arxiv.org/abs/1712.00170', 'Teng Long, Yanshuai Cao, Jackie Chi Kit Cheung (2019). On Posterior Collapse and Encoder Feature Dispersion in Sequence VAEs. arXiv:1911.03976. https://arxiv.org/abs/1911.03976']