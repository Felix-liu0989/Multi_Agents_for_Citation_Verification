{
    "tasks": [
        {
            "paper_id": 0,
            "title": "Discrete Variational Attention Models for Language Generation",
            "abstract": "Variational autoencoders have been widely applied for natural language\ngeneration, however, there are two long-standing problems: information\nunder-representation and posterior collapse. The former arises from the fact\nthat only the last hidden state from the encoder is transformed to the latent\nspace, which is insufficient to summarize data. The latter comes as a result of\nthe imbalanced scale between the reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in this paper we propose the\ndiscrete variational attention model with categorical distribution over the\nattention mechanism owing to the discrete nature in languages. Our approach is\ncombined with an auto-regressive prior to capture the sequential dependency\nfrom observations, which can enhance the latent space for language generation.\nMoreover, thanks to the property of discreteness, the training of our proposed\napproach does not suffer from posterior collapse. Furthermore, we carefully\nanalyze the superiority of discrete latent space over the continuous space with\nthe common Gaussian distribution. Extensive experiments on language generation\ndemonstrate superior advantages of our proposed approach in comparison with the\nstate-of-the-art counterparts."
        },
        {
            "paper_id": 1,
            "title": "Natural Language Generation with Neural Variational Models",
            "abstract": "In this thesis, we explore the use of deep neural networks for generation of\nnatural language. Specifically, we implement two sequence-to-sequence neural\nvariational models - variational autoencoders (VAE) and variational\nencoder-decoders (VED). VAEs for text generation are difficult to train due to\nissues associated with the Kullback-Leibler (KL) divergence term of the loss\nfunction vanishing to zero. We successfully train VAEs by implementing\noptimization heuristics such as KL weight annealing and word dropout. We also\ndemonstrate the effectiveness of this continuous latent space through\nexperiments such as random sampling, linear interpolation and sampling from the\nneighborhood of the input. We argue that if VAEs are not designed\nappropriately, it may lead to bypassing connections which results in the latent\nspace being ignored during training. We show experimentally with the example of\ndecoder hidden state initialization that such bypassing connections degrade the\nVAE into a deterministic model, thereby reducing the diversity of generated\nsentences. We discover that the traditional attention mechanism used in\nsequence-to-sequence VED models serves as a bypassing connection, thereby\ndeteriorating the model's latent space. In order to circumvent this issue, we\npropose the variational attention mechanism where the attention context vector\nis modeled as a random variable that can be sampled from a distribution. We\nshow empirically using automatic evaluation metrics, namely entropy and\ndistinct measures, that our variational attention model generates more diverse\noutput sentences than the deterministic attention model. A qualitative analysis\nwith human evaluation study proves that our model simultaneously produces\nsentences that are of high quality and equally fluent as the ones generated by\nthe deterministic attention counterpart."
        },
        {
            "paper_id": 2,
            "title": "mu-Forcing: Training Variational Recurrent Autoencoders for Text Generation",
            "abstract": "It has been previously observed that training Variational Recurrent\nAutoencoders (VRAE) for text generation suffers from serious uninformative\nlatent variables problem. The model would collapse into a plain language model\nthat totally ignore the latent variables and can only generate repeating and\ndull samples. In this paper, we explore the reason behind this issue and\npropose an effective regularizer based approach to address it. The proposed\nmethod directly injects extra constraints on the posteriors of latent variables\ninto the learning process of VRAE, which can flexibly and stably control the\ntrade-off between the KL term and the reconstruction term, making the model\nlearn dense and meaningful latent representations. The experimental results\nshow that the proposed method outperforms several strong baselines and can make\nthe model learn interpretable latent variables and generate diverse meaningful\nsentences. Furthermore, the proposed method can perform well without using\nother strategies, such as KL annealing."
        },
        {
            "paper_id": 3,
            "title": "Generative Text Modeling through Short Run Inference",
            "abstract": "Latent variable models for text, when trained successfully, accurately model\nthe data distribution and capture global semantic and syntactic features of\nsentences. The prominent approach to train such models is variational\nautoencoders (VAE). It is nevertheless challenging to train and often results\nin a trivial local optimum where the latent variable is ignored and its\nposterior collapses into the prior, an issue known as posterior collapse.\nVarious techniques have been proposed to mitigate this issue. Most of them\nfocus on improving the inference model to yield latent codes of higher quality.\nThe present work proposes a short run dynamics for inference. It is initialized\nfrom the prior distribution of the latent variable and then runs a small number\n(e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The\nmajor advantage of our method is that it does not require a separate inference\nmodel or assume simple geometry of the posterior distribution, thus rendering\nan automatic, natural and flexible inference engine. We show that the models\ntrained with short run dynamics more accurately model the data, compared to\nstrong language model and VAE baselines, and exhibit no sign of posterior\ncollapse. Analyses of the latent space show that interpolation in the latent\nspace is able to generate coherent sentences with smooth transition and\ndemonstrate improved classification over strong baselines with latent features\nfrom unsupervised pretraining. These results together expose a well-structured\nlatent space of our generative model."
        },
        {
            "paper_id": 4,
            "title": "Discrete Auto-regressive Variational Attention Models for Text Modeling",
            "abstract": "Variational autoencoders (VAEs) have been widely applied for text modeling.\nIn practice, however, they are troubled by two challenges: information\nunderrepresentation and posterior collapse. The former arises as only the last\nhidden state of LSTM encoder is transformed into the latent space, which is\ngenerally insufficient to summarize the data. The latter is a long-standing\nproblem during the training of VAEs as the optimization is trapped to a\ndisastrous local optimum. In this paper, we propose Discrete Auto-regressive\nVariational Attention Model (DAVAM) to address the challenges. Specifically, we\nintroduce an auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency from the input.\nWe further design discrete latent space for the variational attention and\nmathematically show that our model is free from posterior collapse. Extensive\nexperiments on language modeling tasks demonstrate the superiority of DAVAM\nagainst several VAE counterparts."
        },
        {
            "paper_id": 5,
            "title": "Preventing Posterior Collapse with Levenshtein Variational Autoencoder",
            "abstract": "Variational autoencoders (VAEs) are a standard framework for inducing latent\nvariable models that have been shown effective in learning text representations\nas well as in text generation. The key challenge with using VAEs is the {\\it\nposterior collapse} problem: learning tends to converge to trivial solutions\nwhere the generators ignore latent variables. In our Levenstein VAE, we propose\nto replace the evidence lower bound (ELBO) with a new objective which is simple\nto optimize and prevents posterior collapse. Intuitively, it corresponds to\ngenerating a sequence from the autoencoder and encouraging the model to predict\nan optimal continuation according to the Levenshtein distance (LD) with the\nreference sentence at each time step in the generated sequence. We motivate the\nmethod from the probabilistic perspective by showing that it is closely related\nto optimizing a bound on the intractable Kullback-Leibler divergence of an\nLD-based kernel density estimator from the model distribution. With this\nobjective, any generator disregarding latent variables will incur large\npenalties and hence posterior collapse does not happen. We relate our approach\nto policy distillation \\cite{RossGB11} and dynamic oracles \\cite{GoldbergN12}.\nBy considering Yelp and SNLI benchmarks, we show that Levenstein VAE produces\nmore informative latent representations than alternative approaches to\npreventing posterior collapse."
        },
        {
            "paper_id": 6,
            "title": "Conditional Variational Autoencoder for Neural Machine Translation",
            "abstract": "We explore the performance of latent variable models for conditional text\ngeneration in the context of neural machine translation (NMT). Similar to Zhang\net al., we augment the encoder-decoder NMT paradigm by introducing a continuous\nlatent variable to model features of the translation process. We extend this\nmodel with a co-attention mechanism motivated by Parikh et al. in the inference\nnetwork. Compared to the vision domain, latent variable models for text face\nadditional challenges due to the discrete nature of language, namely posterior\ncollapse. We experiment with different approaches to mitigate this issue. We\nshow that our conditional variational model improves upon both discriminative\nattention-based translation and the variational baseline presented in Zhang et\nal. Finally, we present some exploration of the learned latent space to\nillustrate what the latent variable is capable of capturing. This is the first\nreported conditional variational model for text that meaningfully utilizes the\nlatent variable without weakening the translation model."
        },
        {
            "paper_id": 7,
            "title": "Improve Diverse Text Generation by Self Labeling Conditional Variational Auto Encoder",
            "abstract": "Diversity plays a vital role in many text generating applications. In recent\nyears, Conditional Variational Auto Encoders (CVAE) have shown promising\nperformances for this task. However, they often encounter the so called\nKL-Vanishing problem. Previous works mitigated such problem by heuristic\nmethods such as strengthening the encoder or weakening the decoder while\noptimizing the CVAE objective function. Nevertheless, the optimizing direction\nof these methods are implicit and it is hard to find an appropriate degree to\nwhich these methods should be applied. In this paper, we propose an explicit\noptimizing objective to complement the CVAE to directly pull away from\nKL-vanishing. In fact, this objective term guides the encoder towards the \"best\nencoder\" of the decoder to enhance the expressiveness. A labeling network is\nintroduced to estimate the \"best encoder\". It provides a continuous label in\nthe latent space of CVAE to help build a close connection between latent\nvariables and targets. The whole proposed method is named Self Labeling\nCVAE~(SLCVAE). To accelerate the research of diverse text generation, we also\npropose a large native one-to-many dataset. Extensive experiments are conducted\non two tasks, which show that our method largely improves the generating\ndiversity while achieving comparable accuracy compared with state-of-art\nalgorithms."
        },
        {
            "paper_id": 8,
            "title": "Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation",
            "abstract": "The variational autoencoder (VAE) imposes a probabilistic distribution\n(typically Gaussian) on the latent space and penalizes the Kullback--Leibler\n(KL) divergence between the posterior and prior. In NLP, VAEs are extremely\ndifficult to train due to the problem of KL collapsing to zero. One has to\nimplement various heuristics such as KL weight annealing and word dropout in a\ncarefully engineered manner to successfully train a VAE for text. In this\npaper, we propose to use the Wasserstein autoencoder (WAE) for probabilistic\nsentence generation, where the encoder could be either stochastic or\ndeterministic. We show theoretically and empirically that, in the original WAE,\nthe stochastically encoded Gaussian distribution tends to become a Dirac-delta\nfunction, and we propose a variant of WAE that encourages the stochasticity of\nthe encoder. Experimental results show that the latent space learned by WAE\nexhibits properties of continuity and smoothness as in VAEs, while\nsimultaneously achieving much higher BLEU scores for sentence reconstruction."
        },
        {
            "paper_id": 9,
            "title": "Implicit Deep Latent Variable Models for Text Generation",
            "abstract": "Deep latent variable models (LVM) such as variational auto-encoder (VAE) have\nrecently played an important role in text generation. One key factor is the\nexploitation of smooth latent structures to guide the generation. However, the\nrepresentation power of VAEs is limited due to two reasons: (1) the Gaussian\nassumption is often made on the variational posteriors; and meanwhile (2) a\nnotorious \"posterior collapse\" issue occurs. In this paper, we advocate\nsample-based representations of variational distributions for natural language,\nleading to implicit latent features, which can provide flexible representation\npower compared with Gaussian-based posteriors. We further develop an LVM to\ndirectly match the aggregated posterior to the prior. It can be viewed as a\nnatural extension of VAEs with a regularization of maximizing mutual\ninformation, mitigating the \"posterior collapse\" issue. We demonstrate the\neffectiveness and versatility of our models in various text generation\nscenarios, including language modeling, unaligned style transfer, and dialog\nresponse generation. The source code to reproduce our experimental results is\navailable on GitHub."
        },
        {
            "paper_id": 10,
            "title": "MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space",
            "abstract": "Multi-aspect controllable text generation aims to generate fluent sentences\nthat possess multiple desired attributes simultaneously. Traditional methods\neither combine many operators in the decoding stage, often with costly\niteration or search in the discrete text space, or train separate controllers\nfor each aspect, resulting in a degeneration of text quality due to the\ndiscrepancy between different aspects. To address these limitations, we\nintroduce a novel approach for multi-aspect control, namely MacLaSa, that\nestimates compact latent space for multiple aspects and performs efficient\nsampling with a robust sampler based on ordinary differential equations (ODEs).\nTo eliminate the domain gaps between different aspects, we utilize a\nVariational Autoencoder (VAE) network to map text sequences from varying data\nsources into close latent representations. The estimated latent space enables\nthe formulation of joint energy-based models (EBMs) and the plugging in of\narbitrary attribute discriminators to achieve multi-aspect control. Afterwards,\nwe draw latent vector samples with an ODE-based sampler and feed sampled\nexamples to the VAE decoder to produce target text sequences. Experimental\nresults demonstrate that MacLaSa outperforms several strong baselines on\nattribute relevance and textual quality while maintaining a high inference\nspeed."
        },
        {
            "paper_id": 11,
            "title": "LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces",
            "abstract": "Deep generative neural networks, such as Variational AutoEncoders (VAEs),\noffer an opportunity to better understand and control language models from the\nperspective of sentence-level latent spaces. To combine the controllability of\nVAE latent spaces with the state-of-the-art performance of recent large\nlanguage models (LLMs), we present in this work LlaMaVAE, which combines\nexpressive encoder and decoder models (sentenceT5 and LlaMA) with a VAE\narchitecture, aiming to provide better text generation control to LLMs. In\naddition, to conditionally guide the VAE generation, we investigate a new\napproach based on flow-based invertible neural networks (INNs) named Invertible\nCVAE. Experimental results reveal that LlaMaVAE can outperform the previous\nstate-of-the-art VAE language model, Optimus, across various tasks, including\nlanguage modelling, semantic textual similarity and definition modelling.\nQualitative analysis on interpolation and traversal experiments also indicates\nan increased degree of semantic clustering and geometric consistency, which\nenables better generation control."
        },
        {
            "paper_id": 12,
            "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space",
            "abstract": "When trained effectively, the Variational Autoencoder (VAE) can be both a\npowerful generative model and an effective representation learning framework\nfor natural language. In this paper, we propose the first large-scale language\nVAE model, Optimus. A universal latent embedding space for sentences is first\npre-trained on large text corpus, and then fine-tuned for various language\ngeneration and understanding tasks. Compared with GPT-2, Optimus enables guided\nlanguage generation from an abstract level using the latent vectors. Compared\nwith BERT, Optimus can generalize better on low-resource language understanding\ntasks due to the smooth latent space structure. Extensive experimental results\non a wide range of language tasks demonstrate the effectiveness of Optimus. It\nachieves new state-of-the-art on VAE language modeling benchmarks. We hope that\nour first pre-trained big VAE language model itself and results can help the\nNLP community renew the interests of deep generative models in the era of\nlarge-scale pre-training, and make these principled methods more practical."
        },
        {
            "paper_id": 13,
            "title": "Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation",
            "abstract": "Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that TRACE could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that TRACE achieves significantly improved diversity\nwhile maintaining satisfactory generation quality."
        },
        {
            "paper_id": 14,
            "title": "Dual Latent Variable Model for Low-Resource Natural Language Generation in Dialogue Systems",
            "abstract": "Recent deep learning models have shown improving results to natural language\ngeneration (NLG) irrespective of providing sufficient annotated data. However,\na modest training data may harm such models performance. Thus, how to build a\ngenerator that can utilize as much of knowledge from a low-resource setting\ndata is a crucial issue in NLG. This paper presents a variational neural-based\ngeneration model to tackle the NLG problem of having limited labeled dataset,\nin which we integrate a variational inference into an encoder-decoder generator\nand introduce a novel auxiliary autoencoding with an effective training\nprocedure. Experiments showed that the proposed methods not only outperform the\nprevious models when having sufficient training dataset but also show strong\nability to work acceptably well when the training data is scarce."
        }
    ],
    "datasets": [
        {
            "paper_id": 7,
            "title": "Improve Diverse Text Generation by Self Labeling Conditional Variational Auto Encoder",
            "abstract": "Diversity plays a vital role in many text generating applications. In recent\nyears, Conditional Variational Auto Encoders (CVAE) have shown promising\nperformances for this task. However, they often encounter the so called\nKL-Vanishing problem. Previous works mitigated such problem by heuristic\nmethods such as strengthening the encoder or weakening the decoder while\noptimizing the CVAE objective function. Nevertheless, the optimizing direction\nof these methods are implicit and it is hard to find an appropriate degree to\nwhich these methods should be applied. In this paper, we propose an explicit\noptimizing objective to complement the CVAE to directly pull away from\nKL-vanishing. In fact, this objective term guides the encoder towards the \"best\nencoder\" of the decoder to enhance the expressiveness. A labeling network is\nintroduced to estimate the \"best encoder\". It provides a continuous label in\nthe latent space of CVAE to help build a close connection between latent\nvariables and targets. The whole proposed method is named Self Labeling\nCVAE~(SLCVAE). To accelerate the research of diverse text generation, we also\npropose a large native one-to-many dataset. Extensive experiments are conducted\non two tasks, which show that our method largely improves the generating\ndiversity while achieving comparable accuracy compared with state-of-art\nalgorithms."
        }
    ],
    "methodologies": [
        {
            "paper_id": 0,
            "title": "Discrete Variational Attention Models for Language Generation",
            "abstract": "Variational autoencoders have been widely applied for natural language\ngeneration, however, there are two long-standing problems: information\nunder-representation and posterior collapse. The former arises from the fact\nthat only the last hidden state from the encoder is transformed to the latent\nspace, which is insufficient to summarize data. The latter comes as a result of\nthe imbalanced scale between the reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in this paper we propose the\ndiscrete variational attention model with categorical distribution over the\nattention mechanism owing to the discrete nature in languages. Our approach is\ncombined with an auto-regressive prior to capture the sequential dependency\nfrom observations, which can enhance the latent space for language generation.\nMoreover, thanks to the property of discreteness, the training of our proposed\napproach does not suffer from posterior collapse. Furthermore, we carefully\nanalyze the superiority of discrete latent space over the continuous space with\nthe common Gaussian distribution. Extensive experiments on language generation\ndemonstrate superior advantages of our proposed approach in comparison with the\nstate-of-the-art counterparts."
        },
        {
            "paper_id": 1,
            "title": "Natural Language Generation with Neural Variational Models",
            "abstract": "In this thesis, we explore the use of deep neural networks for generation of\nnatural language. Specifically, we implement two sequence-to-sequence neural\nvariational models - variational autoencoders (VAE) and variational\nencoder-decoders (VED). VAEs for text generation are difficult to train due to\nissues associated with the Kullback-Leibler (KL) divergence term of the loss\nfunction vanishing to zero. We successfully train VAEs by implementing\noptimization heuristics such as KL weight annealing and word dropout. We also\ndemonstrate the effectiveness of this continuous latent space through\nexperiments such as random sampling, linear interpolation and sampling from the\nneighborhood of the input. We argue that if VAEs are not designed\nappropriately, it may lead to bypassing connections which results in the latent\nspace being ignored during training. We show experimentally with the example of\ndecoder hidden state initialization that such bypassing connections degrade the\nVAE into a deterministic model, thereby reducing the diversity of generated\nsentences. We discover that the traditional attention mechanism used in\nsequence-to-sequence VED models serves as a bypassing connection, thereby\ndeteriorating the model's latent space. In order to circumvent this issue, we\npropose the variational attention mechanism where the attention context vector\nis modeled as a random variable that can be sampled from a distribution. We\nshow empirically using automatic evaluation metrics, namely entropy and\ndistinct measures, that our variational attention model generates more diverse\noutput sentences than the deterministic attention model. A qualitative analysis\nwith human evaluation study proves that our model simultaneously produces\nsentences that are of high quality and equally fluent as the ones generated by\nthe deterministic attention counterpart."
        },
        {
            "paper_id": 2,
            "title": "mu-Forcing: Training Variational Recurrent Autoencoders for Text Generation",
            "abstract": "It has been previously observed that training Variational Recurrent\nAutoencoders (VRAE) for text generation suffers from serious uninformative\nlatent variables problem. The model would collapse into a plain language model\nthat totally ignore the latent variables and can only generate repeating and\ndull samples. In this paper, we explore the reason behind this issue and\npropose an effective regularizer based approach to address it. The proposed\nmethod directly injects extra constraints on the posteriors of latent variables\ninto the learning process of VRAE, which can flexibly and stably control the\ntrade-off between the KL term and the reconstruction term, making the model\nlearn dense and meaningful latent representations. The experimental results\nshow that the proposed method outperforms several strong baselines and can make\nthe model learn interpretable latent variables and generate diverse meaningful\nsentences. Furthermore, the proposed method can perform well without using\nother strategies, such as KL annealing."
        },
        {
            "paper_id": 3,
            "title": "Generative Text Modeling through Short Run Inference",
            "abstract": "Latent variable models for text, when trained successfully, accurately model\nthe data distribution and capture global semantic and syntactic features of\nsentences. The prominent approach to train such models is variational\nautoencoders (VAE). It is nevertheless challenging to train and often results\nin a trivial local optimum where the latent variable is ignored and its\nposterior collapses into the prior, an issue known as posterior collapse.\nVarious techniques have been proposed to mitigate this issue. Most of them\nfocus on improving the inference model to yield latent codes of higher quality.\nThe present work proposes a short run dynamics for inference. It is initialized\nfrom the prior distribution of the latent variable and then runs a small number\n(e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The\nmajor advantage of our method is that it does not require a separate inference\nmodel or assume simple geometry of the posterior distribution, thus rendering\nan automatic, natural and flexible inference engine. We show that the models\ntrained with short run dynamics more accurately model the data, compared to\nstrong language model and VAE baselines, and exhibit no sign of posterior\ncollapse. Analyses of the latent space show that interpolation in the latent\nspace is able to generate coherent sentences with smooth transition and\ndemonstrate improved classification over strong baselines with latent features\nfrom unsupervised pretraining. These results together expose a well-structured\nlatent space of our generative model."
        },
        {
            "paper_id": 4,
            "title": "Discrete Auto-regressive Variational Attention Models for Text Modeling",
            "abstract": "Variational autoencoders (VAEs) have been widely applied for text modeling.\nIn practice, however, they are troubled by two challenges: information\nunderrepresentation and posterior collapse. The former arises as only the last\nhidden state of LSTM encoder is transformed into the latent space, which is\ngenerally insufficient to summarize the data. The latter is a long-standing\nproblem during the training of VAEs as the optimization is trapped to a\ndisastrous local optimum. In this paper, we propose Discrete Auto-regressive\nVariational Attention Model (DAVAM) to address the challenges. Specifically, we\nintroduce an auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency from the input.\nWe further design discrete latent space for the variational attention and\nmathematically show that our model is free from posterior collapse. Extensive\nexperiments on language modeling tasks demonstrate the superiority of DAVAM\nagainst several VAE counterparts."
        },
        {
            "paper_id": 5,
            "title": "Preventing Posterior Collapse with Levenshtein Variational Autoencoder",
            "abstract": "Variational autoencoders (VAEs) are a standard framework for inducing latent\nvariable models that have been shown effective in learning text representations\nas well as in text generation. The key challenge with using VAEs is the {\\it\nposterior collapse} problem: learning tends to converge to trivial solutions\nwhere the generators ignore latent variables. In our Levenstein VAE, we propose\nto replace the evidence lower bound (ELBO) with a new objective which is simple\nto optimize and prevents posterior collapse. Intuitively, it corresponds to\ngenerating a sequence from the autoencoder and encouraging the model to predict\nan optimal continuation according to the Levenshtein distance (LD) with the\nreference sentence at each time step in the generated sequence. We motivate the\nmethod from the probabilistic perspective by showing that it is closely related\nto optimizing a bound on the intractable Kullback-Leibler divergence of an\nLD-based kernel density estimator from the model distribution. With this\nobjective, any generator disregarding latent variables will incur large\npenalties and hence posterior collapse does not happen. We relate our approach\nto policy distillation \\cite{RossGB11} and dynamic oracles \\cite{GoldbergN12}.\nBy considering Yelp and SNLI benchmarks, we show that Levenstein VAE produces\nmore informative latent representations than alternative approaches to\npreventing posterior collapse."
        },
        {
            "paper_id": 6,
            "title": "Conditional Variational Autoencoder for Neural Machine Translation",
            "abstract": "We explore the performance of latent variable models for conditional text\ngeneration in the context of neural machine translation (NMT). Similar to Zhang\net al., we augment the encoder-decoder NMT paradigm by introducing a continuous\nlatent variable to model features of the translation process. We extend this\nmodel with a co-attention mechanism motivated by Parikh et al. in the inference\nnetwork. Compared to the vision domain, latent variable models for text face\nadditional challenges due to the discrete nature of language, namely posterior\ncollapse. We experiment with different approaches to mitigate this issue. We\nshow that our conditional variational model improves upon both discriminative\nattention-based translation and the variational baseline presented in Zhang et\nal. Finally, we present some exploration of the learned latent space to\nillustrate what the latent variable is capable of capturing. This is the first\nreported conditional variational model for text that meaningfully utilizes the\nlatent variable without weakening the translation model."
        },
        {
            "paper_id": 7,
            "title": "Improve Diverse Text Generation by Self Labeling Conditional Variational Auto Encoder",
            "abstract": "Diversity plays a vital role in many text generating applications. In recent\nyears, Conditional Variational Auto Encoders (CVAE) have shown promising\nperformances for this task. However, they often encounter the so called\nKL-Vanishing problem. Previous works mitigated such problem by heuristic\nmethods such as strengthening the encoder or weakening the decoder while\noptimizing the CVAE objective function. Nevertheless, the optimizing direction\nof these methods are implicit and it is hard to find an appropriate degree to\nwhich these methods should be applied. In this paper, we propose an explicit\noptimizing objective to complement the CVAE to directly pull away from\nKL-vanishing. In fact, this objective term guides the encoder towards the \"best\nencoder\" of the decoder to enhance the expressiveness. A labeling network is\nintroduced to estimate the \"best encoder\". It provides a continuous label in\nthe latent space of CVAE to help build a close connection between latent\nvariables and targets. The whole proposed method is named Self Labeling\nCVAE~(SLCVAE). To accelerate the research of diverse text generation, we also\npropose a large native one-to-many dataset. Extensive experiments are conducted\non two tasks, which show that our method largely improves the generating\ndiversity while achieving comparable accuracy compared with state-of-art\nalgorithms."
        },
        {
            "paper_id": 8,
            "title": "Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation",
            "abstract": "The variational autoencoder (VAE) imposes a probabilistic distribution\n(typically Gaussian) on the latent space and penalizes the Kullback--Leibler\n(KL) divergence between the posterior and prior. In NLP, VAEs are extremely\ndifficult to train due to the problem of KL collapsing to zero. One has to\nimplement various heuristics such as KL weight annealing and word dropout in a\ncarefully engineered manner to successfully train a VAE for text. In this\npaper, we propose to use the Wasserstein autoencoder (WAE) for probabilistic\nsentence generation, where the encoder could be either stochastic or\ndeterministic. We show theoretically and empirically that, in the original WAE,\nthe stochastically encoded Gaussian distribution tends to become a Dirac-delta\nfunction, and we propose a variant of WAE that encourages the stochasticity of\nthe encoder. Experimental results show that the latent space learned by WAE\nexhibits properties of continuity and smoothness as in VAEs, while\nsimultaneously achieving much higher BLEU scores for sentence reconstruction."
        },
        {
            "paper_id": 9,
            "title": "Implicit Deep Latent Variable Models for Text Generation",
            "abstract": "Deep latent variable models (LVM) such as variational auto-encoder (VAE) have\nrecently played an important role in text generation. One key factor is the\nexploitation of smooth latent structures to guide the generation. However, the\nrepresentation power of VAEs is limited due to two reasons: (1) the Gaussian\nassumption is often made on the variational posteriors; and meanwhile (2) a\nnotorious \"posterior collapse\" issue occurs. In this paper, we advocate\nsample-based representations of variational distributions for natural language,\nleading to implicit latent features, which can provide flexible representation\npower compared with Gaussian-based posteriors. We further develop an LVM to\ndirectly match the aggregated posterior to the prior. It can be viewed as a\nnatural extension of VAEs with a regularization of maximizing mutual\ninformation, mitigating the \"posterior collapse\" issue. We demonstrate the\neffectiveness and versatility of our models in various text generation\nscenarios, including language modeling, unaligned style transfer, and dialog\nresponse generation. The source code to reproduce our experimental results is\navailable on GitHub."
        },
        {
            "paper_id": 10,
            "title": "MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space",
            "abstract": "Multi-aspect controllable text generation aims to generate fluent sentences\nthat possess multiple desired attributes simultaneously. Traditional methods\neither combine many operators in the decoding stage, often with costly\niteration or search in the discrete text space, or train separate controllers\nfor each aspect, resulting in a degeneration of text quality due to the\ndiscrepancy between different aspects. To address these limitations, we\nintroduce a novel approach for multi-aspect control, namely MacLaSa, that\nestimates compact latent space for multiple aspects and performs efficient\nsampling with a robust sampler based on ordinary differential equations (ODEs).\nTo eliminate the domain gaps between different aspects, we utilize a\nVariational Autoencoder (VAE) network to map text sequences from varying data\nsources into close latent representations. The estimated latent space enables\nthe formulation of joint energy-based models (EBMs) and the plugging in of\narbitrary attribute discriminators to achieve multi-aspect control. Afterwards,\nwe draw latent vector samples with an ODE-based sampler and feed sampled\nexamples to the VAE decoder to produce target text sequences. Experimental\nresults demonstrate that MacLaSa outperforms several strong baselines on\nattribute relevance and textual quality while maintaining a high inference\nspeed."
        },
        {
            "paper_id": 11,
            "title": "LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces",
            "abstract": "Deep generative neural networks, such as Variational AutoEncoders (VAEs),\noffer an opportunity to better understand and control language models from the\nperspective of sentence-level latent spaces. To combine the controllability of\nVAE latent spaces with the state-of-the-art performance of recent large\nlanguage models (LLMs), we present in this work LlaMaVAE, which combines\nexpressive encoder and decoder models (sentenceT5 and LlaMA) with a VAE\narchitecture, aiming to provide better text generation control to LLMs. In\naddition, to conditionally guide the VAE generation, we investigate a new\napproach based on flow-based invertible neural networks (INNs) named Invertible\nCVAE. Experimental results reveal that LlaMaVAE can outperform the previous\nstate-of-the-art VAE language model, Optimus, across various tasks, including\nlanguage modelling, semantic textual similarity and definition modelling.\nQualitative analysis on interpolation and traversal experiments also indicates\nan increased degree of semantic clustering and geometric consistency, which\nenables better generation control."
        },
        {
            "paper_id": 12,
            "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space",
            "abstract": "When trained effectively, the Variational Autoencoder (VAE) can be both a\npowerful generative model and an effective representation learning framework\nfor natural language. In this paper, we propose the first large-scale language\nVAE model, Optimus. A universal latent embedding space for sentences is first\npre-trained on large text corpus, and then fine-tuned for various language\ngeneration and understanding tasks. Compared with GPT-2, Optimus enables guided\nlanguage generation from an abstract level using the latent vectors. Compared\nwith BERT, Optimus can generalize better on low-resource language understanding\ntasks due to the smooth latent space structure. Extensive experimental results\non a wide range of language tasks demonstrate the effectiveness of Optimus. It\nachieves new state-of-the-art on VAE language modeling benchmarks. We hope that\nour first pre-trained big VAE language model itself and results can help the\nNLP community renew the interests of deep generative models in the era of\nlarge-scale pre-training, and make these principled methods more practical."
        },
        {
            "paper_id": 13,
            "title": "Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation",
            "abstract": "Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that TRACE could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that TRACE achieves significantly improved diversity\nwhile maintaining satisfactory generation quality."
        },
        {
            "paper_id": 14,
            "title": "Dual Latent Variable Model for Low-Resource Natural Language Generation in Dialogue Systems",
            "abstract": "Recent deep learning models have shown improving results to natural language\ngeneration (NLG) irrespective of providing sufficient annotated data. However,\na modest training data may harm such models performance. Thus, how to build a\ngenerator that can utilize as much of knowledge from a low-resource setting\ndata is a crucial issue in NLG. This paper presents a variational neural-based\ngeneration model to tackle the NLG problem of having limited labeled dataset,\nin which we integrate a variational inference into an encoder-decoder generator\nand introduce a novel auxiliary autoencoding with an effective training\nprocedure. Experiments showed that the proposed methods not only outperform the\nprevious models when having sufficient training dataset but also show strong\nability to work acceptably well when the training data is scarce."
        }
    ],
    "evaluation_methods": [
        {
            "paper_id": 1,
            "title": "Natural Language Generation with Neural Variational Models",
            "abstract": "In this thesis, we explore the use of deep neural networks for generation of\nnatural language. Specifically, we implement two sequence-to-sequence neural\nvariational models - variational autoencoders (VAE) and variational\nencoder-decoders (VED). VAEs for text generation are difficult to train due to\nissues associated with the Kullback-Leibler (KL) divergence term of the loss\nfunction vanishing to zero. We successfully train VAEs by implementing\noptimization heuristics such as KL weight annealing and word dropout. We also\ndemonstrate the effectiveness of this continuous latent space through\nexperiments such as random sampling, linear interpolation and sampling from the\nneighborhood of the input. We argue that if VAEs are not designed\nappropriately, it may lead to bypassing connections which results in the latent\nspace being ignored during training. We show experimentally with the example of\ndecoder hidden state initialization that such bypassing connections degrade the\nVAE into a deterministic model, thereby reducing the diversity of generated\nsentences. We discover that the traditional attention mechanism used in\nsequence-to-sequence VED models serves as a bypassing connection, thereby\ndeteriorating the model's latent space. In order to circumvent this issue, we\npropose the variational attention mechanism where the attention context vector\nis modeled as a random variable that can be sampled from a distribution. We\nshow empirically using automatic evaluation metrics, namely entropy and\ndistinct measures, that our variational attention model generates more diverse\noutput sentences than the deterministic attention model. A qualitative analysis\nwith human evaluation study proves that our model simultaneously produces\nsentences that are of high quality and equally fluent as the ones generated by\nthe deterministic attention counterpart."
        },
        {
            "paper_id": 11,
            "title": "LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces",
            "abstract": "Deep generative neural networks, such as Variational AutoEncoders (VAEs),\noffer an opportunity to better understand and control language models from the\nperspective of sentence-level latent spaces. To combine the controllability of\nVAE latent spaces with the state-of-the-art performance of recent large\nlanguage models (LLMs), we present in this work LlaMaVAE, which combines\nexpressive encoder and decoder models (sentenceT5 and LlaMA) with a VAE\narchitecture, aiming to provide better text generation control to LLMs. In\naddition, to conditionally guide the VAE generation, we investigate a new\napproach based on flow-based invertible neural networks (INNs) named Invertible\nCVAE. Experimental results reveal that LlaMaVAE can outperform the previous\nstate-of-the-art VAE language model, Optimus, across various tasks, including\nlanguage modelling, semantic textual similarity and definition modelling.\nQualitative analysis on interpolation and traversal experiments also indicates\nan increased degree of semantic clustering and geometric consistency, which\nenables better generation control."
        }
    ]
}